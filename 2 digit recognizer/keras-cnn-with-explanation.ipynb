{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d2b53a9a-2f19-4819-0705-aa59fa93fc7e"
   },
   "source": [
    "Note: it is recommended you download this notebook to your own PC if you want to train the model, as Kaggle's servers will be slow and have a timeout. \n",
    "\n",
    "### Load data and do preprocessing \n",
    "First we load our data into Pandas dataframes and convert them to NumPy arrays using *.values*. We further convert the datatype from *float64* to *float32* for speed. \n",
    "\n",
    "Since the training examples are 1D vectors, and we wish to do convolutions on the 2D images, we reshape the input data from (n_train x 784) to (n_train x 28 x 28). We also normalize the data to the interval [0,1], while this is not really necessary here as all the pixel values already lie in the same range [0, 255], it is a good procedure to follow in general. \n",
    "\n",
    "We used *to_categorical* to transform the target data (which lies in the set [0,1,2,3,4,5,6,7,8,9]) to one hot vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "1a6ad007-48e1-9943-a831-7f4bf749729f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_image_dim_ordering('th') #input shape: (channels, height, width)\n",
    "\n",
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "valid_df = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "x_train = train_df.drop(['label'], axis=1).values.astype('float32')\n",
    "Y_train = train_df['label'].values\n",
    "x_valid = valid_df.values.astype('float32')\n",
    "\n",
    "img_width, img_height = 28, 28\n",
    "\n",
    "n_train = x_train.shape[0]\n",
    "n_valid = x_valid.shape[0]\n",
    "\n",
    "n_classes = 10 \n",
    "\n",
    "x_train = x_train.reshape(n_train,1,img_width,img_height)\n",
    "x_valid = x_valid.reshape(n_valid,1,img_width,img_height)\n",
    "\n",
    "x_train = x_train/255 #normalize from [0,255] to [0,1]\n",
    "x_valid = x_valid/255 \n",
    "\n",
    "y_train = to_categorical(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fb34d450-7de9-f245-1201-e25a9363bbda"
   },
   "source": [
    "### View an image to make sure everything is OK\n",
    "(The images are not color, but *imshow()* applies a colormap by default, and I'm not sure how to disable it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "0d5415bc-bced-8800-de41-5ad93637bb32"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/EBQhB8I6n8Bdc6U4MHnnnAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f56226706d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "imgplot = plt.imshow(x_train[4,0,:,:,],cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6a8f5942-fafa-85e0-fbb7-2be7842bfdc3"
   },
   "source": [
    "### Build Model \n",
    "\n",
    "** Handling edges/borders **  \n",
    "One thing we have to decide is how to deal with the edges. To allow convolution of the data at the edges, one can first 'zero pad' the input array, by adding zeros to the left, right, top, and bottom. ie:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;00000<br>\n",
    "123&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;01230<br>\n",
    "456&nbsp;-->&nbsp;04560<br>\n",
    "789&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;07890<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;00000<br> \n",
    "\n",
    "\n",
    "This can be done with the [ZeroPadding2D()](https://keras.io/layers/convolutional/#ZeroPadding2D) function in Keras. One must make sure to zero pad with enough zeros -- one needs *filter_size/2* zeros. \n",
    "\n",
    "Alternatively, a simpler solution is to set *border_mode='same'*, which returns a filter map of the same size and automatically appends zeros. \n",
    "\n",
    "The other option available in Keras is *border_mode='valid'* which only does convolutions where the filter fits inside the image (also called narrow convolution). With this option set, the filter map has smaller dimensions than the input image. \n",
    "\n",
    "**[2D Convolution](https://keras.io/layers/convolutional/#convolution2d)**  \n",
    "The main operation in the CNN. \n",
    "\n",
    "\n",
    "**[Max Pooling](https://keras.io/layers/pooling/#maxpooling2d)**  \n",
    "Max pooling reduces the size of the filter maps, by applying a *max filter* to non-overlapping subregions. A max pooling layer with pooling_size=2 (ie using 2x2 max filters) will reduce the number total number of parameters in the filter map by a factor of 4.\n",
    "\n",
    "**[Dropout](https://keras.io/layers/core/#dropout)**  \n",
    "This is a technique for preventing overfitting. The dropout layer in Keras randomly drops a certain fraction of the neurons (units) with a probability p in each training round. This forces the network to learn redundant representations, and effectively lowers the number of paramters while maintaining a wide degree of flexibility.\n",
    "\n",
    "**[Flattening](https://keras.io/layers/core/#flaten)**  \n",
    "Flattening converts the input activations, which are in an array of shape (n_filters, filter_size_x, filter_size_y) into a 1D array. \n",
    "\n",
    "**[Dense layer](https://keras.io/layers/core/#dense)**  \n",
    "This is a fully connected layer of neurons which works on the 1D input and gives a 1D output. \n",
    "\n",
    "**[Softmax activation](https://en.wikipedia.org/wiki/Softmax_function#Artificial_neural_networks)**  \n",
    "Softmax converts the input  \n",
    "\n",
    "### Hyperparameters for this model\n",
    "\n",
    "* Number of filters (n_filters) \n",
    "\n",
    "* Size of convolution filters (filter_size1, filter_size2)  \n",
    "\n",
    "* Size of pooling windows (pool_size1, pool_size2)\n",
    "\n",
    "* Size of the dense layer (n_dense)\n",
    "\n",
    "### Compilation  \n",
    "The 'compilation' step is where you specify your loss function in Keras. In this case, we use categorical crossentropy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "95dc8409-4741-1b35-677c-77589b616b44"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import *\n",
    "from keras.layers.core import Dropout, Dense, Flatten, Activation\n",
    "\n",
    "n_filters = 64\n",
    "filter_size1 = 3\n",
    "filter_size2 = 2\n",
    "pool_size1 = 3\n",
    "pool_size2 = 1\n",
    "n_dense = 128\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(n_filters, filter_size1, filter_size1, batch_input_shape=(None, 1, img_width, img_height), activation='relu', border_mode='valid'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(pool_size1, pool_size1)))\n",
    "\n",
    "model.add(Convolution2D(n_filters, filter_size2, filter_size2, activation='relu', border_mode='valid'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(pool_size2, pool_size2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(n_dense))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(n_classes))\n",
    "\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5bb63964-a49f-7d80-4978-d3cf334d136d"
   },
   "source": [
    "### Fit the model\n",
    "see *[fit()](https://keras.io/models/sequential/#fit)* documentation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "aa355513-4b10-6650-6f89-cc94c24fde76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/1\n",
      "335s - loss: 0.4335 - acc: 0.8642 - val_loss: 0.1017 - val_acc: 0.9689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f560b567908>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "n_epochs = 1\n",
    "\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=n_epochs,verbose=2,\n",
    "          validation_split=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1eb906ee-61c2-40aa-b54c-ba5209a1a6dd"
   },
   "source": [
    "### Run model on validation data and save output\n",
    "see *[predict_classes()](https://keras.io/models/sequential/#predict_classes)* documentation. \n",
    "\n",
    "(By contrast, *[predict()](https://keras.io/models/sequential/#predict)*  would return an array with shape (n_examples, n_classes), where each number represents a probability for the class in question.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "0297c706-d7b7-d9b4-e213-1ef4dca5296c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000/28000 [==============================] - 148s   \n"
     ]
    }
   ],
   "source": [
    "yPred = model.predict_classes(x_valid,batch_size=32,verbose=1)\n",
    "\n",
    "np.savetxt('mnist_output.csv', np.c_[range(1,len(yPred)+1),yPred], delimiter=',', header = 'ImageId,Label', comments = '', fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 9,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
