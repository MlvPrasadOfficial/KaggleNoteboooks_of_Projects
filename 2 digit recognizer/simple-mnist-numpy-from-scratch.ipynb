{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This note is an MNIST digit recognizer implemented in numpy from scratch.\n",
    "\n",
    "This is a simple demonstration mainly for pedagogical purposes, which shows the basic workflow of a machine learning algorithm using a simple feedforward neural network. The derivative at the backpropagation stage is computed explicitly through the chain rule.\n",
    "\n",
    "* The model is a 3-layer feedforward neural network (FNN), in which the input layer has 784 units, and the 256-unit hidden layer is activated by ReLU, while the output layer is activated by softmax function to produce a discrete probability distribution for each input. \n",
    "* The loss function, model hypothesis function, and the gradient of the loss function are all implemented from ground-up in numpy in a highly vectorized fashion (no FOR loops).\n",
    "* The training is through a standard gradient descent with step size adaptively changing by Root Mean Square prop (RMSprop), and there is no cross-validation set reserved nor model averaging for simplicity.\n",
    "\n",
    "The code is vectorized and is adapted from the Softmax regression and Neural Network lectures used in [UCI Math 10](https://github.com/scaomath/UCI-Math10). \n",
    "\n",
    "Caveat lector: fluency in linear algebra and multivariate calculus.\n",
    "\n",
    "\n",
    "#### References:\n",
    "* [Stanford Deep Learning tutorial in MATLAB](http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/).\n",
    "* [Learning PyTorch with examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html).\n",
    "* [An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train.csv', 'sample_submission.csv', 'test.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = os.listdir(\"../input\")\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "train_data = pd.read_csv('../input/train.csv')\n",
    "test_data = pd.read_csv(\"../input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Set up the data\n",
    "y_train = train_data['label'].values\n",
    "X_train = train_data.drop(columns=['label']).values/255\n",
    "X_test = test_data.values/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some examples of the input data\n",
    "We randomly choose 10 samples from the training set and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2,5, figsize=(12,5))\n",
    "axes = axes.flatten()\n",
    "idx = np.random.randint(0,42000,size=10)\n",
    "for i in range(10):\n",
    "    axes[i].imshow(X_train[idx[i],:].reshape(28,28), cmap='gray')\n",
    "    axes[i].axis('off') # hide the axes ticks\n",
    "    axes[i].set_title(str(int(y_train[idx[i]])), color= 'black', fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network structures\n",
    "\n",
    "<img src=\"https://faculty.sites.uci.edu/shuhaocao/files/2019/06/nn-3layers.png\" alt=\"drawing\" width=\"700\"/>\n",
    "\n",
    "The figure above is a simplication of the neural network used in this example. The circles labeled \"+1\" are the bias units. Layer 1 is the input layer, and Layer 3 is the output layer. The middle layer, Layer 2, is the hidden layer.\n",
    "\n",
    "The neural network in the figure above has 2 input units (not counting the bias unit), 3 hidden units, and 1 output unit. In this actual computation below, the input layer has 784 units, the hidden layer has 256 units, and the output layers has 10 units ($K =10$ classes).\n",
    "\n",
    "The weight matrix $W^{(0)}$ mapping input $\\mathbf{x}$ from the input layer (Layer 1) to the hidden layer (Layer 2) is of shape `(784,256)` together with a `(256,)` bias. Then $\\mathbf{a}$ is the activation from the hidden layer (Layer 2) can be written as:\n",
    "$$\n",
    "\\mathbf{a} = \\mathrm{ReLU}\\big((W^{(0)})^{\\top}\\mathbf{x} + \\mathbf{b}\\big),\n",
    "$$\n",
    "where the ReLU activation function is $\\mathrm{ReLU}(z) = \\max(z,0)$ and can be implemented in a vectorized fashion as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu activation function\n",
    "# THE fastest vectorized implementation for ReLU\n",
    "def relu(x):\n",
    "    x[x<0]=0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax activation, prediction, and the loss function\n",
    "\n",
    "From the hidden layer (Layer 2) to the output layer (layer 3), the weight matrix $W^{(1)}$ is of shape `(256,10)`, the form of which is as follows:\n",
    "$$\n",
    "W^{(1)} =\n",
    "\\begin{pmatrix}\n",
    "| & | & | & | \\\\\n",
    "\\boldsymbol{\\theta}_1 & \\boldsymbol{\\theta}_2 & \\cdots & \\boldsymbol{\\theta}_K \\\\\n",
    "| & | & | & |\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "which maps the activation from Layer 2 to Layer 3 (output layer), and there is no bias because a constant can be freely added to the activation without changing the final output. \n",
    "\n",
    "At the last layer, a softmax activation is used, which can be written as follows combining the weights matrix $W^{(1)}$ that maps the activation $\\mathbf{a}$ from the hidden layer to output layer:\n",
    "$$\n",
    "P\\big(y = k \\;| \\;\\mathbf{a}; W^{(1)}\\big) = \\sigma_k(\\mathbf{a}; W^{(1)}) := \\frac{\\exp\\big(\\boldsymbol{\\theta}^{\\top}_k \\mathbf{a} \\big)}\n",
    "{\\sum_{j=1}^K \\exp\\big(\\boldsymbol{\\theta}^{\\top}_j \\mathbf{a} \\big)}.\n",
    "$$\n",
    "$\\{P\\big(y = k \\;| \\;\\mathbf{a}; W^{(1)}\\big)\\}_{k=1}^K$ is the probability distribution of our model, which estimates the probability of the input $\\mathbf{x}$'s label $y$ is of class $k$. We denote this distribution by a vector \n",
    "$$\\boldsymbol{\\sigma}:= (\\sigma_1,\\dots, \\sigma_K)^{\\top}.$$\n",
    "We hope that this estimate is as close as possible to the true probability: $1_{\\{y=k\\}}$, that is $1$ if the sample $\\mathbf{x}$ is in the $k$-th class and 0 otherwise. \n",
    "\n",
    "Lastly, our prediction $\\hat{y}$ for sample $\\mathbf{x}$ can be made by choosing the class with the highest probability:\n",
    "$$\n",
    "\\hat{y} = \\operatorname{argmax}_{k=1,\\dots,K}  P\\big(y = k \\;| \\;\\mathbf{a}; W^{(1)}\\big). \\tag{$\\ast$}\n",
    "$$\n",
    "\n",
    "Denote the label of the $i$-th input as $y^{(i)}$, and then the sample-wise loss function is the cross entropy measuring the difference of the distribution of this model function above with the true one $1_{\\{y^{(i)}=k\\}}$: denote $W = (W^{(0)}, W^{(1)})$, $b = (\\mathbf{b})$, let $\\mathbf{a}^{(i)}$ be the activation for the $i$-th sample in the hidden layer (Layer 2),\n",
    "$$\n",
    "J_i:= J(W,b;\\mathbf{x}^{(i)},y^{(i)}) := - \\sum_{k=1}^{K} \\left\\{  1_{\\left\\{y^{(i)} = k\\right\\} }\n",
    "\\log P\\big(y^{(i)} = k \\;| \\;\\mathbf{a}^{(i)}; W^{(1)}\\big)\\right\\}. \\tag{1}\n",
    "$$\n",
    "\n",
    "Denote the data sample matrix $X := (\\mathbf{x}^{(1)}, \\dots, \\mathbf{x}^{(N)})^{\\top}$, its label vector as $\\mathbf{y} := (y^{(1)}, \\dots, y^{(N)})$, and then the final loss has an extra $L^2$-regularization term for the weight matrices (not for bias): \n",
    "$$\n",
    "L(W,b; X, \\mathbf{y}) := \\frac{1}{N}\\sum_{i=1}^{N} J_i  + \\frac{\\alpha}{2} \\Big(\\|W^{(0)}\\|^2 + \\|W^{(1)}\\|^2\\Big),\n",
    "\\tag{2}\n",
    "$$\n",
    "where $\\alpha>0$ is a hyper-parameter determining the strength of the regularization, the bigger the $\\alpha$ is, the smaller the magnitudes of the weights will be after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(X,W,b):\n",
    "    '''\n",
    "    Hypothesis function: simple FNN with 1 hidden layer\n",
    "    Layer 1: input\n",
    "    Layer 2: hidden layer, with a size implied by the arguments W[0], b\n",
    "    Layer 3: output layer, with a size implied by the arguments W[1]\n",
    "    '''\n",
    "    # layer 1 = input layer\n",
    "    a1 = X\n",
    "    # layer 1 (input layer) -> layer 2 (hidden layer)\n",
    "    z1 = np.matmul(X, W[0]) + b[0]\n",
    "    \n",
    "    # add one more layer\n",
    "    \n",
    "    # layer 2 activation\n",
    "    a2 = relu(z1)\n",
    "    # layer 2 (hidden layer) -> layer 3 (output layer)\n",
    "    z2 = np.matmul(a2, W[1])\n",
    "    s = np.exp(z2)\n",
    "    total = np.sum(s, axis=1).reshape(-1,1)\n",
    "    sigma = s/total\n",
    "    # the output is a probability for each sample\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X_in,weights):\n",
    "    '''\n",
    "    Un-used cell for demo\n",
    "    activation function for the last FC layer: softmax function \n",
    "    Output: K probabilities represent an estimate of P(y=k|X_in;weights) for k=1,...,K\n",
    "    the weights has shape (n, K)\n",
    "    n: the number of features X_in has\n",
    "    n = X_in.shape[1]\n",
    "    K: the number of classes\n",
    "    K = 10\n",
    "    '''\n",
    "    \n",
    "    s = np.exp(np.matmul(X_in,weights))\n",
    "    total = np.sum(s, axis=1).reshape(-1,1)\n",
    "    return s / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_pred,y_true):\n",
    "    '''\n",
    "    Loss function: cross entropy with an L^2 regularization\n",
    "    y_true: ground truth, of shape (N, )\n",
    "    y_pred: prediction made by the model, of shape (N, K) \n",
    "    N: number of samples in the batch\n",
    "    K: global variable, number of classes\n",
    "    '''\n",
    "    global K \n",
    "    K = 10\n",
    "    N = len(y_true)\n",
    "    # loss_sample stores the cross entropy for each sample in X\n",
    "    # convert y_true from labels to one-hot-vector encoding\n",
    "    y_true_one_hot_vec = (y_true[:,np.newaxis] == np.arange(K))\n",
    "    loss_sample = (np.log(y_pred) * y_true_one_hot_vec).sum(axis=1)\n",
    "    # loss_sample is a dimension (N,) array\n",
    "    # for the final loss, we need take the average\n",
    "    return -np.mean(loss_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation (Chain rule)\n",
    "\n",
    "The derivative of the cross entropy $J$ in (1), for a single sample and its label $(\\mathbf{x}, y)$ , with respect to the weights and the bias is computed using the following procedure:\n",
    "> **Step 1**: Forward pass: computing the activations $\\mathbf{a} = (a_1,\\dots, a_{n_2})$ from the hidden layer (Layer 2), and $\\boldsymbol{\\sigma} = (\\sigma_1,\\dots, \\sigma_K)$ from the output layer (Layer 3). \n",
    ">\n",
    "> **Step 2**: Derivatives for $W^{(1)}$: recall that $W^{(1)} = (\\boldsymbol{\\theta}_1 ,\\cdots,  \\boldsymbol{\\theta}_K)$ and denote \n",
    "$$\\mathbf{z}^{(2)} = \\big(z^{(2)}_1, \\dots, z^{(2)}_K\\big)  = (W^{(1)})^{\\top}\\mathbf{a} =\n",
    "\\big(\\boldsymbol{\\theta}^{\\top}_1 \\mathbf{a} ,\\cdots,  \\boldsymbol{\\theta}^{\\top}_K \\mathbf{a}\\big),$$ \n",
    "for the $k$-th output unit in the output layer (Layer 3), then\n",
    "$$\n",
    "\\delta^{(2)}_k\n",
    ":= \\frac{\\partial J}{\\partial z_k^{(2)}} = \\Big\\{  P\\big(y = k \\;| \\;\\mathbf{a}; W^{(1)}\\big)- 1_{\\{ y = k\\}} \\Big\\} = \\sigma_k - 1_{\\{ y = k\\}}\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\boldsymbol{\\theta}_k}= \\frac{\\partial J}{\\partial z_k^{(2)}}\\frac{\\partial z_k^{(2)}}{\\partial \\boldsymbol{\\theta}_k} = \\delta^{(2)}_k \\mathbf{a}.\n",
    "$$\n",
    ">\n",
    "> **Step 3**: Derivatives for $W^{(0)}$, $\\mathbf{b}$: recall that $W^{(0)} = (\\boldsymbol{w}_1 ,\\cdots,  \\boldsymbol{w}_{n_2})$, $\\mathbf{b} = (b_1,\\dots, b_{n_2})$, where $n_2$ is the number of units in the hidden layer (Layer 2), and denote \n",
    "$$\\mathbf{z}^{(1)} = (z_1^{(1)}, \\dots, z_{n_2}^{(1)})  = (W^{(0)})^{\\top}\\mathbf{x} + \\mathbf{b} =\n",
    "\\big(\\mathbf{w}^{\\top}_1 \\mathbf{x} +b_1 ,\\cdots,  \\mathbf{w}^{\\top}_{n_2} \\mathbf{x} + b_{n_2}\\big),$$ \n",
    "for each node $i$ in the hidden layer (Layer $2$), $i=1,\\dots, n_2$, then\n",
    "$$\\delta^{(1)}_i : = \\frac{\\partial J}{\\partial z^{(1)}_i}  =\n",
    "\\frac{\\partial J}{\\partial a_i} \n",
    "\\frac{\\partial a_i}{\\partial z^{(1)}_i}=\n",
    "\\frac{\\partial J}{\\partial \\mathbf{z}^{(2)}}\n",
    "\\cdot\\left(\\frac{\\partial \\mathbf{z}^{(2)}}{\\partial a_i} \n",
    "\\frac{\\partial a_i}{\\partial z^{(1)}_i}\\right)\n",
    "\\\\\n",
    "=\\left( \\sum_{k=1}^{K} \\frac{\\partial J}{\\partial {z}^{(2)}_k}\n",
    "\\frac{\\partial {z}^{(2)}_k}{\\partial a_i}  \\right) f'(z^{(1)}_i) = \\left( \\sum_{k=1}^{K} w_{ki} \\delta^{(2)}_k \\right) 1_{\\{z^{(1)}_i\\; > 0\\}},\n",
    "$$\n",
    "where $1_{\\{z^{(1)}_i\\; > 0\\}}$ is ReLU activation $f$'s (weak) derivative, and the partial derivative of the $k$-th component (before activated by the softmax) in the output layer ${z}^{(2)}_k$ with respect to the $i$-th activation $a_i$ from the hidden layer is the weight $w^{(1)}_{ki}$. Thus\n",
    ">\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_{ji}}  = x_j \\delta_i^{(1)} ,\\;\n",
    "\\frac{\\partial J}{\\partial b_{i}} = \\delta_i^{(1)}, \\;\\text{ and }\\;\n",
    "\\frac{\\partial J}{\\partial \\mathbf{w}_{i}}  = \\delta_i^{(1)}\\mathbf{x} ,\\;\n",
    "\\frac{\\partial J}{\\partial \\mathbf{b}} = \\boldsymbol{\\delta}^{(1)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(W,b,X,y,alpha=1e-4):\n",
    "    '''\n",
    "    Step 1: explicit forward pass h(X;W,b)\n",
    "    Step 2: backpropagation for dW and db\n",
    "    '''\n",
    "    K = 10\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    ### Step 1:\n",
    "    # layer 1 = input layer\n",
    "    a1 = X\n",
    "    # layer 1 (input layer) -> layer 2 (hidden layer)\n",
    "    z1 = np.matmul(X, W[0]) + b[0]\n",
    "    # layer 2 activation\n",
    "    a2 = relu(z1)\n",
    "    \n",
    "    # one more layer\n",
    "    \n",
    "    # layer 2 (hidden layer) -> layer 3 (output layer)\n",
    "    z2 = np.matmul(a2, W[1])\n",
    "    s = np.exp(z2)\n",
    "    total = np.sum(s, axis=1).reshape(-1,1)\n",
    "    sigma = s/total\n",
    "    \n",
    "    ### Step 2:\n",
    "    \n",
    "    # layer 2->layer 3 weights' derivative\n",
    "    # delta2 is \\partial L/partial z2, of shape (N,K)\n",
    "    y_one_hot_vec = (y[:,np.newaxis] == np.arange(K))\n",
    "    delta2 = (sigma - y_one_hot_vec)\n",
    "    grad_W1 = np.matmul(a2.T, delta2)\n",
    "    \n",
    "    # layer 1->layer 2 weights' derivative\n",
    "    # delta1 is \\partial a2/partial z1\n",
    "    # layer 2 activation's (weak) derivative is 1*(z1>0)\n",
    "    delta1 = np.matmul(delta2, W[1].T)*(z1>0)\n",
    "    grad_W0 = np.matmul(X.T, delta1)\n",
    "    \n",
    "    # Student project: extra layer of derivative\n",
    "    \n",
    "    # no derivative for layer 1\n",
    "    \n",
    "    # the alpha part is the derivative for the regularization\n",
    "    # regularization = 0.5*alpha*(np.sum(W[1]**2) + np.sum(W[0]**2))\n",
    "    \n",
    "    \n",
    "    dW = [grad_W0/N + alpha*W[0], grad_W1/N + alpha*W[1]]\n",
    "    db = [np.mean(delta1, axis=0)]\n",
    "    # dW[0] is W[0]'s derivative, and dW[1] is W[1]'s derivative; similar for db\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters and network initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 5e-1\n",
    "alpha = 1e-6 # regularization\n",
    "gamma = 0.99 # RMSprop\n",
    "eps = 1e-3 # RMSprop\n",
    "num_iter = 2000 # number of iterations of gradient descent\n",
    "n_H = 256 # number of neurons in the hidden layer\n",
    "n = X_train.shape[1] # number of pixels in an image\n",
    "K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "np.random.seed(1127)\n",
    "W = [1e-1*np.random.randn(n, n_H), 1e-1*np.random.randn(n_H, K)]\n",
    "b = [np.random.randn(n_H)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent: training of the network\n",
    "\n",
    "In the training, we use a GD-variant of the RMSprop: for $\\mathbf{w}$ which stands for the parameter vector in our model\n",
    "> Choose $\\mathbf{w}_0$, $\\eta$, $\\gamma$, $\\epsilon$, and let $g_{-1} = 1$ <br><br>\n",
    ">    For $k=0,1,2, \\cdots, M$<br><br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp;  $g_{k} = \\gamma g_{k-1} + (1 - \\gamma)\\, \\left|\\partial_{\\mathbf{w}} L (\\mathbf{w}_k)\\right|^2$<br><br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp;    $\\displaystyle\\mathbf{w}_{k+1} =  \\mathbf{w}_k -  \\frac{\\eta} {\\sqrt{g_{k}+ \\epsilon}} \\partial_{\\mathbf{w}} L(\\mathbf{w}_k)$  \n",
    "\n",
    "### Remark: \n",
    "The training takes a while since we use the gradient descent for all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_kg_hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss after 1 iterations is 7.3264114\n",
      "Training accuracy after 1 iterations is 37.3524%\n",
      "gW0=1.0715 gW1=1.2174 gb0=0.9919\n",
      "etaW0=0.4828 etaW1=0.4530 etab0=0.5018\n",
      "|dW0|=2.85527 |dW1|=4.76880 |db0|=0.43260 \n",
      "\n",
      "Cross-entropy loss after 501 iterations is 0.077221224\n",
      "Training accuracy after 501 iterations is 97.8333%\n",
      "gW0=0.1288 gW1=0.0487 gb0=0.0095\n",
      "etaW0=1.3878 etaW1=2.2437 etab0=4.8738\n",
      "|dW0|=0.01327 |dW1|=0.00698 |db0|=0.00108 \n",
      "\n",
      "Cross-entropy loss after 1001 iterations is 0.053528417\n",
      "Training accuracy after 1001 iterations is 98.5405%\n",
      "gW0=0.1640 gW1=0.0330 gb0=0.0101\n",
      "etaW0=1.2308 etaW1=2.7135 etab0=4.7466\n",
      "|dW0|=0.01254 |dW1|=0.00551 |db0|=0.00110 \n",
      "\n",
      "Cross-entropy loss after 1501 iterations is 0.025509637\n",
      "Training accuracy after 1501 iterations is 99.4524%\n",
      "gW0=0.0595 gW1=0.0156 gb0=0.0078\n",
      "etaW0=2.0321 etaW1=3.8786 etab0=5.3213\n",
      "|dW0|=0.01524 |dW1|=0.00678 |db0|=0.00237 \n",
      "\n",
      "Final cross-entropy loss is 0.027968823\n",
      "Final training accuracy is 99.3476%\n",
      "CPU times: user 1h 43min 11s, sys: 33min 39s, total: 2h 16min 50s\n",
      "Wall time: 35min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gW0 = gW1 = gb0 = 1\n",
    "\n",
    "for i in range(num_iter):\n",
    "    dW, db = backprop(W,b,X_train,y_train,alpha)\n",
    "    \n",
    "    gW0 = gamma*gW0 + (1-gamma)*np.sum(dW[0]**2)\n",
    "    etaW0 = eta/np.sqrt(gW0 + eps)\n",
    "    W[0] -= etaW0 * dW[0]\n",
    "    \n",
    "    gW1 = gamma*gW1 + (1-gamma)*np.sum(dW[1]**2)\n",
    "    etaW1 = eta/np.sqrt(gW1 + eps)\n",
    "    W[1] -= etaW1 * dW[1]\n",
    "    \n",
    "    gb0 = gamma*gb0 + (1-gamma)*np.sum(db[0]**2)\n",
    "    etab0 = eta/np.sqrt(gb0 + eps)\n",
    "    b[0] -= etab0 * db[0]\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        # sanity check 1\n",
    "        y_pred = h(X_train,W,b)\n",
    "        print(\"Cross-entropy loss after\", i+1, \"iterations is {:.8}\".format(\n",
    "              loss(y_pred,y_train)))\n",
    "        print(\"Training accuracy after\", i+1, \"iterations is {:.4%}\".format( \n",
    "              np.mean(np.argmax(y_pred, axis=1)== y_train)))\n",
    "        \n",
    "        # sanity check 2\n",
    "        print(\"gW0={:.4f} gW1={:.4f} gb0={:.4f}\\netaW0={:.4f} etaW1={:.4f} etab0={:.4f}\"\n",
    "              .format(gW0, gW1, gb0, etaW0, etaW1, etab0))\n",
    "        \n",
    "        # sanity check 3\n",
    "        print(\"|dW0|={:.5f} |dW1|={:.5f} |db0|={:.5f}\"\n",
    "             .format(np.linalg.norm(dW[0]), np.linalg.norm(dW[1]), np.linalg.norm(db[0])), \"\\n\")\n",
    "        \n",
    "        # reset RMSprop\n",
    "        gW0 = gW1 = gb0 = 1\n",
    "\n",
    "y_pred_final = h(X_train,W,b)\n",
    "print(\"Final cross-entropy loss is {:.8}\".format(loss(y_pred_final,y_train)))\n",
    "print(\"Final training accuracy is {:.4%}\".format(np.mean(np.argmax(y_pred_final, axis=1)== y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions for testing data\n",
    "The prediction labels are generated by $(\\ast)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "y_pred_test = np.argmax(h(X_test,W,b), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating submission using pandas for grading\n",
    "submission = pd.DataFrame({'ImageId': range(1,len(X_test)+1) ,'Label': y_pred_test })\n",
    "submission.to_csv(\"simplemnist_result.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
