{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "I've made use of some great kernels already - check them out and give them an upvote if any of this is useful!\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "- https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings \n",
    "- https://www.kaggle.com/theoviel/improve-your-score-with-text-preprocessing-v2 \n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "-- https://www.kaggle.com/tunguz/bi-gru-cnn-poolings-gpu-kernel-version\n",
    "\n",
    "### Other\n",
    "\n",
    "- https://www.kaggle.com/dborkan/benchmark-kernel \n",
    "- https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/discussion/87245"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import re\n",
    "import operator \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Input, Dense, CuDNNGRU,concatenate, Bidirectional, SpatialDropout1D, Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data\n",
    "\n",
    "Let's have a little look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1804874, 45)\n",
      "Test shape :  (97320, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>black</th>\n",
       "      <th>buddhist</th>\n",
       "      <th>christian</th>\n",
       "      <th>female</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>hindu</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>jewish</th>\n",
       "      <th>latino</th>\n",
       "      <th>male</th>\n",
       "      <th>muslim</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>other_religion</th>\n",
       "      <th>other_sexual_orientation</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>transgender</th>\n",
       "      <th>white</th>\n",
       "      <th>created_date</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:41.987077+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:42.870083+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:45.222647+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:47.601894+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-09-29 10:50:48.488476+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            ...             toxicity_annotator_count\n",
       "0  59848            ...                                    4\n",
       "1  59849            ...                                    4\n",
       "2  59852            ...                                    4\n",
       "3  59855            ...                                    4\n",
       "4  59856            ...                                   47\n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\n",
    "test = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\n",
    "\n",
    "print(\"Train shape : \",train.shape)\n",
    "print(\"Test shape : \",test.shape)\n",
    "\n",
    "\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7000000</td>\n",
       "      <td>Jeff Sessions is another one of Trump's Orwell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7000001</td>\n",
       "      <td>I actually inspected the infrastructure on Gra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7000002</td>\n",
       "      <td>No it won't . That's just wishful thinking on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7000003</td>\n",
       "      <td>Instead of wringing our hands and nibbling the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7000004</td>\n",
       "      <td>how many of you commenters have garbage piled ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                       comment_text\n",
       "0  7000000  Jeff Sessions is another one of Trump's Orwell...\n",
       "1  7000001  I actually inspected the infrastructure on Gra...\n",
       "2  7000002  No it won't . That's just wishful thinking on ...\n",
       "3  7000003  Instead of wringing our hands and nibbling the...\n",
       "4  7000004  how many of you commenters have garbage piled ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extra features in the training set must be for analysing the bias. This is going to be an interesting competition with such a metric!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only 13GB of ram available, we gotta be careful !\n",
    "\n",
    "df = pd.concat([train[['id','comment_text']], test], axis=0)\n",
    "del(train, test)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "To start we'll just take the FastText Common Crawl embeddings. Later, we'll hopefully combine multiple embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_common_crawl = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n",
    "embeddings_index = KeyedVectors.load_word2vec_format(ft_common_crawl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Text\n",
    "\n",
    "As with most NLP tasks, we will start by using some pre-trained embeddings for our words. This provides us with a numerical representation of our input that we can use for modelling. Mapping words to embeddings isn't always straight forward, however: the data may not be very tidy.\n",
    "\n",
    "The first step, then, is to ensure we get as many words mapped to a suitable embedding as possible. To do this, we'll make use of two excellent kernels:\n",
    "\n",
    "- https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings \n",
    "- https://www.kaggle.com/theoviel/improve-your-score-with-text-preprocessing-v2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coverage(vocab, embeddings_index):\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            nb_known_words += vocab[word]\n",
    "        except:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.3%} of vocab'.format(len(known_words) / len(vocab)))\n",
    "    print('Found embeddings for  {:.3%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return unknown_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will lower() all words, then look up those that do not appear in lower case in the embeddings but do in upper case, and add them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comment_text'] = df['comment_text'].apply(lambda x: x.lower())\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 12.870% of vocab\n",
      "Found embeddings for  91.201% of all text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(\"i'm\", 87786),\n",
       " (\"i've\", 28451),\n",
       " ('\"the', 26467),\n",
       " (\"trump's\", 25755),\n",
       " (\"let's\", 24190),\n",
       " (\"aren't\", 22808),\n",
       " (\"wouldn't\", 22241),\n",
       " ('so,', 21391),\n",
       " (\"wasn't\", 20224),\n",
       " (\"i'd\", 19329)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = build_vocab(df['comment_text'])\n",
    "oov = check_coverage(vocab, embeddings_index)\n",
    "oov[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immediately we see contractions are an issue for FastText (such as \"was not\" -> \"wasn't\"). Let's try and fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del(vocab,oov)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def known_contractions(embed):\n",
    "    known = []\n",
    "    for contract in contraction_mapping:\n",
    "        if contract in embed:\n",
    "            known.append(contract)\n",
    "    return known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Known Contractions -\n",
      "   FastText :\n",
      "[\"can't\", \"'cause\", \"didn't\", \"doesn't\", \"don't\", \"haven't\", \"he's\", \"I'd\", \"I'll\", \"I'm\", \"I've\", \"isn't\", \"it's\", \"she's\", \"that's\", \"there's\", \"they're\", \"we're\", \"won't\", \"you'll\", \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "print(\"- Known Contractions -\")\n",
    "print(\"   FastText :\")\n",
    "print(known_contractions(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_contractions(text, mapping):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment_text'] = df['comment_text'].apply(lambda x: clean_contractions(x, contraction_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 12.945% of vocab\n",
      "Found embeddings for  91.708% of all text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(\"trump's\", 28503),\n",
       " ('\"the', 26467),\n",
       " ('so,', 21391),\n",
       " ('no,', 17856),\n",
       " ('trump.', 17578),\n",
       " ('it?', 12995),\n",
       " ('trump,', 11879),\n",
       " ('but,', 10540),\n",
       " ('fact,', 10043),\n",
       " ('\"i', 9931)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = build_vocab(df['comment_text'])\n",
    "oov = check_coverage(vocab, embeddings_index)\n",
    "oov[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like punctuation is the next issue here, so let's sort it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del(vocab,oov)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unknown_punct(embed, punct):\n",
    "    unknown = ''\n",
    "    for p in punct:\n",
    "        if p not in embed:\n",
    "            unknown += p\n",
    "            unknown += ' '\n",
    "    return unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ ` \n"
     ]
    }
   ],
   "source": [
    "print(unknown_punct(embeddings_index, punct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_mapping = {\"_\":\" \", \"`\":\" \"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_chars(text, punct, mapping):\n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')     \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment_text'] = df['comment_text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 47.128% of vocab\n",
      "Found embeddings for  99.520% of all text\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(df['comment_text'])\n",
    "oov = check_coverage(vocab, embeddings_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('khadr', 6316),\n",
       " ('murkowski', 3601),\n",
       " ('siemian', 2178),\n",
       " ('sb21', 1986),\n",
       " ('mulroney', 1941),\n",
       " ('chretien', 1863),\n",
       " ('notley', 1708),\n",
       " ('theglobeandmail', 1423),\n",
       " ('manafort', 1417),\n",
       " ('djou', 1290),\n",
       " ('gorsuch', 1214),\n",
       " ('alceste', 1169),\n",
       " ('albertans', 1168),\n",
       " ('horgan', 1089),\n",
       " ('begich', 1057),\n",
       " ('sloter', 1034),\n",
       " ('usccb', 1015),\n",
       " ('gabbard', 1003),\n",
       " ('kealoha', 962),\n",
       " ('tfsa', 959),\n",
       " ('kpmg', 949),\n",
       " ('jpii', 898),\n",
       " ('trumpster', 890),\n",
       " ('guptas', 848),\n",
       " ('sb91', 841),\n",
       " ('conoco', 814),\n",
       " ('chaput', 786),\n",
       " ('klastri', 754),\n",
       " ('imua', 754),\n",
       " ('arpaio', 732),\n",
       " ('mcguinty', 701),\n",
       " ('fptp', 664),\n",
       " ('massengill', 662),\n",
       " ('shannyn', 660),\n",
       " ('punahou', 640),\n",
       " ('inouye', 635),\n",
       " ('raitt', 634),\n",
       " ('mulcair', 615),\n",
       " ('anwr', 601),\n",
       " ('ibbitson', 600),\n",
       " ('kapolei', 600),\n",
       " ('heco', 592),\n",
       " ('zinke', 580),\n",
       " ('poloz', 575),\n",
       " ('rhyner', 574),\n",
       " ('monsef', 570),\n",
       " ('coghill', 561),\n",
       " ('mufi', 559),\n",
       " ('sajjan', 548),\n",
       " ('ᴀɴᴅ', 540),\n",
       " ('wohlforth', 538),\n",
       " ('helfrich', 532),\n",
       " ('dprk', 528),\n",
       " ('hanabusa', 525),\n",
       " ('osweiler', 522),\n",
       " ('ontarians', 518),\n",
       " ('cmhc', 509),\n",
       " ('pfds', 504),\n",
       " ('auwe', 500),\n",
       " ('dlnr', 499),\n",
       " ('clallam', 494),\n",
       " ('hirono', 492),\n",
       " ('tokuda', 491),\n",
       " ('lw1', 484),\n",
       " ('nenshi', 481),\n",
       " ('cupich', 473),\n",
       " ('renzetti', 460),\n",
       " ('kakaako', 458),\n",
       " ('zille', 456),\n",
       " ('coupeville', 456),\n",
       " ('scaramucci', 447),\n",
       " ('hilcorp', 447),\n",
       " ('waianae', 442),\n",
       " ('chenault', 442),\n",
       " ('nationalpost', 438),\n",
       " ('giessel', 438),\n",
       " ('mililani', 423),\n",
       " ('hickenlooper', 420),\n",
       " ('prudhoe', 420),\n",
       " ('trumpsters', 416),\n",
       " ('trumpian', 410),\n",
       " ('cayetano', 402),\n",
       " ('southey', 400),\n",
       " ('goodale', 395),\n",
       " ('spenard', 395),\n",
       " ('splc', 394),\n",
       " ('wiliki', 389),\n",
       " ('micciche', 384),\n",
       " ('onkey', 382),\n",
       " ('2gtbpns', 381),\n",
       " ('ramaphosa', 374),\n",
       " ('shibai', 373),\n",
       " ('ᴛʜᴇ', 371),\n",
       " ('rrsps', 365),\n",
       " ('ahca', 365),\n",
       " ('denverpost', 363),\n",
       " ('hilliary', 359),\n",
       " ('m103', 358),\n",
       " ('saullie', 358),\n",
       " ('ʜᴏᴍᴇ', 358)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of words here that just aren't going to have any embeddings. We could go further and try to correct mispellings, but that is likely a small improvement we can worry about when we're trying to improve the model further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del(vocab,oov)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swears\n",
    "\n",
    "Let's replace any swear words we don't have an embedding for with something we do ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "swear_words = [\n",
    "    ' 4r5e ',\n",
    "    ' 5h1t ',\n",
    "    ' 5hit ',\n",
    "    ' a55 ',\n",
    "    ' anal ',\n",
    "    ' anus ',\n",
    "    ' ar5e ',\n",
    "    ' arrse ',\n",
    "    ' arse ',\n",
    "    ' ass ',\n",
    "    ' ass-fucker ',\n",
    "    ' asses ',\n",
    "    ' assfucker ',\n",
    "    ' assfukka ',\n",
    "    ' asshole ',\n",
    "    ' assholes ',\n",
    "    ' asswhole ',\n",
    "    ' a_s_s ',\n",
    "    ' b!tch ',\n",
    "    ' b00bs ',\n",
    "    ' b17ch ',\n",
    "    ' b1tch ',\n",
    "    ' ballbag ',\n",
    "    ' balls ',\n",
    "    ' ballsack ',\n",
    "    ' bastard ',\n",
    "    ' beastial ',\n",
    "    ' beastiality ',\n",
    "    ' bellend ',\n",
    "    ' bestial ',\n",
    "    ' bestiality ',\n",
    "    ' biatch ',\n",
    "    ' bitch ',\n",
    "    ' bitcher ',\n",
    "    ' bitchers ',\n",
    "    ' bitches ',\n",
    "    ' bitchin ',\n",
    "    ' bitching ',\n",
    "    ' bloody ',\n",
    "    ' blow job ',\n",
    "    ' blowjob ',\n",
    "    ' blowjobs ',\n",
    "    ' boiolas ',\n",
    "    ' bollock ',\n",
    "    ' bollok ',\n",
    "    ' boner ',\n",
    "    ' boob ',\n",
    "    ' boobs ',\n",
    "    ' booobs ',\n",
    "    ' boooobs ',\n",
    "    ' booooobs ',\n",
    "    ' booooooobs ',\n",
    "    ' breasts ',\n",
    "    ' buceta ',\n",
    "    ' bugger ',\n",
    "    ' bum ',\n",
    "    ' bunny fucker ',\n",
    "    ' butt ',\n",
    "    ' butthole ',\n",
    "    ' buttmuch ',\n",
    "    ' buttplug ',\n",
    "    ' c0ck ',\n",
    "    ' c0cksucker ',\n",
    "    ' carpet muncher ',\n",
    "    ' cawk ',\n",
    "    ' chink ',\n",
    "    ' cipa ',\n",
    "    ' cl1t ',\n",
    "    ' clit ',\n",
    "    ' clitoris ',\n",
    "    ' clits ',\n",
    "    ' cnut ',\n",
    "    ' cock ',\n",
    "    ' cock-sucker ',\n",
    "    ' cockface ',\n",
    "    ' cockhead ',\n",
    "    ' cockmunch ',\n",
    "    ' cockmuncher ',\n",
    "    ' cocks ',\n",
    "    ' cocksuck ',\n",
    "    ' cocksucked ',\n",
    "    ' cocksucker ',\n",
    "    ' cocksucking ',\n",
    "    ' cocksucks ',\n",
    "    ' cocksuka ',\n",
    "    ' cocksukka ',\n",
    "    ' cok ',\n",
    "    ' cokmuncher ',\n",
    "    ' coksucka ',\n",
    "    ' coon ',\n",
    "    ' cox ',\n",
    "    ' crap ',\n",
    "    ' cum ',\n",
    "    ' cummer ',\n",
    "    ' cumming ',\n",
    "    ' cums ',\n",
    "    ' cumshot ',\n",
    "    ' cunilingus ',\n",
    "    ' cunillingus ',\n",
    "    ' cunnilingus ',\n",
    "    ' cunt ',\n",
    "    ' cuntlick ',\n",
    "    ' cuntlicker ',\n",
    "    ' cuntlicking ',\n",
    "    ' cunts ',\n",
    "    ' cyalis ',\n",
    "    ' cyberfuc ',\n",
    "    ' cyberfuck ',\n",
    "    ' cyberfucked ',\n",
    "    ' cyberfucker ',\n",
    "    ' cyberfuckers ',\n",
    "    ' cyberfucking ',\n",
    "    ' d1ck ',\n",
    "    ' damn ',\n",
    "    ' dick ',\n",
    "    ' dickhead ',\n",
    "    ' dildo ',\n",
    "    ' dildos ',\n",
    "    ' dink ',\n",
    "    ' dinks ',\n",
    "    ' dirsa ',\n",
    "    ' dlck ',\n",
    "    ' dog-fucker ',\n",
    "    ' doggin ',\n",
    "    ' dogging ',\n",
    "    ' donkeyribber ',\n",
    "    ' doosh ',\n",
    "    ' duche ',\n",
    "    ' dyke ',\n",
    "    ' ejaculate ',\n",
    "    ' ejaculated ',\n",
    "    ' ejaculates ',\n",
    "    ' ejaculating ',\n",
    "    ' ejaculatings ',\n",
    "    ' ejaculation ',\n",
    "    ' ejakulate ',\n",
    "    ' f u c k ',\n",
    "    ' f u c k e r ',\n",
    "    ' f4nny ',\n",
    "    ' fag ',\n",
    "    ' fagging ',\n",
    "    ' faggitt ',\n",
    "    ' faggot ',\n",
    "    ' faggs ',\n",
    "    ' fagot ',\n",
    "    ' fagots ',\n",
    "    ' fags ',\n",
    "    ' fanny ',\n",
    "    ' fannyflaps ',\n",
    "    ' fannyfucker ',\n",
    "    ' fanyy ',\n",
    "    ' fatass ',\n",
    "    ' fcuk ',\n",
    "    ' fcuker ',\n",
    "    ' fcuking ',\n",
    "    ' feck ',\n",
    "    ' fecker ',\n",
    "    ' felching ',\n",
    "    ' fellate ',\n",
    "    ' fellatio ',\n",
    "    ' fingerfuck ',\n",
    "    ' fingerfucked ',\n",
    "    ' fingerfucker ',\n",
    "    ' fingerfuckers ',\n",
    "    ' fingerfucking ',\n",
    "    ' fingerfucks ',\n",
    "    ' fistfuck ',\n",
    "    ' fistfucked ',\n",
    "    ' fistfucker ',\n",
    "    ' fistfuckers ',\n",
    "    ' fistfucking ',\n",
    "    ' fistfuckings ',\n",
    "    ' fistfucks ',\n",
    "    ' flange ',\n",
    "    ' fook ',\n",
    "    ' fooker ',\n",
    "    ' fuck ',\n",
    "    ' fucka ',\n",
    "    ' fucked ',\n",
    "    ' fucker ',\n",
    "    ' fuckers ',\n",
    "    ' fuckhead ',\n",
    "    ' fuckheads ',\n",
    "    ' fuckin ',\n",
    "    ' fucking ',\n",
    "    ' fuckings ',\n",
    "    ' fuckingshitmotherfucker ',\n",
    "    ' fuckme ',\n",
    "    ' fucks ',\n",
    "    ' fuckwhit ',\n",
    "    ' fuckwit ',\n",
    "    ' fudge packer ',\n",
    "    ' fudgepacker ',\n",
    "    ' fuk ',\n",
    "    ' fuker ',\n",
    "    ' fukker ',\n",
    "    ' fukkin ',\n",
    "    ' fuks ',\n",
    "    ' fukwhit ',\n",
    "    ' fukwit ',\n",
    "    ' fux ',\n",
    "    ' fux0r ',\n",
    "    ' f_u_c_k ',\n",
    "    ' gangbang ',\n",
    "    ' gangbanged ',\n",
    "    ' gangbangs ',\n",
    "    ' gaylord ',\n",
    "    ' gaysex ',\n",
    "    ' goatse ',\n",
    "    ' God ',\n",
    "    ' god-dam ',\n",
    "    ' god-damned ',\n",
    "    ' goddamn ',\n",
    "    ' goddamned ',\n",
    "    ' hardcoresex ',\n",
    "    ' hell ',\n",
    "    ' heshe ',\n",
    "    ' hoar ',\n",
    "    ' hoare ',\n",
    "    ' hoer ',\n",
    "    ' homo ',\n",
    "    ' hore ',\n",
    "    ' horniest ',\n",
    "    ' horny ',\n",
    "    ' hotsex ',\n",
    "    ' jack-off ',\n",
    "    ' jackoff ',\n",
    "    ' jap ',\n",
    "    ' jerk-off ',\n",
    "    ' jism ',\n",
    "    ' jiz ',\n",
    "    ' jizm ',\n",
    "    ' jizz ',\n",
    "    ' kawk ',\n",
    "    ' knob ',\n",
    "    ' knobead ',\n",
    "    ' knobed ',\n",
    "    ' knobend ',\n",
    "    ' knobhead ',\n",
    "    ' knobjocky ',\n",
    "    ' knobjokey ',\n",
    "    ' kock ',\n",
    "    ' kondum ',\n",
    "    ' kondums ',\n",
    "    ' kum ',\n",
    "    ' kummer ',\n",
    "    ' kumming ',\n",
    "    ' kums ',\n",
    "    ' kunilingus ',\n",
    "    ' l3itch ',\n",
    "    ' labia ',\n",
    "    ' lmfao ',\n",
    "    ' lust ',\n",
    "    ' lusting ',\n",
    "    ' m0f0 ',\n",
    "    ' m0fo ',\n",
    "    ' m45terbate ',\n",
    "    ' ma5terb8 ',\n",
    "    ' ma5terbate ',\n",
    "    ' masochist ',\n",
    "    ' master-bate ',\n",
    "    ' masterb8 ',\n",
    "    ' masterbat3 ',\n",
    "    ' masterbate ',\n",
    "    ' masterbation ',\n",
    "    ' masterbations ',\n",
    "    ' masturbate ',\n",
    "    ' mo-fo ',\n",
    "    ' mof0 ',\n",
    "    ' mofo ',\n",
    "    ' mothafuck ',\n",
    "    ' mothafucka ',\n",
    "    ' mothafuckas ',\n",
    "    ' mothafuckaz ',\n",
    "    ' mothafucked ',\n",
    "    ' mothafucker ',\n",
    "    ' mothafuckers ',\n",
    "    ' mothafuckin ',\n",
    "    ' mothafucking ',\n",
    "    ' mothafuckings ',\n",
    "    ' mothafucks ',\n",
    "    ' mother fucker ',\n",
    "    ' motherfuck ',\n",
    "    ' motherfucked ',\n",
    "    ' motherfucker ',\n",
    "    ' motherfuckers ',\n",
    "    ' motherfuckin ',\n",
    "    ' motherfucking ',\n",
    "    ' motherfuckings ',\n",
    "    ' motherfuckka ',\n",
    "    ' motherfucks ',\n",
    "    ' muff ',\n",
    "    ' mutha ',\n",
    "    ' muthafecker ',\n",
    "    ' muthafuckker ',\n",
    "    ' muther ',\n",
    "    ' mutherfucker ',\n",
    "    ' n1gga ',\n",
    "    ' n1gger ',\n",
    "    ' nazi ',\n",
    "    ' nigg3r ',\n",
    "    ' nigg4h ',\n",
    "    ' nigga ',\n",
    "    ' niggah ',\n",
    "    ' niggas ',\n",
    "    ' niggaz ',\n",
    "    ' nigger ',\n",
    "    ' niggers ',\n",
    "    ' nob ',\n",
    "    ' nob jokey ',\n",
    "    ' nobhead ',\n",
    "    ' nobjocky ',\n",
    "    ' nobjokey ',\n",
    "    ' numbnuts ',\n",
    "    ' nutsack ',\n",
    "    ' orgasim ',\n",
    "    ' orgasims ',\n",
    "    ' orgasm ',\n",
    "    ' orgasms ',\n",
    "    ' p0rn ',\n",
    "    ' pawn ',\n",
    "    ' pecker ',\n",
    "    ' penis ',\n",
    "    ' penisfucker ',\n",
    "    ' phonesex ',\n",
    "    ' phuck ',\n",
    "    ' phuk ',\n",
    "    ' phuked ',\n",
    "    ' phuking ',\n",
    "    ' phukked ',\n",
    "    ' phukking ',\n",
    "    ' phuks ',\n",
    "    ' phuq ',\n",
    "    ' pigfucker ',\n",
    "    ' pimpis ',\n",
    "    ' piss ',\n",
    "    ' pissed ',\n",
    "    ' pisser ',\n",
    "    ' pissers ',\n",
    "    ' pisses ',\n",
    "    ' pissflaps ',\n",
    "    ' pissin ',\n",
    "    ' pissing ',\n",
    "    ' pissoff ',\n",
    "    ' poop ',\n",
    "    ' porn ',\n",
    "    ' porno ',\n",
    "    ' pornography ',\n",
    "    ' pornos ',\n",
    "    ' prick ',\n",
    "    ' pricks ',\n",
    "    ' pron ',\n",
    "    ' pube ',\n",
    "    ' pusse ',\n",
    "    ' pussi ',\n",
    "    ' pussies ',\n",
    "    ' pussy ',\n",
    "    ' pussys ',\n",
    "    ' rectum ',\n",
    "    ' retard ',\n",
    "    ' rimjaw ',\n",
    "    ' rimming ',\n",
    "    ' s hit ',\n",
    "    ' s.o.b. ',\n",
    "    ' sadist ',\n",
    "    ' schlong ',\n",
    "    ' screwing ',\n",
    "    ' scroat ',\n",
    "    ' scrote ',\n",
    "    ' scrotum ',\n",
    "    ' semen ',\n",
    "    ' sex ',\n",
    "    ' sh!t ',\n",
    "    ' sh1t ',\n",
    "    ' shag ',\n",
    "    ' shagger ',\n",
    "    ' shaggin ',\n",
    "    ' shagging ',\n",
    "    ' shemale ',\n",
    "    ' shit ',\n",
    "    ' shitdick ',\n",
    "    ' shite ',\n",
    "    ' shited ',\n",
    "    ' shitey ',\n",
    "    ' shitfuck ',\n",
    "    ' shitfull ',\n",
    "    ' shithead ',\n",
    "    ' shiting ',\n",
    "    ' shitings ',\n",
    "    ' shits ',\n",
    "    ' shitted ',\n",
    "    ' shitter ',\n",
    "    ' shitters ',\n",
    "    ' shitting ',\n",
    "    ' shittings ',\n",
    "    ' shitty ',\n",
    "    ' skank ',\n",
    "    ' slut ',\n",
    "    ' sluts ',\n",
    "    ' smegma ',\n",
    "    ' smut ',\n",
    "    ' snatch ',\n",
    "    ' son-of-a-bitch ',\n",
    "    ' spac ',\n",
    "    ' spunk ',\n",
    "    ' s_h_i_t ',\n",
    "    ' t1tt1e5 ',\n",
    "    ' t1tties ',\n",
    "    ' teets ',\n",
    "    ' teez ',\n",
    "    ' testical ',\n",
    "    ' testicle ',\n",
    "    ' tit ',\n",
    "    ' titfuck ',\n",
    "    ' tits ',\n",
    "    ' titt ',\n",
    "    ' tittie5 ',\n",
    "    ' tittiefucker ',\n",
    "    ' titties ',\n",
    "    ' tittyfuck ',\n",
    "    ' tittywank ',\n",
    "    ' titwank ',\n",
    "    ' tosser ',\n",
    "    ' turd ',\n",
    "    ' tw4t ',\n",
    "    ' twat ',\n",
    "    ' twathead ',\n",
    "    ' twatty ',\n",
    "    ' twunt ',\n",
    "    ' twunter ',\n",
    "    ' v14gra ',\n",
    "    ' v1gra ',\n",
    "    ' vagina ',\n",
    "    ' viagra ',\n",
    "    ' vulva ',\n",
    "    ' w00se ',\n",
    "    ' wang ',\n",
    "    ' wank ',\n",
    "    ' wanker ',\n",
    "    ' wanky ',\n",
    "    ' whoar ',\n",
    "    ' whore ',\n",
    "    ' willies ',\n",
    "    ' willy ',\n",
    "    ' xrated ',\n",
    "    ' xxx '    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 4r5e | 5h1t | 5hit | ass-fucker | assfucker | assfukka | asswhole | a_s_s | b!tch | b17ch | blow job | boiolas | bollok | boooobs | booooobs | booooooobs | bunny fucker | buttmuch | c0cksucker | carpet muncher | cl1t | cockface | cockmunch | cockmuncher | cocksuka | cocksukka | cokmuncher | coksucka | cunillingus | cuntlick | cuntlicker | cuntlicking | cyalis | cyberfuc | cyberfuck | cyberfucked | cyberfucker | cyberfuckers | cyberfucking | dirsa | dlck | dog-fucker | donkeyribber | ejaculatings | ejakulate | f u c k | f u c k e r | f4nny | faggitt | faggs | fannyflaps | fannyfucker | fanyy | fingerfucker | fingerfuckers | fingerfucks | fistfuck | fistfucked | fistfucker | fistfuckers | fistfucking | fistfuckings | fistfucks | fuckingshitmotherfucker | fuckwhit | fudge packer | fudgepacker | fukwhit | fukwit | fux0r | f_u_c_k | god-dam | kawk | knobead | knobed | knobend | knobjocky | knobjokey | kondum | kondums | kummer | kumming | kums | kunilingus | l3itch | m0f0 | m0fo | m45terbate | ma5terb8 | ma5terbate | master-bate | masterb8 | masterbat3 | masterbations | mof0 | mothafuck | mothafuckaz | mothafucked | mothafucking | mothafuckings | mothafucks | mother fucker | motherfucked | motherfuckings | motherfuckka | motherfucks | muthafecker | muthafuckker | n1gga | n1gger | nigg3r | nigg4h | nob jokey | nobjocky | nobjokey | penisfucker | phuked | phuking | phukked | phukking | phuks | phuq | pigfucker | pimpis | pissflaps | rimjaw | s hit | scroat | sh!t | shitdick | shitfull | shitings | shittings | s_h_i_t | t1tt1e5 | t1tties | teez | tittie5 | tittiefucker | tittywank | tw4t | twathead | twunter | v14gra | v1gra | w00se | whoar '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_with_fuck = []\n",
    "\n",
    "for swear in swear_words:\n",
    "    if swear[1:(len(swear)-1)] not in embeddings_index:\n",
    "        replace_with_fuck.append(swear)\n",
    "        \n",
    "replace_with_fuck = '|'.join(replace_with_fuck)\n",
    "replace_with_fuck\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_swears(text):\n",
    "    text = re.sub(replace_with_fuck, ' fuck ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comment_text'] = df['comment_text'].apply(lambda x: handle_swears(x))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data back into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>this is so cool .  it is like ,    '  would yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>thank you !  !  this would make my life a lot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>this is such an urgent design problem ;  kudos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>is this something i will be able to install on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>haha you guys are a bunch of losers .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                       comment_text\n",
       "0  59848  this is so cool .  it is like ,    '  would yo...\n",
       "1  59849  thank you !  !  this would make my life a lot ...\n",
       "2  59852  this is such an urgent design problem ;  kudos...\n",
       "3  59855  is this something i will be able to install on...\n",
       "4  59856             haha you guys are a bunch of losers . "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = df.iloc[:1804874,:]\n",
    "test = df.iloc[1804874:,:]\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del(df)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>this is so cool .  it is like ,    '  would yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>thank you !  !  this would make my life a lot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>this is such an urgent design problem ;  kudos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>is this something i will be able to install on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>haha you guys are a bunch of losers .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                       comment_text\n",
       "0  59848  this is so cool .  it is like ,    '  would yo...\n",
       "1  59849  thank you !  !  this would make my life a lot ...\n",
       "2  59852  this is such an urgent design problem ;  kudos...\n",
       "3  59855  is this something i will be able to install on...\n",
       "4  59856             haha you guys are a bunch of losers . "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>black</th>\n",
       "      <th>buddhist</th>\n",
       "      <th>christian</th>\n",
       "      <th>female</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>hindu</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>jewish</th>\n",
       "      <th>latino</th>\n",
       "      <th>male</th>\n",
       "      <th>muslim</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>other_religion</th>\n",
       "      <th>other_sexual_orientation</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>transgender</th>\n",
       "      <th>white</th>\n",
       "      <th>created_date</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:41.987077+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:42.870083+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:45.222647+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:47.601894+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-09-29 10:50:48.488476+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            ...             toxicity_annotator_count\n",
       "0  59848            ...                                    4\n",
       "1  59849            ...                                    4\n",
       "2  59852            ...                                    4\n",
       "3  59855            ...                                    4\n",
       "4  59856            ...                                   47\n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_orig = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\n",
    "train_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>this is so cool .  it is like ,    '  would yo...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>thank you !  !  this would make my life a lot ...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>this is such an urgent design problem ;  kudos...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>is this something i will be able to install on...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>haha you guys are a bunch of losers .</td>\n",
       "      <td>0.893617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                       comment_text    target\n",
       "0  59848  this is so cool .  it is like ,    '  would yo...  0.000000\n",
       "1  59849  thank you !  !  this would make my life a lot ...  0.000000\n",
       "2  59852  this is such an urgent design problem ;  kudos...  0.000000\n",
       "3  59855  is this something i will be able to install on...  0.000000\n",
       "4  59856             haha you guys are a bunch of losers .   0.893617"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.concat([train,train_orig[['target']]],axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del(train_orig)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert target to binary flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['target'] = np.where(train['target'] >= 0.5, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train/validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1624386 train comments, 180488 validate comments\n"
     ]
    }
   ],
   "source": [
    "train_df, validate_df = model_selection.train_test_split(train, test_size=0.1)\n",
    "print('%d train comments, %d validate comments' % (len(train_df), len(validate_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 100000\n",
    "TOXICITY_COLUMN = 'target'\n",
    "TEXT_COLUMN = 'comment_text'\n",
    "\n",
    "# Create a text tokenizer.\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_df[TEXT_COLUMN])\n",
    "\n",
    "# All comments must be truncated or padded to be the same length.\n",
    "MAX_SEQUENCE_LENGTH = 256\n",
    "def pad_text(texts, tokenizer):\n",
    "    return pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_DIMENSION = 300\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1,EMBEDDINGS_DIMENSION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words_in_embedding = 0\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in embeddings_index.vocab:\n",
    "        embedding_vector = embeddings_index[word]\n",
    "        embedding_matrix[i] = embedding_vector        \n",
    "        num_words_in_embedding += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = pad_text(train_df[TEXT_COLUMN], tokenizer)\n",
    "train_labels = train_df[TOXICITY_COLUMN]\n",
    "validate_text = pad_text(validate_df[TEXT_COLUMN], tokenizer)\n",
    "validate_labels = validate_df[TOXICITY_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "\n",
    "Adding dropout / 1d conv / concatenated poolings based on the architecture presented @ https://www.kaggle.com/tunguz/bi-gru-cnn-poolings-gpu-kernel-version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 256, 300)     90345600    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 256, 300)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 256, 128)     140544      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 255, 64)      16448       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 64)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 64)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            129         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 90,502,721\n",
      "Trainable params: 157,121\n",
      "Non-trainable params: 90,345,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedding_layer = Embedding(len(tokenizer.word_index) + 1,\n",
    "                            EMBEDDINGS_DIMENSION,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "x = embedding_layer(sequence_input)\n",
    "x = SpatialDropout1D(0.2)(x)\n",
    "x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)   \n",
    "x = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
    "\n",
    "avg_pool1 = GlobalAveragePooling1D()(x)\n",
    "max_pool1 = GlobalMaxPooling1D()(x)     \n",
    "\n",
    "x = concatenate([avg_pool1, max_pool1])\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "NUM_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 1624386 samples, validate on 180488 samples\n",
      "Epoch 1/100\n",
      "1624386/1624386 [==============================] - 181s 111us/step - loss: 0.1467 - acc: 0.9443 - val_loss: 0.1248 - val_acc: 0.9513\n",
      "Epoch 2/100\n",
      "1624386/1624386 [==============================] - 179s 110us/step - loss: 0.1269 - acc: 0.9499 - val_loss: 0.1194 - val_acc: 0.9525\n",
      "Epoch 3/100\n",
      "1624386/1624386 [==============================] - 179s 110us/step - loss: 0.1219 - acc: 0.9515 - val_loss: 0.1182 - val_acc: 0.9523\n",
      "Epoch 4/100\n",
      "1076224/1624386 [==================>...........] - ETA: 57s - loss: 0.1188 - acc: 0.9526"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_text,\n",
    "    train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    validation_data=(validate_text, validate_labels),\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict & Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's submit this as our first submission. Once we have a reasonable pipeline setup, we can move on to looking at the competition metric in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7000000</td>\n",
       "      <td>0.003056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7000001</td>\n",
       "      <td>0.000150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7000002</td>\n",
       "      <td>0.007791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7000003</td>\n",
       "      <td>0.002761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7000004</td>\n",
       "      <td>0.976216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  prediction\n",
       "0  7000000    0.003056\n",
       "1  7000001    0.000150\n",
       "2  7000002    0.007791\n",
       "3  7000003    0.002761\n",
       "4  7000004    0.976216"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv', index_col='id')\n",
    "submission['prediction'] = model.predict(pad_text(test[TEXT_COLUMN], tokenizer))\n",
    "submission.reset_index(drop=False, inplace=True)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
