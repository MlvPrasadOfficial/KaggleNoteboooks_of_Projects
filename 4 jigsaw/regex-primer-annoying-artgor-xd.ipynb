{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - This has been created by using Past Quora comps + Artgor's kernel ! \n",
    " Thanks a lot :)\n",
    " - PS  The goal isn't really to beat Artgor's baseline, he can well beat me anytime but to help everyone in picking up Regex mainly!\n",
    " \n",
    " > Few Kernels used\n",
    ">> - https://www.kaggle.com/theoviel/improve-your-score-with-text-preprocessing-v2\n",
    ">> - https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings\n",
    ">> - https://www.kaggle.com/taindow/simple-cudnngru-python-keras\n",
    ">> - https://www.kaggle.com/artgor/basic-cnn-in-keras\n",
    ">> - https://www.kaggle.com/takuok/bidirectional-lstm-and-attention-lb-0-043/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-crawl-300d-2m', 'glove840b300dtxt', 'jigsaw-unintended-bias-in-toxicity-classification']\n",
      "number of cores: 2\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "pd.set_option('max_colwidth',400)\n",
    "\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "import random\n",
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed(2411)\n",
    "SEED = 42\n",
    "import psutil\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "\n",
    "num_partitions = 10  # number of partitions to split dataframe\n",
    "num_cores = psutil.cpu_count()  # number of cores on your machine\n",
    "\n",
    "print('number of cores:', num_cores)\n",
    "\n",
    "def df_parallelize_run(df, func):\n",
    "    \n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> A Tutorial On Understanding ([Rr]ege)(x|xp|xes|xps|xen)</center>\n",
    "\n",
    "<center> A common workflow with regular expressions is that you write a pattern for the thing you are looking for... <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's see our very first expression\n",
    "\n",
    "- **<\\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,4}\\b>**  It's a complex pattern as it includes lots of things like \n",
    "    - Character Class\n",
    "    - Alphabets\n",
    "    - Percentage\n",
    "    - Numbers\n",
    "    - Underscores\n",
    "    - $\\{\\}$, word Boundaries etc...\n",
    "    \n",
    "> In short  - **this pattern describes an email address; With the above regex pattern, we can search through a text file to find email addresses, or verify if a given string looks like an email address..**\n",
    "\n",
    "The most basic regex pattern in a token like just an $<b>$ i.e a single literal character. In the string **\" Zebra is an animal.\"**, this will match the very first $b$ in the **Ze$b$** Note that it doesn't matter whether it's present in the middle of the word as of now..\n",
    "\n",
    "Now let me introduce few very basics things used in $<regex>$ to define itself (remeber the e-mail address pattern above, now we will break into piece by piece..)\n",
    "\n",
    "In the regex discussed in this tutorial, there are **11** characters with special meanings:\n",
    "the opening square bracket $<[>$, the backslash, the caret <^>, the dollar sign <$>, the period or dot <.>, the\n",
    "vertical bar or pipe symbol <|>, the question mark <?>, the asterisk or star <*>, the plus sign <+>, the opening\n",
    "round bracket <(> and the closing round bracket <)>. These special characters are often called **‚Äúmetacharacters‚Äù**.\n",
    "\n",
    "|Meta character|Description|\n",
    "|:----:|----|\n",
    "|**.**|<b>Period matches any single character except a line break.<b>|\n",
    "|**[ ]**|<b>Character class. Matches any character contained between the square brackets.<b>|\n",
    "|**[^ ]**|<b>Negated character class. Matches any character that is not contained between the square brackets <b>.|\n",
    "|*****|<b>Matches 0 or more repetitions of the preceding symbol.<b>|\n",
    "|**+**|<b>Matches 1 or more repetitions of the preceding symbol.<b>|\n",
    "|**?**|<b>Makes the preceding symbol optional.<b>|\n",
    "|**{n,m}**|<b>Braces. Matches at least \"n\" but not more than \"m\" repetitions of the preceding symbol.<b>|\n",
    "|**(xyz)**|<b>Character group. Matches the characters xyz in that exact order.<b>|\n",
    "|**&#124;**|<b>Alternation. Matches either the characters before or the characters after the symbol.<b>|\n",
    "|**&#92;**|<b>Escapes the next character. This allows you to match reserved characters `[ ] ( ) { } . * + ? ^ $ \\`.<b>| \n",
    "|**^**|<b>Matches the beginning of the input.<b>|\n",
    "|**$**|<b>Matches the end of the input.<b>|\n",
    "\n",
    "\n",
    "Read More [here](http://www.rexegg.com/regex-quickstart.html#chars) and [here](http://www.greenend.org.uk/rjk/tech/regexp.html). Both are Very Very Good...\n",
    "\n",
    "- Example  - If you want to use any of these characters as a literal in a regex, you need to escape them with a backslash. If\n",
    "you want to match **<1+1=2>**, the correct regex is $1\\+1=2$. Otherwise, the plus sign will have a special meaning. **Note** that **<1+1=2>**, with the *backslash omitted*, is a **valid** regex. So you will **not** get an error message. But it\n",
    "will not match **<1+1=2>**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Regex-Directed Engine Always Returns the Left-most Match\n",
    "This is a very important point to understand: a regex-directed engine will always return the leftmost match,\n",
    "even if a **better** match could be found later. When applying a regex to a string, the engine will start at the\n",
    "first character of the string. It will try all possible permutations of the regular expression at the first character.\n",
    "Only if all possibilities have been tried and found to fail, will the engine continue with the second character in\n",
    "the text. Again, it will try all possible permutations of the regex, in exactly the same order. The result is that\n",
    "the regex-directed engine will return the leftmost match.\n",
    "- When applying **<cat\\>** to **He captured a catfish for his cat.**, the engine will try to match the first\n",
    "token in the regex **<c\\>** to the first character in the match **H**. This fails. There are no other possible\n",
    "permutations of this regex, because it merely consists of a sequence of literal characters. So the regex engine\n",
    "tries to match the **<c\\>** with the **e**. This fails too, as does matching the **c** with the space. Arriving at the 4th\n",
    "character in the match, **<c\\>** matches **c**. The engine will then try to match the second token **<a\\>** to the 5th\n",
    "character, **a**. This succeeds too. But then, **<t\\>** fails to match **p**. At that point, the engine knows the regex\n",
    "cannot be matched starting at the 4th character in the match. So it will continue with the 5th: **a**. Again, **<c\\>**\n",
    "fails to match here and the engine carries on. At the 15th character in the match, **<c\\>** again matches **c**. The\n",
    "engine then proceeds to attempt to match the remainder of the regex at character 15 and finds that **<a\\>**\n",
    "matches **a** and **<t\\>** matches **t**.\n",
    "\n",
    "- The entire regular expression could be matched starting at character 15. The engine is **\"eager\"** to report a\n",
    "match. **It will therefore report the first three letters of catfish as a valid match**. The engine **never** proceeds\n",
    "beyond this point to see if there are any **better** matches. The *first match* is considered good enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex's Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Sets/Classes\n",
    "Character sets are also called character class. **Square brackets** are used to specify character sets. Use a **hyphen** inside a character set to specify the characters' range. The order of the character range inside square brackets doesn't matter. For example, the regular expression `[Tt]he` means: `an uppercase T or lowercase t, followed by the letter h, followed by the letter e.`\n",
    "\n",
    "- **<[Tt]he>** => <font color=red>The</font> car parked in <font color=red>the</font> garage.\n",
    "\n",
    "A period inside a character set, however, means a literal period. The regular expression **<ar[.]>** means: a lowercase character a, followed by letter r, followed by a period **.** character.\n",
    "\n",
    "- **<ar[.]>** => A garage is a good place to park a c<font color=red>ar.</font>\n",
    "\n",
    "- **<[0-9]>** => Matches a **single digit between 0 and 9**. You can use more than one range.\n",
    "- **<[0-9a-fA-F]>** => Matches a **single hexadecimal digit**, case insensitively. \n",
    "- You can combine ranges and single characters. **<[0-9a-fxA-FX]>** matches a hexadecimal digit or the letter X.* Again, the order of the characters and the ranges does not matter.*\n",
    "- Find a word, even if it is misspelled, such as **<sep[ae]r[ae]te>** or **<li[cs]en[cs]e>**. \n",
    "\n",
    "### Negated Character Sets/Classes\n",
    "\n",
    "Typing a **caret(^)** after the opening square bracket will negate the character class. **The result is that the character\n",
    "class will match any character that is <font color = red>not </font> in the character class.**\n",
    "- It is important to remember that a negated character class **still must match a character**. **<q[^u]>** does not\n",
    "mean: **<font color= red> a q not followed by a u </font>**. It means: **<font color= red a q followed by a character that is not a u </font>**. It will **not** match the\n",
    "$q$ in the string $Iraq$. It will match the $q$ and $the space$ after the $q$ in **Iraq is a country**.\n",
    "\n",
    "###  Shorthand Character Sets\n",
    "\n",
    "Regular expression provides **shorthands** for the commonly used character sets,\n",
    "which offer **convenient shorthands** for commonly used regular expressions. The\n",
    "shorthand character sets are as follows:\n",
    "\n",
    "|Shorthand|Description|\n",
    "|:----:|----|\n",
    "|<b>.<b>|<b>Any character except new line. It's the most commonly misused metacharacter.<b>|\n",
    "|<b>\\w<b>|<b>Matches alphanumeric characters: `[a-zA-Z0-9_]`<b>|\n",
    "|<b>\\W<b>|<b>Matches non-alphanumeric characters: `[^\\w]`<b>|\n",
    "|<b>\\d<b>|<b>Matches digit: `[0-9]`<b>|\n",
    "|<b>\\D<b>|<b>Matches non-digit: `[^\\d]`<b>|\n",
    "|<b>\\s<b>|<b>Matches whitespace character: `[\\t\\n\\f\\r\\p{Z}]`<b>|\n",
    "|<b>\\S<b>|<b>Matches non-whitespace character: `[^\\s]`<b>|\n",
    "    \n",
    "## Repetitions\n",
    "\n",
    "Following meta characters `+`, `*` or `?` are used to specify how many times a\n",
    "subpattern can occur. These meta characters act differently in different\n",
    "situations.\n",
    "\n",
    "### The Star *\n",
    "\n",
    "The symbol `*` matches zero or more repetitions of the preceding matcher. The\n",
    "regular expression `a*` means: zero or more repetitions of preceding lowercase\n",
    "character `a`. But if it appears after a character set or class then it finds\n",
    "the repetitions of the whole character set. \n",
    "For example, the regular expression\n",
    "- `[a-z]*` means: any number of lowercase letters in a row.\n",
    "\n",
    "The `*` symbol can be used with the meta character `.` to match any string of\n",
    "characters `.*`. The `*` symbol can be used with the whitespace character `\\s`\n",
    "to match a string of whitespace characters. For example, the expression\n",
    "`\\s*cat\\s*` means: zero or more spaces, followed by lowercase character `c`,\n",
    "followed by lowercase character `a`, followed by lowercase character `t`,\n",
    "followed by zero or more spaces.\n",
    "\n",
    "### The Plus +\n",
    "\n",
    "The symbol `+` matches one or more repetitions of the preceding character. For\n",
    "example, the regular expression `c.+t` means: lowercase letter `c`, followed by\n",
    "at least one character, followed by the lowercase character `t`. It needs to be\n",
    "clarified that `t` is the last `t` in the sentence.\n",
    "\n",
    "- **<c.+t>** => The fat <font color='red'> cat sat on the mat</font>.\n",
    "\n",
    "### The Question Mark ?\n",
    "In regular expression the meta character `?` makes the preceding character optional. This symbol matches zero or one instance of the preceding character. For example, the regular expression `[T]?he` means: `Optional the uppercase letter T, followed by the lowercase character h, followed by the lowercase character e.`\n",
    "\n",
    "- **<[Tt]he>** => <font color=red>The</font> car parked in <font color=red>the</font> garage.\n",
    "\n",
    "### The Lazy Star *? \n",
    "\n",
    "Repeats the previous item zero or more times. Lazy, so the engine first attempts to skip the\n",
    "previous item, before trying permutations with ever increasing matches of the preceding\n",
    "item. \n",
    "\n",
    "|Regex|Means|\n",
    "|:----:|----|\n",
    "|abc+|        matches a string that has ab followed by one or more c|\n",
    "|abc?|       matches a string that has ab followed by zero or one c|\n",
    "|abc{2}|      matches a string that has ab followed by 2 c|\n",
    "|abc{2,}|    matches a string that has ab followed by 2 or more c|\n",
    "|abc{2,5}|    matches a string that has ab followed by 2 up to 5 c|\n",
    "|a(bc)\\*|     matches a string that has a followed by zero or more copies of the sequence bc|\n",
    "|a(bc){2,5}|  matches a string that has a followed by 2 up to 5 copies of the sequence bc|\n",
    "|**<.+>**| matches `<div>simple div</div>`|\n",
    "\n",
    "## Full stop or Period or dot **.**\n",
    "\n",
    "In regular expressions, the dot or period is one of the most commonly used metacharacters. Unfortunately, it\n",
    "is also the most commonly misused metacharacter. The dot is short for the negated character class **<[^\\n]>** (UNIX regex flavors) or\n",
    "**<[^\\r\\n]>** (Windows regex flavors).\n",
    "\n",
    "<font color = 'Red'> <b> Use The Dot Sparingly </b> </font>\n",
    "- The dot is a **very powerful** regex metacharacter. It allows you to be **lazy**. `Put in a dot, and everything will\n",
    "match just fine when you test the regex on valid data. The problem is that the regex will also match in cases\n",
    "where it should not match..`\n",
    "    \n",
    "Example - Let‚Äôs say we want to match a date in `mm/dd/yy` format, but we\n",
    "want to leave the user the choice of date separators. The quick solution is **<\\d\\d.\\d\\d.\\d\\d>**. Seems fine at\n",
    "first sight.. It will match a date like `02/12/03` just what we intended, So fine... \n",
    "- <font color='red'> <b> Trouble is: 02512703<b></font> is also considered a **valid date** by this regular expression. In this match, the first dot matched $5$, and the second matched $7$. Obviously $not$ what we intended. \n",
    "    \n",
    "## Start of String and End of String Anchors ( $ and ^)\n",
    "\n",
    "Anchors are a different breed. They do not match any character at all. Instead, they match a position before,\n",
    "after or between characters. They can be used to `anchor` the regex match at a certain position. \n",
    "- The caret **<^>**\n",
    "matches the position before the first character in the string. Applying **<^a>** to `abc` matches `a`. **<^b>** will\n",
    "not match `abc` at all, because the **<b\\>** cannot be matched right after the start of the string, matched by **<^>**.\n",
    "- Similarly, **<\\$>** matches right after the last character in the string. **<c\\$>** matches `c` in `abc`, while **<a\\$>** `does not` match `abc` at all...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n",
    "- Regex are now written in Quotes (\\`)\n",
    "- The String to be matched is in (\"bold\")\n",
    "\n",
    "Suppose you want to use a regex to match a list of function names in a programming language: \"**Get, GetValue, Set or SetValue.**\" \n",
    "- The obvious solution is `Get|GetValue|Set|SetValue`\n",
    "\n",
    "*Now take a look closer carefully at the regex and the string, both.\n",
    "Here are some other ways to do the same task*\n",
    "-  `Get(Value)?|Set(Value)?`\n",
    "-  `\\b(Get|GetValue|Set|SetValue)\\b`\n",
    "-  `\\b(Get(Value)?|Set(Value)?)\\b`\n",
    "- Even this one is correct `\\b(Get|Set)(Value)?\\b`\n",
    "\n",
    "**Regex**:\t`<[^>]+>`\n",
    "- **What it does**:\tThis finds any HTML, such as `<\\a>, <\\b>, <\\img />, <\\br />, etc`. You can use this to find segments that have HTML tags you need to deal with, or to remove all HTML tags from a text.\n",
    "    \n",
    "**Regex**:\t`https?:\\/\\/[\\w\\.\\/\\-?=&%,]+`\n",
    "- What it does:\tThis will find a URL. It will capture most URLs that begin with http:// or https://.\n",
    "\n",
    "**Regex**:\t`'\\w+?'`\n",
    "- **What it does**:\tThis finds single words that are surrounded by apostrophes.\n",
    "\n",
    "**Regex**:\t`([-A-Za-z0-9_]*?([-A-Za-z_][0-9]|[0-9][-A-Za-z_])[-A-Za-z0-9_]*)`\n",
    "- **What it does**:\tAlphanumeric part numbers and references like: 1111_A, AA1AAA or 1-1-1-A, 21A1 and 10UC10P-BACW, abcd-1234, 1234-pqtJK, sft-0021 or 21-1_AB and 55A or AK7_GY.\n",
    "This can be very useful if you are translating documents that have a lot of alphanumeric codes or references in them, and you need to be able to find them easily.\n",
    "\n",
    "**Regex**:\t`\\b(the|The)\\b.*?\\b(?=\\W?\\b(is|are|was|can|shall| must|that|which|about|by|at|if|when|should|among|above|under|$)\\b)`\n",
    "- **What it does**:\tThis finds text that begins with the or The and ends with stop words such as is, are, was, can, shall, must, that, which, about, by, at, if, when, should, among, above or under, or the end of the segment.\n",
    "This is particularly useful when you need to extract terminology. Suppose you have segments like these:\n",
    "`\n",
    "The Web based look up is our new feature. A project manager should not proofread... Our Product Name is...`\n",
    "    - The Regex shown above would find anything between The and is, or should. With most texts, there is a good chance that anything this Regex finds is a good term that you can add to your Termbase.\n",
    "\n",
    "**Regex**:\t`\\b(a|an|A|An)\\b.*?\\b(?=\\W?\\b(is|are|was|can|shall|must |that|which|about|by|at|if|when|among|above|under|$)\\b)`\n",
    "- **What it does**:\tThis works much like the Regex shown above, except that it finds text that begins with a or an, rather than the. This can also be very helpful when you need to extract terminology from a project.\n",
    "\n",
    "**Regex**:  `\\b(this|these|This|These)\\b.*?\\b(?=\\W?\\b(is|are|was|can|shall|must|that|which|about|by|at|if|when|among|above|under|$)\\b)`\n",
    "    - **What it does**:\tThis works much like the Regex shown above, except that it finds text that begins with this or these. This can also be very helpful when you need to extract terminology from a project.\n",
    "\n",
    "**Regex** :`(.*?)`\n",
    "- **What it does** : Accept blah-blah-blah...\n",
    "\n",
    "## Python [re module](https://docs.python.org/3/library/re.html)\n",
    "\n",
    "- `re.sub(regex, replacement, subject)` performs a search-and-replace across subject, replacing all\n",
    "matches of regex in subject with replacement. The result is returned by the sub() function. **The subject\n",
    "string you pass is not modified**. The re.sub() function applies the same backslash logic to the replacement text as is applied to the regular expression. Therefore, you should use raw strings for the replacement text..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tweet ---- \n",
      " #fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android +#apps +#beautiful          #cute #health #igers #iphoneonly #iphonesia #iphone              <3 ;D :( :-(\n",
      "\n",
      " Tweet after replacing hashtags ----\n",
      "   fingerprint   Pregnancy  Test https://goo.gl/h1MfQV  android  + apps  + beautiful            cute   health   igers   iphoneonly   iphonesia   iphone               <3 ;D :( :-(\n",
      "\n",
      " Tweet after replacing Emojis for Love with EMP_POS ----\n",
      "   fingerprint   Pregnancy  Test https://goo.gl/h1MfQV  android  + apps  + beautiful            cute   health   igers   iphoneonly   iphonesia   iphone                EMO_POS  ;D :( :-(\n",
      "\n",
      " Tweet after replacing Emojis for Wink with EMP_POS ----\n",
      "   fingerprint   Pregnancy  Test https://goo.gl/h1MfQV  android  + apps  + beautiful            cute   health   igers   iphoneonly   iphonesia   iphone                EMO_POS   EMO_POS  :( :-(\n",
      "\n",
      " Tweet after replacing Emojis for Sad with EMP_NEG ----\n",
      "   fingerprint   Pregnancy  Test https://goo.gl/h1MfQV  android  + apps  + beautiful            cute   health   igers   iphoneonly   iphonesia   iphone                EMO_POS   EMO_POS   EMO_NEG   EMO_NEG \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "tweet = '#fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android +#apps +#beautiful \\\n",
    "         #cute #health #igers #iphoneonly #iphonesia #iphone \\\n",
    "             <3 ;D :( :-('\n",
    "\n",
    "#Let's take care of emojis and the #(hash-tags)...\n",
    "\n",
    "print(f'Original Tweet ---- \\n {tweet}')\n",
    "\n",
    "## Replacing #hashtag with only hashtag\n",
    "tweet = re.sub(r'#(\\S+)', r' \\1 ', tweet)\n",
    "#this gets a bit technical as here we are using Backreferencing and Character Sets Shorthands and replacing the captured Group.\n",
    "#\\S = [^\\s] Matches any charachter that isn't white space\n",
    "print(f'\\n Tweet after replacing hashtags ----\\n  {tweet}')\n",
    "\n",
    "## Love -- <3, :*\n",
    "tweet = re.sub(r'(<3|:\\*)', ' EMO_POS ', tweet)\n",
    "print(f'\\n Tweet after replacing Emojis for Love with EMP_POS ----\\n  {tweet}')\n",
    "\n",
    "#The parentheses are for Grouping, so we search (remeber the raw string (`r`))\n",
    "#either for <3 or(|) :\\* (as * is a meta character, so preceeded by the backslash)\n",
    "\n",
    "## Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', tweet)\n",
    "print(f'\\n Tweet after replacing Emojis for Wink with EMP_POS ----\\n  {tweet}')\n",
    "\n",
    "#The parentheses are for Grouping as usual, then we first focus on `;-), ;),`, so we can see that 1st we need to have a ;\n",
    "#and then we can either have a `-` or nothing, so we can do this via using our `?` clubbed with `;` and hence we have the very\n",
    "#starting with `(;-?\\)` and simarly for others...\n",
    "\n",
    "## Sad -- :-(, : (, :(, ):, )-:\n",
    "tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', tweet)\n",
    "print(f'\\n Tweet after replacing Emojis for Sad with EMP_NEG ----\\n  {tweet}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tweet after replacing xtra spaces ----\n",
      "   fingerprint Pregnancy Test https://goo.gl/h1MfQV android + apps + beautiful cute health igers iphoneonly iphonesia iphone EMO_POS EMO_POS EMO_NEG EMO_NEG \n",
      "\n",
      " Tweet after replacing Punctuation + with PUNC ----\n",
      "   fingerprint Pregnancy Test httpsgooglh1MfQV android  apps  beautiful cute health igers iphoneonly iphonesia iphone EMO_POS EMO_POS EMO_NEG EMO_NEG \n"
     ]
    }
   ],
   "source": [
    "##See the Output Carefully, there are Spaces inbetween un-necessary...\n",
    "## Replace multiple spaces with a single space\n",
    "tweet = re.sub(r'\\s+', ' ', tweet)\n",
    "print(f'\\n Tweet after replacing xtra spaces ----\\n  {tweet}')\n",
    "      \n",
    "##Replace the Puctuations (+,;) \n",
    "tweet = re.sub(r'[^\\w\\s]','',tweet)\n",
    "print(f'\\n Tweet after replacing Punctuation + with PUNC ----\\n  {tweet}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bags of positive/negative smiles (You can extend the above example to take care of these few too...))) A good Excercise...\n",
    "\n",
    "positive_emojis = set([\n",
    "\":‚Äë)\",\":)\",\":-]\",\":]\",\":-3\",\":3\",\":->\",\":>\",\"8-)\",\"8)\",\":-}\",\":}\",\":o)\",\":c)\",\":^)\",\"=]\",\"=)\",\":‚ÄëD\",\":D\",\"8‚ÄëD\",\"8D\",\n",
    "\"x‚ÄëD\",\"xD\",\"X‚ÄëD\",\"XD\",\"=D\",\"=3\",\"B^D\",\":-))\",\";‚Äë)\",\";)\",\"*-)\",\"*)\",\";‚Äë]\",\";]\",\";^)\",\":‚Äë,\",\";D\",\":‚ÄëP\",\":P\",\"X‚ÄëP\",\"XP\",\n",
    "\"x‚Äëp\",\"xp\",\":‚Äëp\",\":p\",\":‚Äë√û\",\":√û\",\":‚Äë√æ\",\":√æ\",\":‚Äëb\",\":b\",\"d:\",\"=p\",\">:P\", \":'‚Äë)\", \":')\",  \":-*\", \":*\", \":√ó\"\n",
    "])\n",
    "negative_emojis = set([\n",
    "\":‚Äë(\",\":(\",\":‚Äëc\",\":c\",\":‚Äë<\",\":<\",\":‚Äë[\",\":[\",\":-||\",\">:[\",\":{\",\":@\",\">:(\",\"D‚Äë':\",\"D:<\",\"D:\",\"D8\",\"D;\",\"D=\",\"DX\",\":‚Äë/\",\n",
    "\":/\",\":‚Äë.\",'>:\\\\', \">:/\", \":\\\\\", \"=/\" ,\"=\\\\\", \":L\", \"=L\",\":S\",\":‚Äë|\",\":|\",\"|‚ÄëO\",\"<:‚Äë|\"\n",
    "])\n",
    "\n",
    "del positive_emojis, negative_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Valid Dates..\n",
    "pattern = r'(19|20)\\d\\d[- /.](0[1-9]|1[012])[- /.](0[1-9]|[12][0-9]|3[01])' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- matches a date in yyyy-mm-dd format from between 1900-01-01 and 2099-12-31, with a choice of four separators(space included :))\n",
    "\n",
    "- The year is matched by `(19|20)\\d\\d`\n",
    "- The month is matched by `(0[1-9]|1[012])` (rounding brackets are necessary so that to include both the options)\n",
    "    - By using character classes, \n",
    "        - the first option matches a number between `01 and 09`, and \n",
    "        - the second matches `10, 11 or 12`\n",
    "- The last part of the regex consists of three options. The first matches the numbers `01\n",
    "through 09`, the second `10 through 29`, and the third matches `30 or 31`... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pattern to match any IP Addresses \n",
    "pattern = r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the above pattern will also match `999.999.999.999` but that isn't a valid IP at all\n",
    "Now this depends on the data at hand as to how far you want the regex to be accurate...\n",
    "To restrict all `4` numbers in the IP address to `0..255`, you can use this\n",
    "complex beast: \n",
    "- `\\b(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-\n",
    "9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-\n",
    "4][0-9]|[01]?[0-9][0-9]?)\\b`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References (A lot)\n",
    "\n",
    "- http://www.rexegg.com/ <<<< THE ULTIMATE WEBSITE>>>>\n",
    "- https://github.com/aloisdg/awesome-regex\n",
    "- http://linuxreviews.org/beginner/tao_of_regular_expressions/tao_of_regular_expressions.en.print.pdf\n",
    "- https://developers.google.com/edu/python/regular-expressions\n",
    "- https://www.youtube.com/watch?v=EkluES9Rvak\n",
    "\n",
    "PS I wrote (above things) for the [Amazing Course](https://mlcourse.ai/) which is maintained by [@kashnitsky](https://www.kaggle.com/kashnitsky),[@artgor](https://www.kaggle.com/artgor) , [@datamove](https://www.kaggle.com/metadist) etc. and many many many other amazing peoples from ODS who framed the amazing course;\n",
    "\n",
    "You can find the source nbs [here](https://github.com/Yorko/mlcourse.ai/tree/master/jupyter_english/tutorials) and a lot of very cool stuffs there as well !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> ** <font color ='red'> So Now we are good to go!! Armed with regex, let's see what they can do.. </font> ** </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### StarDate Captain's Logs xD\n",
    " - latest - I tried updating the arch.\n",
    " - v14 - I tried some text cleaning via spelling correction and addded a new re pattern to fix texts. Looking fo an automated way to clean text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.23 s, sys: 800 ms, total: 9.03 s\n",
      "Wall time: 9.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv',usecols=['comment_text', 'target'])\n",
    "test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n",
    "sub = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# From Quora kaggle Comp's (latest one)\n",
    "import re\n",
    "# remove space\n",
    "spaces = ['\\u200b', '\\u200e', '\\u202a', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\x10', '\\x7f', '\\x9d', '\\xad', '\\xa0']\n",
    "def remove_space(text):\n",
    "    \"\"\"\n",
    "    remove extra spaces and ending space if any\n",
    "    \"\"\"\n",
    "    for space in spaces:\n",
    "        text = text.replace(space, ' ')\n",
    "    text = text.strip()\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# replace strange punctuations and raplace diacritics\n",
    "from unicodedata import category, name, normalize\n",
    "\n",
    "def remove_diacritics(s):\n",
    "    return ''.join(c for c in normalize('NFKD', s.replace('√∏', 'o').replace('√ò', 'O').replace('‚Åª', '-').replace('‚Çã', '-'))\n",
    "                  if category(c) != 'Mn')\n",
    "\n",
    "special_punc_mappings = {\"‚Äî\": \"-\", \"‚Äì\": \"-\", \"_\": \"-\", '‚Äù': '\"', \"‚Ä≥\": '\"', '‚Äú': '\"', '‚Ä¢': '.', '‚àí': '-',\n",
    "                         \"‚Äô\": \"'\", \"‚Äò\": \"'\", \"¬¥\": \"'\", \"`\": \"'\", '\\u200b': ' ', '\\xa0': ' ','ÿå':'','‚Äû':'',\n",
    "                         '‚Ä¶': ' ... ', '\\ufeff': ''}\n",
    "def clean_special_punctuations(text):\n",
    "    for punc in special_punc_mappings:\n",
    "        if punc in text:\n",
    "            text = text.replace(punc, special_punc_mappings[punc])\n",
    "    text = remove_diacritics(text)\n",
    "    return text\n",
    "\n",
    "# clean numbers\n",
    "def clean_number(text):\n",
    "    if bool(re.search(r'\\d', text)):\n",
    "        text = re.sub(r'(\\d+)([a-zA-Z])', '\\g<1> \\g<2>', text) # digits followed by a single alphabet...\n",
    "        text = re.sub(r'(\\d+) (th|st|nd|rd) ', '\\g<1>\\g<2> ', text) #1st, 2nd, 3rd, 4th...\n",
    "        text = re.sub(r'(\\d+),(\\d+)', '\\g<1>\\g<2>', text)\n",
    "    return text\n",
    "\n",
    "import string\n",
    "regular_punct = list(string.punctuation)\n",
    "extra_punct = [\n",
    "    ',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "    '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '‚Ä¢',  '~', '@', '¬£',\n",
    "    '¬∑', '_', '{', '}', '¬©', '^', '¬Æ', '`',  '<', '‚Üí', '¬∞', '‚Ç¨', '‚Ñ¢', '‚Ä∫',\n",
    "    '‚ô•', '‚Üê', '√ó', '¬ß', '‚Ä≥', '‚Ä≤', '√Ç', '‚ñà', '¬Ω', '√†', '‚Ä¶', '‚Äú', '‚òÖ', '‚Äù',\n",
    "    '‚Äì', '‚óè', '√¢', '‚ñ∫', '‚àí', '¬¢', '¬≤', '¬¨', '‚ñë', '¬∂', '‚Üë', '¬±', '¬ø', '‚ñæ',\n",
    "    '‚ïê', '¬¶', '‚ïë', '‚Äï', '¬•', '‚ñì', '‚Äî', '‚Äπ', '‚îÄ', '‚ñí', 'Ôºö', '¬º', '‚äï', '‚ñº',\n",
    "    '‚ñ™', '‚Ä†', '‚ñ†', '‚Äô', '‚ñÄ', '¬®', '‚ñÑ', '‚ô´', '‚òÜ', '√©', '¬Ø', '‚ô¶', '¬§', '‚ñ≤',\n",
    "    '√®', '¬∏', '¬æ', '√É', '‚ãÖ', '‚Äò', '‚àû', '‚àô', 'Ôºâ', '‚Üì', '„ÄÅ', '‚îÇ', 'Ôºà', '¬ª',\n",
    "    'Ôºå', '‚ô™', '‚ï©', '‚ïö', '¬≥', '„Éª', '‚ï¶', '‚ï£', '‚ïî', '‚ïó', '‚ñ¨', '‚ù§', '√Ø', '√ò',\n",
    "    '¬π', '‚â§', '‚Ä°', '‚àö', '¬´', '¬ª', '¬¥', '¬∫', '¬æ', '¬°', '¬ß', '¬£', '‚Ç§',\n",
    "    ':)', ': )', ':-)', '(:', '( :', '(-:', ':\\')',\n",
    "    ':D', ': D', ':-D', 'xD', 'x-D', 'XD', 'X-D',\n",
    "    '<3', ':*',\n",
    "    ';-)', ';)', ';-D', ';D', '(;',  '(-;',\n",
    "    ':-(', ': (', ':(', '\\'):', ')-:',\n",
    "    '-- :','(', ':\\'(', ':\"(\\'',]\n",
    "\n",
    "def handle_emojis(text): #Speed can be improved via a simple if check :)\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    text = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', text)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    text = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMO_POS ', text)\n",
    "    # Love -- <3, :*\n",
    "    text = re.sub(r'(<3|:\\*)', ' EMO_POS ', text)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    text = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', text)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:\n",
    "    text = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', text)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    text = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', text)\n",
    "    return text\n",
    "\n",
    "def stop(text):\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    text = \" \".join([w.lower() for w in text.split()])\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    words = [w for w in text.split() if not w in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "all_punct = list(set(regular_punct + extra_punct))\n",
    "# do not spacing - and .\n",
    "all_punct.remove('-')\n",
    "all_punct.remove('.')\n",
    "\n",
    "# clean repeated letters\n",
    "def clean_repeat_words(text):\n",
    "    \n",
    "    text = re.sub(r\"(I|i)(I|i)+ng\", \"ing\", text)\n",
    "    text = re.sub(r\"(L|l)(L|l)(L|l)+y\", \"lly\", text)\n",
    "    text = re.sub(r\"(A|a)(A|a)(A|a)+\", \"a\", text)\n",
    "    text = re.sub(r\"(C|c)(C|c)(C|c)+\", \"cc\", text)\n",
    "    text = re.sub(r\"(D|d)(D|d)(D|d)+\", \"dd\", text)\n",
    "    text = re.sub(r\"(E|e)(E|e)(E|e)+\", \"ee\", text)\n",
    "    text = re.sub(r\"(F|f)(F|f)(F|f)+\", \"ff\", text)\n",
    "    text = re.sub(r\"(G|g)(G|g)(G|g)+\", \"gg\", text)\n",
    "    text = re.sub(r\"(I|i)(I|i)(I|i)+\", \"i\", text)\n",
    "    text = re.sub(r\"(K|k)(K|k)(K|k)+\", \"k\", text)\n",
    "    text = re.sub(r\"(L|l)(L|l)(L|l)+\", \"ll\", text)\n",
    "    text = re.sub(r\"(M|m)(M|m)(M|m)+\", \"mm\", text)\n",
    "    text = re.sub(r\"(N|n)(N|n)(N|n)+\", \"nn\", text)\n",
    "    text = re.sub(r\"(O|o)(O|o)(O|o)+\", \"oo\", text)\n",
    "    text = re.sub(r\"(P|p)(P|p)(P|p)+\", \"pp\", text)\n",
    "    text = re.sub(r\"(Q|q)(Q|q)+\", \"q\", text)\n",
    "    text = re.sub(r\"(R|r)(R|r)(R|r)+\", \"rr\", text)\n",
    "    text = re.sub(r\"(S|s)(S|s)(S|s)+\", \"ss\", text)\n",
    "    text = re.sub(r\"(T|t)(T|t)(T|t)+\", \"tt\", text)\n",
    "    text = re.sub(r\"(V|v)(V|v)+\", \"v\", text)\n",
    "    text = re.sub(r\"(Y|y)(Y|y)(Y|y)+\", \"y\", text)\n",
    "    text = re.sub(r\"plzz+\", \"please\", text)\n",
    "    text = re.sub(r\"(Z|z)(Z|z)(Z|z)+\", \"zz\", text)\n",
    "    text = re.sub(r\"(-+|\\.+)\", \" \", text) #new haha #this adds a space token so we need to remove xtra spaces\n",
    "    return text\n",
    "\n",
    "def spacing_punctuation(text):\n",
    "    \"\"\"\n",
    "    add space before and after punctuation and symbols\n",
    "    \"\"\"\n",
    "    for punc in all_punct:\n",
    "        if punc in text:\n",
    "            text = text.replace(punc, f' {punc} ')\n",
    "    return text\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    preprocess text main steps\n",
    "    \"\"\"\n",
    "    text = remove_space(text)\n",
    "    text = clean_special_punctuations(text)\n",
    "    text = handle_emojis(text)\n",
    "    text = clean_number(text)\n",
    "    text = spacing_punctuation(text)\n",
    "    text = clean_repeat_words(text)\n",
    "    text = remove_space(text)\n",
    "    #text = stop(text)# if changing this, then chnage the dims \n",
    "    #(not to be done yet as its effecting the embeddings..,we might be\n",
    "    #loosing words)...\n",
    "    return text\n",
    "\n",
    "mispell_dict = {'üòâ':'wink','üòÇ':'joy','üòÄ':'stuck out tongue', 'theguardian':'the guardian','deplorables':'deplorable', 'theglobeandmail':'the globe and mail', 'justiciaries': 'justiciary','creditdation': 'Accreditation','doctrne':'doctrine','fentayal': 'fentanyl','designation-': 'designation','CONartist' : 'con-artist','Mutilitated' : 'Mutilated','Obumblers': 'bumblers','negotiatiations': 'negotiations','dood-': 'dood','irakis' : 'iraki','cooerate': 'cooperate','COx':'cox','racistcomments':'racist comments','envirnmetalists': 'environmentalists',}\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "\n",
    "def correct_spelling(x, dic):\n",
    "    for word in dic.keys():\n",
    "        x = x.replace(word, dic[word])\n",
    "    return x\n",
    "\n",
    "def correct_contraction(x, dic):\n",
    "    for word in dic.keys():\n",
    "        x = x.replace(word, dic[word])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.25 s, sys: 3.17 s, total: 6.42 s\n",
      "Wall time: 12min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def text_clean_wrapper(df):\n",
    "    \n",
    "    df[\"comment_text\"] = df[\"comment_text\"].astype('str').transform(preprocess)\n",
    "    df['comment_text'] = df['comment_text'].transform(lambda x: correct_spelling(x, mispell_dict))\n",
    "    df['comment_text'] = df['comment_text'].transform(lambda x: correct_contraction(x, contraction_mapping))\n",
    "    \n",
    "    return df\n",
    "\n",
    "#fast!\n",
    "train = df_parallelize_run(train, text_clean_wrapper)\n",
    "test  = df_parallelize_run(test, text_clean_wrapper)\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "del mispell_dict, all_punct, special_punc_mappings, regular_punct, extra_punct\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\n",
    "\n",
    "import os\n",
    "import time\n",
    "import gensim\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lc = LancasterStemmer()\n",
    "from nltk.stem import SnowballStemmer\n",
    "sb = SnowballStemmer(\"english\")\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D, BatchNormalization\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import sys\n",
    "from os.path import dirname\n",
    "from keras.engine import InputSpec, Layer\n",
    "\n",
    "import spacy\n",
    "\n",
    "\n",
    "# https://www.kaggle.com/takuok/bidirectional-lstm-and-attention-lb-0-043/\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>This is so cool It ' s like , ' would you want your mother to read this ? ? ' Really great idea , well done !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                                                                                   comment_text\n",
       "0     0.0  This is so cool It ' s like , ' would you want your mother to read this ? ? ' Really great idea , well done !"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "--- 0.5262837409973145 seconds ---\n",
      "Spacy NLP ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3003b5160b53415d9bfc4cc14051d61a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "print(\"Loading data ...\")\n",
    "\n",
    "full_text = pd.concat([train['comment_text'].astype(str), test['comment_text'].astype(str)])\n",
    "y = train['target']\n",
    "num_train_data = y.shape[0]\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "start_time = time.time()\n",
    "print(\"Spacy NLP ...\")\n",
    "nlp = spacy.load('en_core_web_lg', disable=['parser','ner','tagger'])\n",
    "nlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)\n",
    "word_dict = {}\n",
    "word_index = 1\n",
    "lemma_dict = {}\n",
    "docs = nlp.pipe(full_text, n_threads = 4)\n",
    "word_sequences = []\n",
    "for doc in tqdm(docs):\n",
    "    word_seq = []\n",
    "    for token in doc:\n",
    "        if (token.text not in word_dict) and (token.pos_ is not \"PUNCT\"):\n",
    "            word_dict[token.text] = word_index\n",
    "            word_index += 1\n",
    "            lemma_dict[token.text] = token.lemma_\n",
    "        if token.pos_ is not \"PUNCT\":\n",
    "            word_seq.append(word_dict[token.text])\n",
    "    word_sequences.append(word_seq)\n",
    "del docs\n",
    "import gc\n",
    "gc.collect()\n",
    "train_word_sequences = word_sequences[:num_train_data]\n",
    "test_word_sequences = word_sequences[num_train_data:]\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 128\n",
    "X_train = pad_sequences(train_word_sequences, maxlen = max_len)\n",
    "X_test  = pad_sequences(test_word_sequences, maxlen = max_len)\n",
    "del train_word_sequences, test_word_sequences\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "#max_features = 377645 #NB this will change if you change any pre-processing (working to auto-mating this, kinda NEW to NLP:))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding matrix ...\n",
      "[-1. -1. -1. -1. -1.]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd97ba2187d4d18ab50221633c4d278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=411174), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 1740.061482667923 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#quora comp\n",
    "def load_glove(word_dict, lemma_dict):\n",
    "    from gensim.models import KeyedVectors\n",
    "    EMBEDDING_FILE = '../input/glove840b300dtxt/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100 and o.split(\" \")[0] in word_dict.keys())\n",
    "    #embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "    embed_size = 300\n",
    "    nb_words = len(word_dict)+1\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)\n",
    "    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1.\n",
    "    print(unknown_vector[:5])\n",
    "    for key in tqdm(word_dict):\n",
    "        word = key\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.lower()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.upper()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.capitalize()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = ps.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lc.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = sb.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lemma_dict[key]\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        embedding_matrix[word_dict[key]] = unknown_vector                    \n",
    "    return embedding_matrix, nb_words\n",
    "\n",
    "print(\"Loading embedding matrix ...\")\n",
    "embedding_matrix, nb_words =  load_glove(word_dict, lemma_dict)\n",
    "max_features = nb_words\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "#.........."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.where(train['target'] >= 0.5, True, False) * 1 #As per comp's DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(411175, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del nb_words, lemma_dict, word_dict, word_index, train, test\n",
    "gc.collect()\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "def build_model(lr=0.0, lr_d=0.0, spatial_dr=0.0,  dense_units=128, dr=0.1):\n",
    "    \n",
    "    from keras.layers import LSTM, Bidirectional, Dropout\n",
    "    \n",
    "    file_path = \"best_model.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                                  save_best_only = True, mode = \"min\")\n",
    "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "    \n",
    "    inp = Input(shape=(max_len,))\n",
    "\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = SpatialDropout1D(spatial_dr)(x)\n",
    "    \n",
    "    x1 = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n",
    "    x2 = Bidirectional(CuDNNGRU(128, return_sequences=True))(x1)\n",
    "    \n",
    "    max_pool1 = GlobalMaxPooling1D()(x1)\n",
    "    max_pool2 = GlobalMaxPooling1D()(x2)\n",
    "    \n",
    "    conc = Concatenate()([max_pool1, max_pool2])\n",
    "    predictions = Dense(1, activation='sigmoid')(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=predictions)\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "\n",
    "    history = model.fit(X_train, y, batch_size = 512, epochs = 15, validation_split=0.1, \n",
    "                        verbose = 1, callbacks = [check_point, early_stop])\n",
    "    \n",
    "    #model = load_model(file_path)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1624386 samples, validate on 180488 samples\n",
      "Epoch 1/15\n",
      "1624386/1624386 [==============================] - 334s 205us/step - loss: 0.1406 - acc: 0.9464 - val_loss: 0.1336 - val_acc: 0.9469\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13355, saving model to best_model.hdf5\n",
      "Epoch 2/15\n",
      "1624386/1624386 [==============================] - 333s 205us/step - loss: 0.1249 - acc: 0.9513 - val_loss: 0.1294 - val_acc: 0.9482\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.13355 to 0.12944, saving model to best_model.hdf5\n",
      "Epoch 3/15\n",
      "1624386/1624386 [==============================] - 332s 204us/step - loss: 0.1212 - acc: 0.9527 - val_loss: 0.1287 - val_acc: 0.9482\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.12944 to 0.12874, saving model to best_model.hdf5\n",
      "Epoch 4/15\n",
      "1100288/1624386 [===================>..........] - ETA: 1:42 - loss: 0.1191 - acc: 0.9532"
     ]
    }
   ],
   "source": [
    "model = build_model(lr = 1e-3, lr_d = 0.001, spatial_dr = 0.23, dr=0.2)\n",
    "del X_train, embedding_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97320/97320 [==============================] - 8s 82us/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test, batch_size = 512, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(pred);\n",
    "plt.title('Distribution of predictions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['prediction'] = pred\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97320/97320 [==============================] - 8s 79us/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test, batch_size = 512, verbose = 1)\n",
    "\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "06ae8f1c25134dd4a0ad6f5f7ea214a6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "107206d9da424ceba497c5d8239acc5d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_73fac6f5e2584e4b8148842cab8bd784",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_a8844f7fb95a4aa0a9d75ae5f27af3bb",
       "value": "100% 411174/411174 [00:12&lt;00:00, 32256.63it/s]"
      }
     },
     "171052a808554be0bc25bee8d02562a5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "18e8a6741b3542748ac160a130b0552f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3003b5160b53415d9bfc4cc14051d61a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ed49b4da757046c6a2a915a29678d262",
        "IPY_MODEL_7d98ea52fbc845af8945873c1215748b"
       ],
       "layout": "IPY_MODEL_f18b4afa29594f83a3dd1a41b006dd40"
      }
     },
     "3258ec88fb2a48dc9574b62eb364c047": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "67d5ebf9831f4c19a2746ec740cbe3f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6b9e256c0a3d4fc4bd3d314990f0b670": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "73fac6f5e2584e4b8148842cab8bd784": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7d98ea52fbc845af8945873c1215748b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_06ae8f1c25134dd4a0ad6f5f7ea214a6",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_3258ec88fb2a48dc9574b62eb364c047",
       "value": "774890/|/| 774890/? [11:01&lt;00:00, 1104.13it/s]"
      }
     },
     "a8844f7fb95a4aa0a9d75ae5f27af3bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "bfd97ba2187d4d18ab50221633c4d278": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c8455072b91c42df87e443343249e9a1",
        "IPY_MODEL_107206d9da424ceba497c5d8239acc5d"
       ],
       "layout": "IPY_MODEL_171052a808554be0bc25bee8d02562a5"
      }
     },
     "c8455072b91c42df87e443343249e9a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_18e8a6741b3542748ac160a130b0552f",
       "max": 411174,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ebea7d59aede413b9a231b3de07cdf68",
       "value": 411174
      }
     },
     "ebea7d59aede413b9a231b3de07cdf68": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ed49b4da757046c6a2a915a29678d262": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "ProgressView",
       "bar_style": "info",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6b9e256c0a3d4fc4bd3d314990f0b670",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_67d5ebf9831f4c19a2746ec740cbe3f2",
       "value": 1
      }
     },
     "f18b4afa29594f83a3dd1a41b006dd40": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
