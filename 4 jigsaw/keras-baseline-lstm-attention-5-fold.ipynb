{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "TEXT_COL = 'comment_text'\n",
    "EMB_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n",
    "train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv', index_col='id')\n",
    "test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv', index_col='id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embeddings(embed_dir=EMB_PATH):\n",
    "    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in tqdm(open(embed_dir)))\n",
    "    return embedding_index\n",
    "\n",
    "def build_embedding_matrix(word_index, embeddings_index, max_features, lower = True, verbose = True):\n",
    "    embedding_matrix = np.zeros((max_features, 300))\n",
    "    for word, i in tqdm(word_index.items(),disable = not verbose):\n",
    "        if lower:\n",
    "            word = word.lower()\n",
    "        if i >= max_features: continue\n",
    "        try:\n",
    "            embedding_vector = embeddings_index[word]\n",
    "        except:\n",
    "            embedding_vector = embeddings_index[\"unknown\"]\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "def build_matrix(word_index, embeddings_index):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1,300))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        except:\n",
    "            embedding_matrix[i] = embeddings_index[\"unknown\"]\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting tokenizer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import gc\n",
    "\n",
    "maxlen = 220\n",
    "max_features = 100000\n",
    "embed_size = 300\n",
    "tokenizer = Tokenizer(num_words=max_features, lower=True) #filters = ''\n",
    "#tokenizer = text.Tokenizer(num_words=max_features)\n",
    "print('fitting tokenizer')\n",
    "tokenizer.fit_on_texts(list(train[TEXT_COL]) + list(test[TEXT_COL]))\n",
    "word_index = tokenizer.word_index\n",
    "X_train = tokenizer.texts_to_sequences(list(train[TEXT_COL]))\n",
    "y_train = train['target'].values\n",
    "X_test = tokenizer.texts_to_sequences(list(test[TEXT_COL]))\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "\n",
    "del tokenizer\n",
    "gc.collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000001it [02:12, 15062.90it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = load_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = build_matrix(word_index, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del embeddings_index\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import keras.layers as L\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def build_model(verbose = False, compile = True):\n",
    "    sequence_input = L.Input(shape=(maxlen,), dtype='int32')\n",
    "    embedding_layer = L.Embedding(len(word_index) + 1,\n",
    "                                300,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=maxlen,\n",
    "                                trainable=False)\n",
    "    x = embedding_layer(sequence_input)\n",
    "    x = L.SpatialDropout1D(0.2)(x)\n",
    "    x = L.Bidirectional(L.CuDNNLSTM(64, return_sequences=True))(x)\n",
    "\n",
    "    att = Attention(maxlen)(x)\n",
    "    avg_pool1 = L.GlobalAveragePooling1D()(x)\n",
    "    max_pool1 = L.GlobalMaxPooling1D()(x)\n",
    "\n",
    "    x = L.concatenate([att,avg_pool1, max_pool1])\n",
    "\n",
    "    preds = L.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    if compile:\n",
    "        model.compile(loss='binary_crossentropy',optimizer=Adam(0.005),metrics=['acc'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 1443899 samples, validate on 360975 samples\n",
      "Epoch 1/100\n",
      "1443899/1443899 [==============================] - 138s 95us/step - loss: 0.1169 - acc: 0.9584 - val_loss: 0.0923 - val_acc: 0.9646\n",
      "Epoch 2/100\n",
      "1443899/1443899 [==============================] - 137s 95us/step - loss: 0.0935 - acc: 0.9644 - val_loss: 0.0909 - val_acc: 0.9652\n",
      "Epoch 3/100\n",
      "1443899/1443899 [==============================] - 137s 95us/step - loss: 0.0893 - acc: 0.9658 - val_loss: 0.0881 - val_acc: 0.9662\n",
      "Epoch 4/100\n",
      "1443899/1443899 [==============================] - 137s 95us/step - loss: 0.0867 - acc: 0.9664 - val_loss: 0.0927 - val_acc: 0.9633\n",
      "Epoch 5/100\n",
      "1443899/1443899 [==============================] - 137s 95us/step - loss: 0.0847 - acc: 0.9673 - val_loss: 0.1025 - val_acc: 0.9585\n",
      "Epoch 6/100\n",
      "1443899/1443899 [==============================] - 137s 95us/step - loss: 0.0831 - acc: 0.9677 - val_loss: 0.0942 - val_acc: 0.9628\n",
      "Epoch 00006: early stopping\n",
      "Train on 1443899 samples, validate on 360975 samples\n",
      "Epoch 1/100\n",
      "1443899/1443899 [==============================] - 138s 95us/step - loss: 0.1193 - acc: 0.9576 - val_loss: 0.0995 - val_acc: 0.9613\n",
      "Epoch 2/100\n",
      "1443899/1443899 [==============================] - 137s 95us/step - loss: 0.0950 - acc: 0.9639 - val_loss: 0.0930 - val_acc: 0.9634\n",
      "Epoch 3/100\n",
      "1443899/1443899 [==============================] - 137s 95us/step - loss: 0.0906 - acc: 0.9653 - val_loss: 0.0932 - val_acc: 0.9650\n",
      "Epoch 4/100\n",
      "1443899/1443899 [==============================] - 137s 95us/step - loss: 0.0881 - acc: 0.9663 - val_loss: 0.0876 - val_acc: 0.9661\n",
      "Epoch 5/100\n",
      "1443899/1443899 [==============================] - 137s 95us/step - loss: 0.0856 - acc: 0.9670 - val_loss: 0.0922 - val_acc: 0.9631\n",
      "Epoch 6/100\n",
      "1443899/1443899 [==============================] - 137s 95us/step - loss: 0.0841 - acc: 0.9676 - val_loss: 0.0887 - val_acc: 0.9654\n",
      "Epoch 7/100\n",
      "1443899/1443899 [==============================] - 137s 95us/step - loss: 0.0824 - acc: 0.9683 - val_loss: 0.0880 - val_acc: 0.9662\n",
      "Epoch 00007: early stopping\n",
      "Train on 1443899 samples, validate on 360975 samples\n",
      "Epoch 1/100\n",
      "1443899/1443899 [==============================] - 138s 95us/step - loss: 0.1179 - acc: 0.9573 - val_loss: 0.0868 - val_acc: 0.9669\n",
      "Epoch 2/100\n",
      "1443899/1443899 [==============================] - 137s 95us/step - loss: 0.0937 - acc: 0.9642 - val_loss: 0.0845 - val_acc: 0.9672\n",
      "Epoch 3/100\n",
      " 106496/1443899 [=>............................] - ETA: 1:58 - loss: 0.0902 - acc: 0.9649"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "splits = list(KFold(n_splits=5).split(X_train,y_train))\n",
    "\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "BATCH_SIZE = 2048\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "oof_preds = np.zeros((X_train.shape[0]))\n",
    "test_preds = np.zeros((X_test.shape[0]))\n",
    "for fold in [0,1,2,3,4]:\n",
    "    K.clear_session()\n",
    "    tr_ind, val_ind = splits[fold]\n",
    "    ckpt = ModelCheckpoint(f'gru_{fold}.hdf5', save_best_only = True)\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "    model = build_model()\n",
    "    model.fit(X_train[tr_ind],\n",
    "        y_train[tr_ind]>0.5,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        validation_data=(X_train[val_ind], y_train[val_ind]>0.5),\n",
    "        callbacks = [es,ckpt])\n",
    "\n",
    "    oof_preds[val_ind] += model.predict(X_train[val_ind])[:,0]\n",
    "    test_preds += model.predict(X_test)[:,0]\n",
    "test_preds /= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9701654011792239"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_train>0.5,oof_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7000000</td>\n",
       "      <td>0.001369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7000001</td>\n",
       "      <td>0.000080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7000002</td>\n",
       "      <td>0.005752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7000003</td>\n",
       "      <td>0.001052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7000004</td>\n",
       "      <td>0.992313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  prediction\n",
       "0  7000000    0.001369\n",
       "1  7000001    0.000080\n",
       "2  7000002    0.005752\n",
       "3  7000003    0.001052\n",
       "4  7000004    0.992313"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv', index_col='id')\n",
    "submission['prediction'] = test_preds\n",
    "submission.reset_index(drop=False, inplace=True)\n",
    "submission.head()\n",
    "#%%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
