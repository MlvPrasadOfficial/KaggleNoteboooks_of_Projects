{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo Labelling + Lasso + Gaussian Mixture + PCA + QDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a merge of  [Pseudo labelling with PCA-QDA](http://https://www.kaggle.com/rdekou/pseudo-labelling-with-pca-qda) and [GraphicalLasso + GaussianMixture](http://https://www.kaggle.com/christofhenkel/graphicallasso-gaussianmixture). Thanks to the original authors for their work. First QDA model is replaced with Lasso and Gaussian Mixture which slightly boosts the score on the LB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>muggy-smalt-axolotl-pembus</th>\n",
       "      <th>dorky-peach-sheepdog-ordinal</th>\n",
       "      <th>slimy-seashell-cassowary-goose</th>\n",
       "      <th>snazzy-harlequin-chicken-distraction</th>\n",
       "      <th>frumpy-smalt-mau-ordinal</th>\n",
       "      <th>stealthy-beige-pinscher-golden</th>\n",
       "      <th>chummy-cream-tarantula-entropy</th>\n",
       "      <th>hazy-emerald-cuttlefish-unsorted</th>\n",
       "      <th>nerdy-indigo-wolfhound-sorted</th>\n",
       "      <th>leaky-amaranth-lizard-sorted</th>\n",
       "      <th>ugly-tangerine-chihuahua-important</th>\n",
       "      <th>shaggy-silver-indri-fimbus</th>\n",
       "      <th>flaky-chocolate-beetle-grandmaster</th>\n",
       "      <th>squirrely-harlequin-sheep-sumble</th>\n",
       "      <th>freaky-tan-angelfish-noise</th>\n",
       "      <th>lousy-plum-penguin-sumble</th>\n",
       "      <th>bluesy-rose-wallaby-discard</th>\n",
       "      <th>baggy-copper-oriole-dummy</th>\n",
       "      <th>stealthy-scarlet-hound-fepid</th>\n",
       "      <th>greasy-cinnamon-bonobo-contributor</th>\n",
       "      <th>cranky-cardinal-dogfish-ordinal</th>\n",
       "      <th>snippy-auburn-vole-learn</th>\n",
       "      <th>greasy-sepia-coral-dataset</th>\n",
       "      <th>flabby-tangerine-fowl-entropy</th>\n",
       "      <th>lousy-smalt-pinscher-dummy</th>\n",
       "      <th>bluesy-brass-chihuahua-distraction</th>\n",
       "      <th>goopy-eggplant-indri-entropy</th>\n",
       "      <th>homey-sepia-bombay-sorted</th>\n",
       "      <th>homely-ruby-bulldog-entropy</th>\n",
       "      <th>hasty-blue-sheep-contributor</th>\n",
       "      <th>blurry-wisteria-oyster-master</th>\n",
       "      <th>snoopy-auburn-dogfish-expert</th>\n",
       "      <th>stinky-maroon-blue-kernel</th>\n",
       "      <th>bumpy-amaranth-armadillo-important</th>\n",
       "      <th>slaphappy-peach-oyster-master</th>\n",
       "      <th>dorky-tomato-ragdoll-dataset</th>\n",
       "      <th>messy-mauve-wolverine-ordinal</th>\n",
       "      <th>geeky-pumpkin-moorhen-important</th>\n",
       "      <th>crabby-teal-otter-unsorted</th>\n",
       "      <th>...</th>\n",
       "      <th>beady-mauve-frog-distraction</th>\n",
       "      <th>surly-brass-maltese-ordinal</th>\n",
       "      <th>beady-asparagus-opossum-expert</th>\n",
       "      <th>beady-rust-impala-dummy</th>\n",
       "      <th>droopy-amethyst-dachshund-hint</th>\n",
       "      <th>homey-crimson-budgerigar-grandmaster</th>\n",
       "      <th>droopy-cardinal-impala-important</th>\n",
       "      <th>woozy-apricot-moose-hint</th>\n",
       "      <th>paltry-sapphire-labradoodle-dummy</th>\n",
       "      <th>crappy-carmine-eagle-entropy</th>\n",
       "      <th>greasy-magnolia-spider-grandmaster</th>\n",
       "      <th>crabby-carmine-flounder-sorted</th>\n",
       "      <th>skimpy-copper-fowl-grandmaster</th>\n",
       "      <th>hasty-seashell-woodpecker-hint</th>\n",
       "      <th>snappy-purple-bobcat-important</th>\n",
       "      <th>thirsty-carmine-corgi-ordinal</th>\n",
       "      <th>homely-auburn-reindeer-unsorted</th>\n",
       "      <th>crappy-beige-tiger-fepid</th>\n",
       "      <th>cranky-auburn-swan-novice</th>\n",
       "      <th>chewy-bistre-buzzard-expert</th>\n",
       "      <th>skinny-cyan-macaque-pembus</th>\n",
       "      <th>slimy-periwinkle-otter-expert</th>\n",
       "      <th>snazzy-burgundy-clam-novice</th>\n",
       "      <th>cozy-ochre-gorilla-gaussian</th>\n",
       "      <th>homey-sangria-wolfhound-dummy</th>\n",
       "      <th>snazzy-asparagus-hippopotamus-contributor</th>\n",
       "      <th>paltry-red-hamster-sorted</th>\n",
       "      <th>zippy-dandelion-insect-golden</th>\n",
       "      <th>baggy-coral-bandicoot-unsorted</th>\n",
       "      <th>goopy-lavender-wolverine-fimbus</th>\n",
       "      <th>wheezy-myrtle-mandrill-entropy</th>\n",
       "      <th>wiggy-lilac-lemming-sorted</th>\n",
       "      <th>gloppy-cerise-snail-contributor</th>\n",
       "      <th>woozy-silver-havanese-gaussian</th>\n",
       "      <th>jumpy-thistle-discus-sorted</th>\n",
       "      <th>muggy-turquoise-donkey-important</th>\n",
       "      <th>blurry-buff-hyena-entropy</th>\n",
       "      <th>bluesy-chocolate-kudu-fepid</th>\n",
       "      <th>gamy-white-monster-expert</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>707b395ecdcbb4dc2eabea00e4d1b179</td>\n",
       "      <td>-2.070654</td>\n",
       "      <td>1.018160</td>\n",
       "      <td>0.228643</td>\n",
       "      <td>0.857221</td>\n",
       "      <td>0.052271</td>\n",
       "      <td>0.230303</td>\n",
       "      <td>-6.385090</td>\n",
       "      <td>0.439369</td>\n",
       "      <td>-0.721946</td>\n",
       "      <td>-0.227027</td>\n",
       "      <td>0.575964</td>\n",
       "      <td>1.541908</td>\n",
       "      <td>1.745286</td>\n",
       "      <td>-0.624271</td>\n",
       "      <td>3.600958</td>\n",
       "      <td>1.176489</td>\n",
       "      <td>-0.182776</td>\n",
       "      <td>-0.228391</td>\n",
       "      <td>1.682263</td>\n",
       "      <td>-0.833236</td>\n",
       "      <td>-4.377688</td>\n",
       "      <td>-5.372410</td>\n",
       "      <td>-0.477742</td>\n",
       "      <td>-0.179005</td>\n",
       "      <td>-0.516475</td>\n",
       "      <td>0.127391</td>\n",
       "      <td>-0.857591</td>\n",
       "      <td>-0.461500</td>\n",
       "      <td>2.160303</td>\n",
       "      <td>-2.118371</td>\n",
       "      <td>0.515493</td>\n",
       "      <td>-1.201493</td>\n",
       "      <td>-0.027377</td>\n",
       "      <td>-1.154024</td>\n",
       "      <td>0.753204</td>\n",
       "      <td>-0.179651</td>\n",
       "      <td>-0.807341</td>\n",
       "      <td>-1.663626</td>\n",
       "      <td>0.893806</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.829848</td>\n",
       "      <td>2.347131</td>\n",
       "      <td>0.082462</td>\n",
       "      <td>-1.012654</td>\n",
       "      <td>0.593752</td>\n",
       "      <td>2.904654</td>\n",
       "      <td>-0.428974</td>\n",
       "      <td>-0.919979</td>\n",
       "      <td>2.849575</td>\n",
       "      <td>-0.906744</td>\n",
       "      <td>0.729459</td>\n",
       "      <td>0.386140</td>\n",
       "      <td>0.319814</td>\n",
       "      <td>-0.407682</td>\n",
       "      <td>-0.170667</td>\n",
       "      <td>-1.242919</td>\n",
       "      <td>-1.719046</td>\n",
       "      <td>-0.132395</td>\n",
       "      <td>-0.368991</td>\n",
       "      <td>-5.112553</td>\n",
       "      <td>-2.085988</td>\n",
       "      <td>-0.897257</td>\n",
       "      <td>1.080671</td>\n",
       "      <td>-0.273262</td>\n",
       "      <td>0.342824</td>\n",
       "      <td>0.640177</td>\n",
       "      <td>-0.415298</td>\n",
       "      <td>-0.483126</td>\n",
       "      <td>-0.080799</td>\n",
       "      <td>2.416224</td>\n",
       "      <td>0.351895</td>\n",
       "      <td>0.618824</td>\n",
       "      <td>-1.542423</td>\n",
       "      <td>0.598175</td>\n",
       "      <td>0.611757</td>\n",
       "      <td>0.678772</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>-0.806677</td>\n",
       "      <td>-0.193649</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5880c03c6582a7b42248668e56b4bdec</td>\n",
       "      <td>-0.491702</td>\n",
       "      <td>0.082645</td>\n",
       "      <td>-0.011193</td>\n",
       "      <td>1.071266</td>\n",
       "      <td>-0.346347</td>\n",
       "      <td>-0.082209</td>\n",
       "      <td>0.110579</td>\n",
       "      <td>-0.382374</td>\n",
       "      <td>-0.229620</td>\n",
       "      <td>0.783980</td>\n",
       "      <td>-1.280579</td>\n",
       "      <td>-1.003480</td>\n",
       "      <td>-7.753201</td>\n",
       "      <td>-1.320547</td>\n",
       "      <td>0.919078</td>\n",
       "      <td>-1.036068</td>\n",
       "      <td>0.030213</td>\n",
       "      <td>0.910172</td>\n",
       "      <td>-0.905345</td>\n",
       "      <td>0.646641</td>\n",
       "      <td>-0.465291</td>\n",
       "      <td>-0.531735</td>\n",
       "      <td>-0.756781</td>\n",
       "      <td>0.193724</td>\n",
       "      <td>0.224277</td>\n",
       "      <td>-0.474412</td>\n",
       "      <td>1.885805</td>\n",
       "      <td>0.205439</td>\n",
       "      <td>-6.481422</td>\n",
       "      <td>1.035620</td>\n",
       "      <td>-0.453623</td>\n",
       "      <td>0.375936</td>\n",
       "      <td>-0.320670</td>\n",
       "      <td>-0.144646</td>\n",
       "      <td>-0.220129</td>\n",
       "      <td>0.577826</td>\n",
       "      <td>-0.360512</td>\n",
       "      <td>-0.600107</td>\n",
       "      <td>0.008111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.982205</td>\n",
       "      <td>-1.161978</td>\n",
       "      <td>0.532269</td>\n",
       "      <td>1.133215</td>\n",
       "      <td>0.003503</td>\n",
       "      <td>-1.390962</td>\n",
       "      <td>0.158572</td>\n",
       "      <td>0.143794</td>\n",
       "      <td>-0.317185</td>\n",
       "      <td>1.017192</td>\n",
       "      <td>-0.395342</td>\n",
       "      <td>-0.642357</td>\n",
       "      <td>-0.627209</td>\n",
       "      <td>0.257271</td>\n",
       "      <td>-1.461564</td>\n",
       "      <td>0.325613</td>\n",
       "      <td>1.628369</td>\n",
       "      <td>0.640040</td>\n",
       "      <td>0.750735</td>\n",
       "      <td>1.164573</td>\n",
       "      <td>0.900373</td>\n",
       "      <td>0.063489</td>\n",
       "      <td>0.948158</td>\n",
       "      <td>0.273014</td>\n",
       "      <td>-1.269147</td>\n",
       "      <td>-0.251101</td>\n",
       "      <td>-2.271731</td>\n",
       "      <td>-0.044167</td>\n",
       "      <td>-0.443766</td>\n",
       "      <td>-1.144794</td>\n",
       "      <td>-0.645115</td>\n",
       "      <td>-1.246090</td>\n",
       "      <td>2.613357</td>\n",
       "      <td>-0.479664</td>\n",
       "      <td>1.581289</td>\n",
       "      <td>0.931258</td>\n",
       "      <td>0.151937</td>\n",
       "      <td>-0.766595</td>\n",
       "      <td>0.474351</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4ccbcb3d13e5072ff1d9c61afe2c4f77</td>\n",
       "      <td>-1.680473</td>\n",
       "      <td>0.860529</td>\n",
       "      <td>-1.076195</td>\n",
       "      <td>0.740124</td>\n",
       "      <td>3.678445</td>\n",
       "      <td>0.288558</td>\n",
       "      <td>0.515875</td>\n",
       "      <td>0.920590</td>\n",
       "      <td>-1.223277</td>\n",
       "      <td>-1.029780</td>\n",
       "      <td>-2.203397</td>\n",
       "      <td>-7.088717</td>\n",
       "      <td>0.438218</td>\n",
       "      <td>-0.848173</td>\n",
       "      <td>1.542666</td>\n",
       "      <td>-2.166858</td>\n",
       "      <td>-0.867670</td>\n",
       "      <td>-0.980947</td>\n",
       "      <td>0.567793</td>\n",
       "      <td>1.323430</td>\n",
       "      <td>-2.076700</td>\n",
       "      <td>-0.291598</td>\n",
       "      <td>-1.564816</td>\n",
       "      <td>-8.718695</td>\n",
       "      <td>0.340144</td>\n",
       "      <td>-0.566402</td>\n",
       "      <td>0.844324</td>\n",
       "      <td>0.816421</td>\n",
       "      <td>-1.019114</td>\n",
       "      <td>-0.881431</td>\n",
       "      <td>-2.285710</td>\n",
       "      <td>-0.090958</td>\n",
       "      <td>-0.898440</td>\n",
       "      <td>-0.584417</td>\n",
       "      <td>-0.143660</td>\n",
       "      <td>-0.182084</td>\n",
       "      <td>0.798516</td>\n",
       "      <td>0.010756</td>\n",
       "      <td>-0.347155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.829467</td>\n",
       "      <td>0.588236</td>\n",
       "      <td>0.427946</td>\n",
       "      <td>-0.563037</td>\n",
       "      <td>-0.103990</td>\n",
       "      <td>-0.817698</td>\n",
       "      <td>1.251046</td>\n",
       "      <td>-0.977157</td>\n",
       "      <td>2.732600</td>\n",
       "      <td>1.997984</td>\n",
       "      <td>-0.214285</td>\n",
       "      <td>-0.389428</td>\n",
       "      <td>-1.007633</td>\n",
       "      <td>0.336435</td>\n",
       "      <td>-0.851292</td>\n",
       "      <td>-0.024184</td>\n",
       "      <td>0.455908</td>\n",
       "      <td>0.458753</td>\n",
       "      <td>-0.267230</td>\n",
       "      <td>-2.032402</td>\n",
       "      <td>0.203082</td>\n",
       "      <td>0.654107</td>\n",
       "      <td>-3.512338</td>\n",
       "      <td>-0.840937</td>\n",
       "      <td>0.519407</td>\n",
       "      <td>-0.028053</td>\n",
       "      <td>-1.621083</td>\n",
       "      <td>0.142132</td>\n",
       "      <td>1.514664</td>\n",
       "      <td>0.828815</td>\n",
       "      <td>0.516422</td>\n",
       "      <td>0.130521</td>\n",
       "      <td>-0.459210</td>\n",
       "      <td>2.028205</td>\n",
       "      <td>-0.093968</td>\n",
       "      <td>-0.218274</td>\n",
       "      <td>-0.163136</td>\n",
       "      <td>-0.870289</td>\n",
       "      <td>0.064038</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e350f17a357f12a1941f0837afb7eb8d</td>\n",
       "      <td>0.183774</td>\n",
       "      <td>0.919134</td>\n",
       "      <td>-0.946958</td>\n",
       "      <td>0.918492</td>\n",
       "      <td>0.862278</td>\n",
       "      <td>1.155287</td>\n",
       "      <td>0.911106</td>\n",
       "      <td>0.562598</td>\n",
       "      <td>-1.349685</td>\n",
       "      <td>-1.182729</td>\n",
       "      <td>0.003159</td>\n",
       "      <td>-0.626847</td>\n",
       "      <td>0.368980</td>\n",
       "      <td>1.560784</td>\n",
       "      <td>0.502851</td>\n",
       "      <td>-0.108050</td>\n",
       "      <td>0.633208</td>\n",
       "      <td>-0.411502</td>\n",
       "      <td>-3.201592</td>\n",
       "      <td>-0.710612</td>\n",
       "      <td>0.786816</td>\n",
       "      <td>0.500979</td>\n",
       "      <td>-1.040048</td>\n",
       "      <td>-1.369170</td>\n",
       "      <td>0.987666</td>\n",
       "      <td>-0.681838</td>\n",
       "      <td>-0.331372</td>\n",
       "      <td>2.254289</td>\n",
       "      <td>-0.009330</td>\n",
       "      <td>2.007067</td>\n",
       "      <td>1.203750</td>\n",
       "      <td>-2.003928</td>\n",
       "      <td>-0.566088</td>\n",
       "      <td>0.223452</td>\n",
       "      <td>0.434202</td>\n",
       "      <td>-1.203766</td>\n",
       "      <td>-0.103490</td>\n",
       "      <td>0.441111</td>\n",
       "      <td>1.818458</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.231836</td>\n",
       "      <td>0.833236</td>\n",
       "      <td>-0.454226</td>\n",
       "      <td>-1.614694</td>\n",
       "      <td>0.159948</td>\n",
       "      <td>-0.150059</td>\n",
       "      <td>-1.570599</td>\n",
       "      <td>0.960839</td>\n",
       "      <td>0.102214</td>\n",
       "      <td>0.077236</td>\n",
       "      <td>0.852834</td>\n",
       "      <td>-1.265608</td>\n",
       "      <td>-3.219190</td>\n",
       "      <td>0.251194</td>\n",
       "      <td>0.215861</td>\n",
       "      <td>-0.009520</td>\n",
       "      <td>1.611203</td>\n",
       "      <td>1.679806</td>\n",
       "      <td>-0.008419</td>\n",
       "      <td>0.658384</td>\n",
       "      <td>-0.132437</td>\n",
       "      <td>-1.466823</td>\n",
       "      <td>-1.577080</td>\n",
       "      <td>-0.800346</td>\n",
       "      <td>1.960795</td>\n",
       "      <td>-4.042900</td>\n",
       "      <td>1.722143</td>\n",
       "      <td>-0.261888</td>\n",
       "      <td>-1.145005</td>\n",
       "      <td>-1.864582</td>\n",
       "      <td>-1.168967</td>\n",
       "      <td>1.385089</td>\n",
       "      <td>-0.353028</td>\n",
       "      <td>3.316150</td>\n",
       "      <td>-0.524087</td>\n",
       "      <td>-0.794327</td>\n",
       "      <td>3.936365</td>\n",
       "      <td>0.682989</td>\n",
       "      <td>-2.521211</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a8f910ea6075b6376af079055965ff68</td>\n",
       "      <td>-0.203933</td>\n",
       "      <td>-0.177252</td>\n",
       "      <td>0.368074</td>\n",
       "      <td>-0.701320</td>\n",
       "      <td>-1.104391</td>\n",
       "      <td>0.735760</td>\n",
       "      <td>0.894273</td>\n",
       "      <td>-1.375826</td>\n",
       "      <td>-5.144946</td>\n",
       "      <td>-2.048711</td>\n",
       "      <td>0.629773</td>\n",
       "      <td>-4.252669</td>\n",
       "      <td>-0.087420</td>\n",
       "      <td>-0.794367</td>\n",
       "      <td>-1.063963</td>\n",
       "      <td>0.115997</td>\n",
       "      <td>0.895180</td>\n",
       "      <td>3.184848</td>\n",
       "      <td>2.057840</td>\n",
       "      <td>-0.950821</td>\n",
       "      <td>0.961059</td>\n",
       "      <td>-1.837828</td>\n",
       "      <td>-0.437156</td>\n",
       "      <td>-0.828433</td>\n",
       "      <td>0.373747</td>\n",
       "      <td>-0.099787</td>\n",
       "      <td>-0.976280</td>\n",
       "      <td>-0.165921</td>\n",
       "      <td>3.297221</td>\n",
       "      <td>3.914132</td>\n",
       "      <td>-4.971376</td>\n",
       "      <td>-0.286520</td>\n",
       "      <td>-0.160133</td>\n",
       "      <td>-3.301453</td>\n",
       "      <td>-1.021032</td>\n",
       "      <td>-0.562744</td>\n",
       "      <td>0.574065</td>\n",
       "      <td>-0.368194</td>\n",
       "      <td>-0.507458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178099</td>\n",
       "      <td>-0.410396</td>\n",
       "      <td>-1.184236</td>\n",
       "      <td>1.681727</td>\n",
       "      <td>0.589606</td>\n",
       "      <td>0.064222</td>\n",
       "      <td>0.258885</td>\n",
       "      <td>0.560241</td>\n",
       "      <td>-1.545597</td>\n",
       "      <td>0.822283</td>\n",
       "      <td>1.518209</td>\n",
       "      <td>0.460143</td>\n",
       "      <td>0.822488</td>\n",
       "      <td>1.362718</td>\n",
       "      <td>0.218560</td>\n",
       "      <td>-1.038514</td>\n",
       "      <td>1.000763</td>\n",
       "      <td>-0.975878</td>\n",
       "      <td>-0.551268</td>\n",
       "      <td>-0.133044</td>\n",
       "      <td>-0.393092</td>\n",
       "      <td>1.236473</td>\n",
       "      <td>1.657100</td>\n",
       "      <td>0.833020</td>\n",
       "      <td>0.665379</td>\n",
       "      <td>-0.900025</td>\n",
       "      <td>0.291908</td>\n",
       "      <td>0.482727</td>\n",
       "      <td>0.552399</td>\n",
       "      <td>0.970496</td>\n",
       "      <td>-0.279168</td>\n",
       "      <td>1.544356</td>\n",
       "      <td>2.959727</td>\n",
       "      <td>1.641201</td>\n",
       "      <td>-0.130818</td>\n",
       "      <td>-0.264292</td>\n",
       "      <td>-0.748668</td>\n",
       "      <td>0.964218</td>\n",
       "      <td>0.087079</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id   ...    target\n",
       "0  707b395ecdcbb4dc2eabea00e4d1b179   ...         0\n",
       "1  5880c03c6582a7b42248668e56b4bdec   ...         0\n",
       "2  4ccbcb3d13e5072ff1d9c61afe2c4f77   ...         1\n",
       "3  e350f17a357f12a1941f0837afb7eb8d   ...         0\n",
       "4  a8f910ea6075b6376af079055965ff68   ...         0\n",
       "\n",
       "[5 rows x 258 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, os\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from tqdm import tqdm\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "from sklearn.covariance import GraphicalLasso\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_cov(x,y):\n",
    "    model = GraphicalLasso()\n",
    "    ones = (y==1).astype(bool)\n",
    "    x2 = x[ones]\n",
    "    model.fit(x2)\n",
    "    p1 = model.precision_\n",
    "    m1 = model.location_\n",
    "    \n",
    "    onesb = (y==0).astype(bool)\n",
    "    x2b = x[onesb]\n",
    "    model.fit(x2b)\n",
    "    p2 = model.precision_\n",
    "    m2 = model.location_\n",
    "    \n",
    "    ms = np.stack([m1,m2])\n",
    "    ps = np.stack([p1,p2])\n",
    "    return ms,ps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/512 [00:00<?, ?it/s]/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: 1.327e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      " 13%|█▎        | 68/512 [02:42<17:04,  2.31s/it]/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: 1.271e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      " 16%|█▌        | 80/512 [03:08<15:10,  2.11s/it]/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: 1.834e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      " 19%|█▉        | 97/512 [03:49<15:51,  2.29s/it]/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: 1.377e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      " 65%|██████▍   | 332/512 [13:12<06:50,  2.28s/it]/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: 1.144e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      " 73%|███████▎  | 375/512 [14:52<05:07,  2.25s/it]/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: -2.243e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: -1.275e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: -2.746e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: -1.973e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      " 90%|█████████ | 463/512 [18:18<01:56,  2.39s/it]/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: 1.310e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: 1.542e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: 1.152e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: 1.404e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: 2.338e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: 2.551e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      "100%|██████████| 512/512 [20:20<00:00,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA scores CV = 0.96749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# INITIALIZE VARIABLES\n",
    "cols = [c for c in train.columns if c not in ['id', 'target']]\n",
    "cols.remove('wheezy-copper-turtle-magic')\n",
    "oof = np.zeros(len(train))\n",
    "preds = np.zeros(len(test))\n",
    "\n",
    "# BUILD 512 SEPARATE MODELS\n",
    "for i in tqdm(range(512)):\n",
    "    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n",
    "    train2 = train[train['wheezy-copper-turtle-magic']==i]\n",
    "    test2 = test[test['wheezy-copper-turtle-magic']==i]\n",
    "    idx1 = train2.index; idx2 = test2.index\n",
    "    train2.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n",
    "    sel = VarianceThreshold(threshold=1.5).fit(train2[cols])\n",
    "    train3 = sel.transform(train2[cols])\n",
    "    test3 = sel.transform(test2[cols])\n",
    "    \n",
    "    # STRATIFIED K-FOLD\n",
    "    skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n",
    "    for train_index, test_index in skf.split(train3, train2['target']):\n",
    "        \n",
    "        # MODEL AND PREDICT WITH QDA\n",
    "        ms, ps = get_mean_cov(train3[train_index,:],train2.loc[train_index]['target'].values)\n",
    "        \n",
    "        gm = GaussianMixture(n_components=2, init_params='random', covariance_type='full', tol=0.001,reg_covar=0.001, max_iter=100, n_init=1,means_init=ms, precisions_init=ps)\n",
    "        gm.fit(np.concatenate([train3,test3],axis = 0))\n",
    "        oof[idx1[test_index]] = gm.predict_proba(train3[test_index,:])[:,0]\n",
    "        preds[idx2] += gm.predict_proba(test3)[:,0] / skf.n_splits\n",
    "\n",
    "        \n",
    "# PRINT CV AUC\n",
    "auc = roc_auc_score(train['target'],oof)\n",
    "print('QDA scores CV =',round(auc,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dict = dict()\n",
    "\n",
    "# INITIALIZE VARIABLES\n",
    "cols = [c for c in train.columns if c not in ['id', 'target']]\n",
    "cols.remove('wheezy-copper-turtle-magic')\n",
    "\n",
    "for i in range(512):\n",
    "\n",
    "    \n",
    "    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n",
    "    train2 = train[train['wheezy-copper-turtle-magic']==i]\n",
    "    test2 = test[test['wheezy-copper-turtle-magic']==i]\n",
    "    idx1 = train2.index; idx2 = test2.index\n",
    "    train2.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    sel = VarianceThreshold(threshold=1.5).fit(train2[cols])\n",
    "    train3 = sel.transform(train2[cols])\n",
    "    test3 = sel.transform(test2[cols])\n",
    "        \n",
    "    cat_dict[i] = train3.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2444924a58>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(list(cat_dict.items()))[1].value_counts().plot.barh()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add pseudo label data from PCA-QDA and run a new model Variance-QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pseudo Labeled QDA scores CV = 0.96918\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# INITIALIZE VARIABLES\n",
    "test['target'] = preds\n",
    "oof_var = np.zeros(len(train))\n",
    "preds_var = np.zeros(len(test))\n",
    "\n",
    "# BUILD 512 SEPARATE MODELS\n",
    "for k in range(512):\n",
    "    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n",
    "    train2 = train[train['wheezy-copper-turtle-magic']==k] \n",
    "    train2p = train2.copy(); idx1 = train2.index \n",
    "    test2 = test[test['wheezy-copper-turtle-magic']==k]\n",
    "    \n",
    "    # ADD PSEUDO LABELED DATA\n",
    "    test2p = test2[ (test2['target']<=0.01) | (test2['target']>=0.99) ].copy()\n",
    "    test2p.loc[ test2p['target']>=0.5, 'target' ] = 1\n",
    "    test2p.loc[ test2p['target']<0.5, 'target' ] = 0 \n",
    "    train2p = pd.concat([train2p,test2p],axis=0)\n",
    "    train2p.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n",
    "    \n",
    "  \n",
    "    \n",
    "    pca = PCA(n_components=cat_dict[k], random_state= 1234)\n",
    "    pca.fit(train2p[cols])\n",
    "    train3p = pca.transform(train2p[cols])\n",
    "    train3 = pca.transform(train2[cols])\n",
    "    test3 = pca.transform(test2[cols])\n",
    "\n",
    "           \n",
    "        \n",
    "    # STRATIFIED K FOLD\n",
    "    skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n",
    "    for train_index, test_index in skf.split(train3p, train2p['target']):\n",
    "        test_index3 = test_index[ test_index<len(train3) ] # ignore pseudo in oof\n",
    "        \n",
    "        clf = QuadraticDiscriminantAnalysis(reg_param=0.5)\n",
    "        clf.fit(train3p[train_index,:],train2p.loc[train_index]['target'])\n",
    "        oof_var[idx1[test_index3]] += clf.predict_proba(train3[test_index3,:])[:,1]\n",
    "        preds_var[test2.index] += clf.predict_proba(test3)[:,1] / skf.n_splits\n",
    "       \n",
    "       \n",
    "    #if k%64==0: print(k)\n",
    "        \n",
    "# PRINT CV AUC\n",
    "auc = roc_auc_score(train['target'],oof_var)\n",
    "print('Pseudo Labeled QDA scores CV =',round(auc,5)) #0.97035\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pseudo Labeled QDA scores CV = 0.96964\n"
     ]
    }
   ],
   "source": [
    "# INITIALIZE VARIABLES\n",
    "test['target'] = preds_var  \n",
    "oof_var2 = np.zeros(len(train))\n",
    "preds_var2 = np.zeros(len(test))\n",
    "\n",
    "# BUILD 512 SEPARATE MODELS\n",
    "for k in range(512):\n",
    "    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n",
    "    train2 = train[train['wheezy-copper-turtle-magic']==k] \n",
    "    train2p = train2.copy(); idx1 = train2.index \n",
    "    test2 = test[test['wheezy-copper-turtle-magic']==k]\n",
    "    \n",
    "    # ADD PSEUDO LABELED DATA\n",
    "    test2p = test2[ (test2['target']<=0.01) | (test2['target']>=0.99) ].copy()\n",
    "    test2p.loc[ test2p['target']>=0.5, 'target' ] = 1\n",
    "    test2p.loc[ test2p['target']<0.5, 'target' ] = 0 \n",
    "    train2p = pd.concat([train2p,test2p],axis=0)\n",
    "    train2p.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n",
    "    \n",
    "    \n",
    "    \n",
    "       \n",
    "    sel = VarianceThreshold(threshold=1.5).fit(train2p[cols])     \n",
    "    train3p = sel.transform(train2p[cols])\n",
    "    train3 = sel.transform(train2[cols])\n",
    "    test3 = sel.transform(test2[cols])\n",
    "           \n",
    "        \n",
    "    # STRATIFIED K FOLD\n",
    "    skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n",
    "    for train_index, test_index in skf.split(train3p, train2p['target']):\n",
    "        test_index3 = test_index[ test_index<len(train3) ] # ignore pseudo in oof\n",
    "        \n",
    "        clf = QuadraticDiscriminantAnalysis(reg_param=0.5)\n",
    "        clf.fit(train3p[train_index,:],train2p.loc[train_index]['target'])\n",
    "        oof_var2[idx1[test_index3]] += clf.predict_proba(train3[test_index3,:])[:,1]\n",
    "        preds_var2[test2.index] += clf.predict_proba(test3)[:,1] / skf.n_splits\n",
    "       \n",
    "       \n",
    "    #if k%64==0: print(k)\n",
    "        \n",
    "# PRINT CV AUC\n",
    "auc = roc_auc_score(train['target'],oof_var2)\n",
    "print('Pseudo Labeled QDA scores CV =',round(auc,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pseudo Labeled QDA scores CV = 0.96959\n"
     ]
    }
   ],
   "source": [
    "auc = roc_auc_score(train['target'],0.5*(oof_var+ oof_var2) )\n",
    "print('Pseudo Labeled QDA scores CV =',round(auc,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sub = pd.read_csv('../input/sample_submission.csv')\n",
    "sub['target'] = 0.5* preds_var + 0.5*preds_var2\n",
    "sub.to_csv('submission.csv',index=False)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(preds,bins=100)\n",
    "plt.title('Final Test.csv predictions')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
