{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "67f1ef572ea1f522bcdc6a775bf629ee740d87ac"
   },
   "source": [
    "# Bayesian global optimization with gaussian processes for finding (sub-)optimal parameters of LightGBM\n",
    "\n",
    "As many of fellow kaggler asking how did I get LightGBM parameters for the kernel [Customer Transaction Prediction](https://www.kaggle.com/fayzur/customer-transaction-prediction) I published. So, I decided to publish a kernel to optimize parameters. \n",
    "\n",
    "\n",
    "\n",
    "In this kernel I use Bayesian global optimization with gaussian processes for finding optimal parameters. This optimization attempts to find the maximum value of an black box function in as few iterations as possible. In our case the black box function will be a function that I will write to optimize (maximize) the evaluation function (AUC) so that parameters get maximize AUC in training and validation, and expect to do good in the private. The final prediction will be **rank average on 5 fold cross validation predictions**.\n",
    "\n",
    "Continue to the end of this kernel and **upvote it if you find it is interesting**.\n",
    "\n",
    "![image.jpg](https://i.imgur.com/XKS1oqU.jpg)\n",
    "\n",
    "Image taken from : https://github.com/fmfn/BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7409352edc21584713b1225028f087c6989e94be"
   },
   "source": [
    "## Notebook  Content\n",
    "0. [Installing Bayesian global optimization library](#0) <br>    \n",
    "1. [Loading the data](#1)\n",
    "2. [Black box function to be optimized (LightGBM)](#2)\n",
    "3. [Training LightGBM model](#3)\n",
    "4. [Rank averaging](#4)\n",
    "5. [Submission](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ea005def562ae65202ec9322bec60fd25a1961e1"
   },
   "source": [
    "<a id=\"0\"></a> <br>\n",
    "## 0. Installing Bayesian global optimization library\n",
    "\n",
    "Let's install the latest release from pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "81c2a7386319cae39c1cf83d39394d530f549128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bayesian-optimization in /opt/conda/lib/python3.6/site-packages (1.0.1)\r\n",
      "Requirement already satisfied: scipy>=0.14.0 in /opt/conda/lib/python3.6/site-packages (from bayesian-optimization) (1.1.0)\r\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in /opt/conda/lib/python3.6/site-packages (from bayesian-optimization) (0.20.2)\r\n",
      "Requirement already satisfied: numpy>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from bayesian-optimization) (1.16.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ea005def562ae65202ec9322bec60fd25a1961e1"
   },
   "source": [
    "<a id=\"1\"></a> <br>\n",
    "## 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats import rankdata\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "f5624fd428106888ec023312d3479d324ef0eac9"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/train.csv')\n",
    "\n",
    "test_df = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba5573cfaec6625aed13e98c6e034809e2997b5b"
   },
   "source": [
    "We are given anonymized dataset containing 200 numeric feature variables from var_0 to var_199. Let's have a look train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "7365ac9a050f611cb284bbb47519e04ac1ee19f9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>var_11</th>\n",
       "      <th>var_12</th>\n",
       "      <th>var_13</th>\n",
       "      <th>var_14</th>\n",
       "      <th>var_15</th>\n",
       "      <th>var_16</th>\n",
       "      <th>var_17</th>\n",
       "      <th>var_18</th>\n",
       "      <th>var_19</th>\n",
       "      <th>var_20</th>\n",
       "      <th>var_21</th>\n",
       "      <th>var_22</th>\n",
       "      <th>var_23</th>\n",
       "      <th>var_24</th>\n",
       "      <th>var_25</th>\n",
       "      <th>var_26</th>\n",
       "      <th>var_27</th>\n",
       "      <th>var_28</th>\n",
       "      <th>var_29</th>\n",
       "      <th>var_30</th>\n",
       "      <th>var_31</th>\n",
       "      <th>var_32</th>\n",
       "      <th>var_33</th>\n",
       "      <th>var_34</th>\n",
       "      <th>var_35</th>\n",
       "      <th>var_36</th>\n",
       "      <th>var_37</th>\n",
       "      <th>var_38</th>\n",
       "      <th>var_39</th>\n",
       "      <th>var_40</th>\n",
       "      <th>var_41</th>\n",
       "      <th>var_42</th>\n",
       "      <th>var_43</th>\n",
       "      <th>var_44</th>\n",
       "      <th>var_45</th>\n",
       "      <th>var_46</th>\n",
       "      <th>var_47</th>\n",
       "      <th>var_48</th>\n",
       "      <th>var_49</th>\n",
       "      <th>var_50</th>\n",
       "      <th>var_51</th>\n",
       "      <th>var_52</th>\n",
       "      <th>var_53</th>\n",
       "      <th>var_54</th>\n",
       "      <th>var_55</th>\n",
       "      <th>var_56</th>\n",
       "      <th>var_57</th>\n",
       "      <th>var_58</th>\n",
       "      <th>var_59</th>\n",
       "      <th>var_60</th>\n",
       "      <th>var_61</th>\n",
       "      <th>var_62</th>\n",
       "      <th>var_63</th>\n",
       "      <th>var_64</th>\n",
       "      <th>var_65</th>\n",
       "      <th>var_66</th>\n",
       "      <th>var_67</th>\n",
       "      <th>var_68</th>\n",
       "      <th>var_69</th>\n",
       "      <th>var_70</th>\n",
       "      <th>var_71</th>\n",
       "      <th>var_72</th>\n",
       "      <th>var_73</th>\n",
       "      <th>var_74</th>\n",
       "      <th>var_75</th>\n",
       "      <th>var_76</th>\n",
       "      <th>var_77</th>\n",
       "      <th>var_78</th>\n",
       "      <th>var_79</th>\n",
       "      <th>var_80</th>\n",
       "      <th>var_81</th>\n",
       "      <th>var_82</th>\n",
       "      <th>var_83</th>\n",
       "      <th>var_84</th>\n",
       "      <th>var_85</th>\n",
       "      <th>var_86</th>\n",
       "      <th>var_87</th>\n",
       "      <th>var_88</th>\n",
       "      <th>var_89</th>\n",
       "      <th>var_90</th>\n",
       "      <th>var_91</th>\n",
       "      <th>var_92</th>\n",
       "      <th>var_93</th>\n",
       "      <th>var_94</th>\n",
       "      <th>var_95</th>\n",
       "      <th>var_96</th>\n",
       "      <th>var_97</th>\n",
       "      <th>...</th>\n",
       "      <th>var_100</th>\n",
       "      <th>var_101</th>\n",
       "      <th>var_102</th>\n",
       "      <th>var_103</th>\n",
       "      <th>var_104</th>\n",
       "      <th>var_105</th>\n",
       "      <th>var_106</th>\n",
       "      <th>var_107</th>\n",
       "      <th>var_108</th>\n",
       "      <th>var_109</th>\n",
       "      <th>var_110</th>\n",
       "      <th>var_111</th>\n",
       "      <th>var_112</th>\n",
       "      <th>var_113</th>\n",
       "      <th>var_114</th>\n",
       "      <th>var_115</th>\n",
       "      <th>var_116</th>\n",
       "      <th>var_117</th>\n",
       "      <th>var_118</th>\n",
       "      <th>var_119</th>\n",
       "      <th>var_120</th>\n",
       "      <th>var_121</th>\n",
       "      <th>var_122</th>\n",
       "      <th>var_123</th>\n",
       "      <th>var_124</th>\n",
       "      <th>var_125</th>\n",
       "      <th>var_126</th>\n",
       "      <th>var_127</th>\n",
       "      <th>var_128</th>\n",
       "      <th>var_129</th>\n",
       "      <th>var_130</th>\n",
       "      <th>var_131</th>\n",
       "      <th>var_132</th>\n",
       "      <th>var_133</th>\n",
       "      <th>var_134</th>\n",
       "      <th>var_135</th>\n",
       "      <th>var_136</th>\n",
       "      <th>var_137</th>\n",
       "      <th>var_138</th>\n",
       "      <th>var_139</th>\n",
       "      <th>var_140</th>\n",
       "      <th>var_141</th>\n",
       "      <th>var_142</th>\n",
       "      <th>var_143</th>\n",
       "      <th>var_144</th>\n",
       "      <th>var_145</th>\n",
       "      <th>var_146</th>\n",
       "      <th>var_147</th>\n",
       "      <th>var_148</th>\n",
       "      <th>var_149</th>\n",
       "      <th>var_150</th>\n",
       "      <th>var_151</th>\n",
       "      <th>var_152</th>\n",
       "      <th>var_153</th>\n",
       "      <th>var_154</th>\n",
       "      <th>var_155</th>\n",
       "      <th>var_156</th>\n",
       "      <th>var_157</th>\n",
       "      <th>var_158</th>\n",
       "      <th>var_159</th>\n",
       "      <th>var_160</th>\n",
       "      <th>var_161</th>\n",
       "      <th>var_162</th>\n",
       "      <th>var_163</th>\n",
       "      <th>var_164</th>\n",
       "      <th>var_165</th>\n",
       "      <th>var_166</th>\n",
       "      <th>var_167</th>\n",
       "      <th>var_168</th>\n",
       "      <th>var_169</th>\n",
       "      <th>var_170</th>\n",
       "      <th>var_171</th>\n",
       "      <th>var_172</th>\n",
       "      <th>var_173</th>\n",
       "      <th>var_174</th>\n",
       "      <th>var_175</th>\n",
       "      <th>var_176</th>\n",
       "      <th>var_177</th>\n",
       "      <th>var_178</th>\n",
       "      <th>var_179</th>\n",
       "      <th>var_180</th>\n",
       "      <th>var_181</th>\n",
       "      <th>var_182</th>\n",
       "      <th>var_183</th>\n",
       "      <th>var_184</th>\n",
       "      <th>var_185</th>\n",
       "      <th>var_186</th>\n",
       "      <th>var_187</th>\n",
       "      <th>var_188</th>\n",
       "      <th>var_189</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>-4.9200</td>\n",
       "      <td>5.7470</td>\n",
       "      <td>2.9252</td>\n",
       "      <td>3.1821</td>\n",
       "      <td>14.0137</td>\n",
       "      <td>0.5745</td>\n",
       "      <td>8.7989</td>\n",
       "      <td>14.5691</td>\n",
       "      <td>5.7487</td>\n",
       "      <td>-7.2393</td>\n",
       "      <td>4.2840</td>\n",
       "      <td>30.7133</td>\n",
       "      <td>10.5350</td>\n",
       "      <td>16.2191</td>\n",
       "      <td>2.5791</td>\n",
       "      <td>2.4716</td>\n",
       "      <td>14.3831</td>\n",
       "      <td>13.4325</td>\n",
       "      <td>-5.1488</td>\n",
       "      <td>-0.4073</td>\n",
       "      <td>4.9306</td>\n",
       "      <td>5.9965</td>\n",
       "      <td>-0.3085</td>\n",
       "      <td>12.9041</td>\n",
       "      <td>-3.8766</td>\n",
       "      <td>16.8911</td>\n",
       "      <td>11.1920</td>\n",
       "      <td>10.5785</td>\n",
       "      <td>0.6764</td>\n",
       "      <td>7.8871</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>3.8743</td>\n",
       "      <td>-5.2387</td>\n",
       "      <td>7.3746</td>\n",
       "      <td>11.5767</td>\n",
       "      <td>12.0446</td>\n",
       "      <td>11.6418</td>\n",
       "      <td>-7.0170</td>\n",
       "      <td>5.9226</td>\n",
       "      <td>-14.2136</td>\n",
       "      <td>16.0283</td>\n",
       "      <td>5.3253</td>\n",
       "      <td>12.9194</td>\n",
       "      <td>29.0460</td>\n",
       "      <td>-0.6940</td>\n",
       "      <td>5.1736</td>\n",
       "      <td>-0.7474</td>\n",
       "      <td>14.8322</td>\n",
       "      <td>11.2668</td>\n",
       "      <td>5.3822</td>\n",
       "      <td>2.0183</td>\n",
       "      <td>10.1166</td>\n",
       "      <td>16.1828</td>\n",
       "      <td>4.9590</td>\n",
       "      <td>2.0771</td>\n",
       "      <td>-0.2154</td>\n",
       "      <td>8.6748</td>\n",
       "      <td>9.5319</td>\n",
       "      <td>5.8056</td>\n",
       "      <td>22.4321</td>\n",
       "      <td>5.0109</td>\n",
       "      <td>-4.7010</td>\n",
       "      <td>21.6374</td>\n",
       "      <td>0.5663</td>\n",
       "      <td>5.1999</td>\n",
       "      <td>8.8600</td>\n",
       "      <td>43.1127</td>\n",
       "      <td>18.3816</td>\n",
       "      <td>-2.3440</td>\n",
       "      <td>23.4104</td>\n",
       "      <td>6.5199</td>\n",
       "      <td>12.1983</td>\n",
       "      <td>13.6468</td>\n",
       "      <td>13.8372</td>\n",
       "      <td>1.3675</td>\n",
       "      <td>2.9423</td>\n",
       "      <td>-4.5213</td>\n",
       "      <td>21.4669</td>\n",
       "      <td>9.3225</td>\n",
       "      <td>16.4597</td>\n",
       "      <td>7.9984</td>\n",
       "      <td>-1.7069</td>\n",
       "      <td>-21.4494</td>\n",
       "      <td>6.7806</td>\n",
       "      <td>11.0924</td>\n",
       "      <td>9.9913</td>\n",
       "      <td>14.8421</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>8.9642</td>\n",
       "      <td>16.2572</td>\n",
       "      <td>...</td>\n",
       "      <td>9.4763</td>\n",
       "      <td>13.3102</td>\n",
       "      <td>26.5376</td>\n",
       "      <td>1.4403</td>\n",
       "      <td>14.7100</td>\n",
       "      <td>6.0454</td>\n",
       "      <td>9.5426</td>\n",
       "      <td>17.1554</td>\n",
       "      <td>14.1104</td>\n",
       "      <td>24.3627</td>\n",
       "      <td>2.0323</td>\n",
       "      <td>6.7602</td>\n",
       "      <td>3.9141</td>\n",
       "      <td>-0.4851</td>\n",
       "      <td>2.5240</td>\n",
       "      <td>1.5093</td>\n",
       "      <td>2.5516</td>\n",
       "      <td>15.5752</td>\n",
       "      <td>-13.4221</td>\n",
       "      <td>7.2739</td>\n",
       "      <td>16.0094</td>\n",
       "      <td>9.7268</td>\n",
       "      <td>0.8897</td>\n",
       "      <td>0.7754</td>\n",
       "      <td>4.2218</td>\n",
       "      <td>12.0039</td>\n",
       "      <td>13.8571</td>\n",
       "      <td>-0.7338</td>\n",
       "      <td>-1.9245</td>\n",
       "      <td>15.4462</td>\n",
       "      <td>12.8287</td>\n",
       "      <td>0.3587</td>\n",
       "      <td>9.6508</td>\n",
       "      <td>6.5674</td>\n",
       "      <td>5.1726</td>\n",
       "      <td>3.1345</td>\n",
       "      <td>29.4547</td>\n",
       "      <td>31.4045</td>\n",
       "      <td>2.8279</td>\n",
       "      <td>15.6599</td>\n",
       "      <td>8.3307</td>\n",
       "      <td>-5.6011</td>\n",
       "      <td>19.0614</td>\n",
       "      <td>11.2663</td>\n",
       "      <td>8.6989</td>\n",
       "      <td>8.3694</td>\n",
       "      <td>11.5659</td>\n",
       "      <td>-16.4727</td>\n",
       "      <td>4.0288</td>\n",
       "      <td>17.9244</td>\n",
       "      <td>18.5177</td>\n",
       "      <td>10.7800</td>\n",
       "      <td>9.0056</td>\n",
       "      <td>16.6964</td>\n",
       "      <td>10.4838</td>\n",
       "      <td>1.6573</td>\n",
       "      <td>12.1749</td>\n",
       "      <td>-13.1324</td>\n",
       "      <td>17.6054</td>\n",
       "      <td>11.5423</td>\n",
       "      <td>15.4576</td>\n",
       "      <td>5.3133</td>\n",
       "      <td>3.6159</td>\n",
       "      <td>5.0384</td>\n",
       "      <td>6.6760</td>\n",
       "      <td>12.6644</td>\n",
       "      <td>2.7004</td>\n",
       "      <td>-0.6975</td>\n",
       "      <td>9.5981</td>\n",
       "      <td>5.4879</td>\n",
       "      <td>-4.7645</td>\n",
       "      <td>-8.4254</td>\n",
       "      <td>20.8773</td>\n",
       "      <td>3.1531</td>\n",
       "      <td>18.5618</td>\n",
       "      <td>7.7423</td>\n",
       "      <td>-10.1245</td>\n",
       "      <td>13.7241</td>\n",
       "      <td>-3.5189</td>\n",
       "      <td>1.7202</td>\n",
       "      <td>-8.4051</td>\n",
       "      <td>9.0164</td>\n",
       "      <td>3.0657</td>\n",
       "      <td>14.3691</td>\n",
       "      <td>25.8398</td>\n",
       "      <td>5.8764</td>\n",
       "      <td>11.8411</td>\n",
       "      <td>-19.7159</td>\n",
       "      <td>17.5743</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>3.1468</td>\n",
       "      <td>8.0851</td>\n",
       "      <td>-0.4032</td>\n",
       "      <td>8.0585</td>\n",
       "      <td>14.0239</td>\n",
       "      <td>8.4135</td>\n",
       "      <td>5.4345</td>\n",
       "      <td>13.7003</td>\n",
       "      <td>13.8275</td>\n",
       "      <td>-15.5849</td>\n",
       "      <td>7.8000</td>\n",
       "      <td>28.5708</td>\n",
       "      <td>3.4287</td>\n",
       "      <td>2.7407</td>\n",
       "      <td>8.5524</td>\n",
       "      <td>3.3716</td>\n",
       "      <td>6.9779</td>\n",
       "      <td>13.8910</td>\n",
       "      <td>-11.7684</td>\n",
       "      <td>-2.5586</td>\n",
       "      <td>5.0464</td>\n",
       "      <td>0.5481</td>\n",
       "      <td>-9.2987</td>\n",
       "      <td>7.8755</td>\n",
       "      <td>1.2859</td>\n",
       "      <td>19.3710</td>\n",
       "      <td>11.3702</td>\n",
       "      <td>0.7399</td>\n",
       "      <td>2.7995</td>\n",
       "      <td>5.8434</td>\n",
       "      <td>10.8160</td>\n",
       "      <td>3.6783</td>\n",
       "      <td>-11.1147</td>\n",
       "      <td>1.8730</td>\n",
       "      <td>9.8775</td>\n",
       "      <td>11.7842</td>\n",
       "      <td>1.2444</td>\n",
       "      <td>-47.3797</td>\n",
       "      <td>7.3718</td>\n",
       "      <td>0.1948</td>\n",
       "      <td>34.4014</td>\n",
       "      <td>25.7037</td>\n",
       "      <td>11.8343</td>\n",
       "      <td>13.2256</td>\n",
       "      <td>-4.1083</td>\n",
       "      <td>6.6885</td>\n",
       "      <td>-8.0946</td>\n",
       "      <td>18.5995</td>\n",
       "      <td>19.3219</td>\n",
       "      <td>7.0118</td>\n",
       "      <td>1.9210</td>\n",
       "      <td>8.8682</td>\n",
       "      <td>8.0109</td>\n",
       "      <td>-7.2417</td>\n",
       "      <td>1.7944</td>\n",
       "      <td>-1.3147</td>\n",
       "      <td>8.1042</td>\n",
       "      <td>1.5365</td>\n",
       "      <td>5.4007</td>\n",
       "      <td>7.9344</td>\n",
       "      <td>5.0220</td>\n",
       "      <td>2.2302</td>\n",
       "      <td>40.5632</td>\n",
       "      <td>0.5134</td>\n",
       "      <td>3.1701</td>\n",
       "      <td>20.1068</td>\n",
       "      <td>7.7841</td>\n",
       "      <td>7.0529</td>\n",
       "      <td>3.2709</td>\n",
       "      <td>23.4822</td>\n",
       "      <td>5.5075</td>\n",
       "      <td>13.7814</td>\n",
       "      <td>2.5462</td>\n",
       "      <td>18.1782</td>\n",
       "      <td>0.3683</td>\n",
       "      <td>-4.8210</td>\n",
       "      <td>-5.4850</td>\n",
       "      <td>13.7867</td>\n",
       "      <td>-13.5901</td>\n",
       "      <td>11.0993</td>\n",
       "      <td>7.9022</td>\n",
       "      <td>12.2301</td>\n",
       "      <td>0.4768</td>\n",
       "      <td>6.8852</td>\n",
       "      <td>8.0905</td>\n",
       "      <td>10.9631</td>\n",
       "      <td>11.7569</td>\n",
       "      <td>-1.2722</td>\n",
       "      <td>24.7876</td>\n",
       "      <td>26.6881</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.6950</td>\n",
       "      <td>8.4068</td>\n",
       "      <td>35.4734</td>\n",
       "      <td>1.7093</td>\n",
       "      <td>15.1866</td>\n",
       "      <td>2.6227</td>\n",
       "      <td>7.3412</td>\n",
       "      <td>32.0888</td>\n",
       "      <td>13.9550</td>\n",
       "      <td>13.0858</td>\n",
       "      <td>6.6203</td>\n",
       "      <td>7.1051</td>\n",
       "      <td>5.3523</td>\n",
       "      <td>8.5426</td>\n",
       "      <td>3.6159</td>\n",
       "      <td>4.1569</td>\n",
       "      <td>3.0454</td>\n",
       "      <td>7.8522</td>\n",
       "      <td>-11.5100</td>\n",
       "      <td>7.5109</td>\n",
       "      <td>31.5899</td>\n",
       "      <td>9.5018</td>\n",
       "      <td>8.2736</td>\n",
       "      <td>10.1633</td>\n",
       "      <td>0.1225</td>\n",
       "      <td>12.5942</td>\n",
       "      <td>14.5697</td>\n",
       "      <td>2.4354</td>\n",
       "      <td>0.8194</td>\n",
       "      <td>16.5346</td>\n",
       "      <td>12.4205</td>\n",
       "      <td>-0.1780</td>\n",
       "      <td>5.7582</td>\n",
       "      <td>7.0513</td>\n",
       "      <td>1.9568</td>\n",
       "      <td>-8.9921</td>\n",
       "      <td>9.7797</td>\n",
       "      <td>18.1577</td>\n",
       "      <td>-1.9721</td>\n",
       "      <td>16.1622</td>\n",
       "      <td>3.6937</td>\n",
       "      <td>6.6803</td>\n",
       "      <td>-0.3243</td>\n",
       "      <td>12.2806</td>\n",
       "      <td>8.6086</td>\n",
       "      <td>11.0738</td>\n",
       "      <td>8.9231</td>\n",
       "      <td>11.7700</td>\n",
       "      <td>4.2578</td>\n",
       "      <td>-4.4223</td>\n",
       "      <td>20.6294</td>\n",
       "      <td>14.8743</td>\n",
       "      <td>9.4317</td>\n",
       "      <td>16.7242</td>\n",
       "      <td>-0.5687</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>12.2419</td>\n",
       "      <td>-9.6953</td>\n",
       "      <td>22.3949</td>\n",
       "      <td>10.6261</td>\n",
       "      <td>29.4846</td>\n",
       "      <td>5.8683</td>\n",
       "      <td>3.8208</td>\n",
       "      <td>15.8348</td>\n",
       "      <td>-5.0121</td>\n",
       "      <td>15.1345</td>\n",
       "      <td>3.2003</td>\n",
       "      <td>9.3192</td>\n",
       "      <td>3.8821</td>\n",
       "      <td>5.7999</td>\n",
       "      <td>5.5378</td>\n",
       "      <td>5.0988</td>\n",
       "      <td>22.0330</td>\n",
       "      <td>5.5134</td>\n",
       "      <td>30.2645</td>\n",
       "      <td>10.4968</td>\n",
       "      <td>-7.2352</td>\n",
       "      <td>16.5721</td>\n",
       "      <td>-7.3477</td>\n",
       "      <td>11.0752</td>\n",
       "      <td>-5.5937</td>\n",
       "      <td>9.4878</td>\n",
       "      <td>-14.9100</td>\n",
       "      <td>9.4245</td>\n",
       "      <td>22.5441</td>\n",
       "      <td>-4.8622</td>\n",
       "      <td>7.6543</td>\n",
       "      <td>-15.9319</td>\n",
       "      <td>13.3175</td>\n",
       "      <td>-0.3566</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>-4.9193</td>\n",
       "      <td>5.9525</td>\n",
       "      <td>-0.3249</td>\n",
       "      <td>-11.2648</td>\n",
       "      <td>14.1929</td>\n",
       "      <td>7.3124</td>\n",
       "      <td>7.5244</td>\n",
       "      <td>14.6472</td>\n",
       "      <td>7.6782</td>\n",
       "      <td>-1.7395</td>\n",
       "      <td>4.7011</td>\n",
       "      <td>20.4775</td>\n",
       "      <td>17.7559</td>\n",
       "      <td>18.1377</td>\n",
       "      <td>1.2145</td>\n",
       "      <td>3.5137</td>\n",
       "      <td>5.6777</td>\n",
       "      <td>13.2177</td>\n",
       "      <td>-7.9940</td>\n",
       "      <td>-2.9029</td>\n",
       "      <td>5.8463</td>\n",
       "      <td>6.1439</td>\n",
       "      <td>-11.1025</td>\n",
       "      <td>12.4858</td>\n",
       "      <td>-2.2871</td>\n",
       "      <td>19.0422</td>\n",
       "      <td>11.0449</td>\n",
       "      <td>4.1087</td>\n",
       "      <td>4.6974</td>\n",
       "      <td>6.9346</td>\n",
       "      <td>10.8917</td>\n",
       "      <td>0.9003</td>\n",
       "      <td>-13.5174</td>\n",
       "      <td>2.2439</td>\n",
       "      <td>11.5283</td>\n",
       "      <td>12.0406</td>\n",
       "      <td>4.1006</td>\n",
       "      <td>-7.9078</td>\n",
       "      <td>11.1405</td>\n",
       "      <td>-5.7864</td>\n",
       "      <td>20.7477</td>\n",
       "      <td>6.8874</td>\n",
       "      <td>12.9143</td>\n",
       "      <td>19.5856</td>\n",
       "      <td>0.7268</td>\n",
       "      <td>6.4059</td>\n",
       "      <td>9.3124</td>\n",
       "      <td>6.2846</td>\n",
       "      <td>15.6372</td>\n",
       "      <td>5.8200</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>9.1854</td>\n",
       "      <td>12.5963</td>\n",
       "      <td>-10.3734</td>\n",
       "      <td>0.8748</td>\n",
       "      <td>5.8042</td>\n",
       "      <td>3.7163</td>\n",
       "      <td>-1.1016</td>\n",
       "      <td>7.3667</td>\n",
       "      <td>9.8565</td>\n",
       "      <td>5.0228</td>\n",
       "      <td>-5.7828</td>\n",
       "      <td>2.3612</td>\n",
       "      <td>0.8520</td>\n",
       "      <td>6.3577</td>\n",
       "      <td>12.1719</td>\n",
       "      <td>19.7312</td>\n",
       "      <td>19.4465</td>\n",
       "      <td>4.5048</td>\n",
       "      <td>23.2378</td>\n",
       "      <td>6.3191</td>\n",
       "      <td>12.8046</td>\n",
       "      <td>7.4729</td>\n",
       "      <td>15.7811</td>\n",
       "      <td>13.3529</td>\n",
       "      <td>10.1852</td>\n",
       "      <td>5.4604</td>\n",
       "      <td>19.0773</td>\n",
       "      <td>-4.4577</td>\n",
       "      <td>9.5413</td>\n",
       "      <td>11.9052</td>\n",
       "      <td>2.1447</td>\n",
       "      <td>-22.4038</td>\n",
       "      <td>7.0883</td>\n",
       "      <td>14.1613</td>\n",
       "      <td>10.5080</td>\n",
       "      <td>14.2621</td>\n",
       "      <td>0.2647</td>\n",
       "      <td>20.4031</td>\n",
       "      <td>17.0360</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.3939</td>\n",
       "      <td>12.6317</td>\n",
       "      <td>14.8863</td>\n",
       "      <td>1.3854</td>\n",
       "      <td>15.0284</td>\n",
       "      <td>3.9995</td>\n",
       "      <td>5.3683</td>\n",
       "      <td>8.6273</td>\n",
       "      <td>14.1963</td>\n",
       "      <td>20.3882</td>\n",
       "      <td>3.2304</td>\n",
       "      <td>5.7033</td>\n",
       "      <td>4.5255</td>\n",
       "      <td>2.1929</td>\n",
       "      <td>3.1290</td>\n",
       "      <td>2.9044</td>\n",
       "      <td>1.1696</td>\n",
       "      <td>28.7632</td>\n",
       "      <td>-17.2738</td>\n",
       "      <td>2.1056</td>\n",
       "      <td>21.1613</td>\n",
       "      <td>8.9573</td>\n",
       "      <td>2.7768</td>\n",
       "      <td>-2.1746</td>\n",
       "      <td>3.6932</td>\n",
       "      <td>12.4653</td>\n",
       "      <td>14.1978</td>\n",
       "      <td>-2.5511</td>\n",
       "      <td>-0.9479</td>\n",
       "      <td>17.1092</td>\n",
       "      <td>11.5419</td>\n",
       "      <td>0.0975</td>\n",
       "      <td>8.8186</td>\n",
       "      <td>6.6231</td>\n",
       "      <td>3.9358</td>\n",
       "      <td>-11.7218</td>\n",
       "      <td>24.5437</td>\n",
       "      <td>15.5827</td>\n",
       "      <td>3.8212</td>\n",
       "      <td>8.6674</td>\n",
       "      <td>7.3834</td>\n",
       "      <td>-2.4438</td>\n",
       "      <td>10.2158</td>\n",
       "      <td>7.4844</td>\n",
       "      <td>9.1104</td>\n",
       "      <td>4.3649</td>\n",
       "      <td>11.4934</td>\n",
       "      <td>1.7624</td>\n",
       "      <td>4.0714</td>\n",
       "      <td>-1.2681</td>\n",
       "      <td>14.3330</td>\n",
       "      <td>8.0088</td>\n",
       "      <td>4.4015</td>\n",
       "      <td>14.1479</td>\n",
       "      <td>-5.1747</td>\n",
       "      <td>0.5778</td>\n",
       "      <td>14.5362</td>\n",
       "      <td>-1.7624</td>\n",
       "      <td>33.8820</td>\n",
       "      <td>11.6041</td>\n",
       "      <td>13.2070</td>\n",
       "      <td>5.8442</td>\n",
       "      <td>4.7086</td>\n",
       "      <td>5.7141</td>\n",
       "      <td>-1.0410</td>\n",
       "      <td>20.5092</td>\n",
       "      <td>3.2790</td>\n",
       "      <td>-5.5952</td>\n",
       "      <td>7.3176</td>\n",
       "      <td>5.7690</td>\n",
       "      <td>-7.0927</td>\n",
       "      <td>-3.9116</td>\n",
       "      <td>7.2569</td>\n",
       "      <td>-5.8234</td>\n",
       "      <td>25.6820</td>\n",
       "      <td>10.9202</td>\n",
       "      <td>-0.3104</td>\n",
       "      <td>8.8438</td>\n",
       "      <td>-9.7009</td>\n",
       "      <td>2.4013</td>\n",
       "      <td>-4.2935</td>\n",
       "      <td>9.3908</td>\n",
       "      <td>-13.2648</td>\n",
       "      <td>3.1545</td>\n",
       "      <td>23.0866</td>\n",
       "      <td>-5.3000</td>\n",
       "      <td>5.3745</td>\n",
       "      <td>-6.2660</td>\n",
       "      <td>10.1934</td>\n",
       "      <td>-0.8417</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>1.6704</td>\n",
       "      <td>1.6858</td>\n",
       "      <td>21.6042</td>\n",
       "      <td>3.1417</td>\n",
       "      <td>-6.5213</td>\n",
       "      <td>8.2675</td>\n",
       "      <td>14.7222</td>\n",
       "      <td>0.3965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>-5.8609</td>\n",
       "      <td>8.2450</td>\n",
       "      <td>2.3061</td>\n",
       "      <td>2.8102</td>\n",
       "      <td>13.8463</td>\n",
       "      <td>11.9704</td>\n",
       "      <td>6.4569</td>\n",
       "      <td>14.8372</td>\n",
       "      <td>10.7430</td>\n",
       "      <td>-0.4299</td>\n",
       "      <td>15.9426</td>\n",
       "      <td>13.7257</td>\n",
       "      <td>20.3010</td>\n",
       "      <td>12.5579</td>\n",
       "      <td>6.8202</td>\n",
       "      <td>2.7229</td>\n",
       "      <td>12.1354</td>\n",
       "      <td>13.7367</td>\n",
       "      <td>0.8135</td>\n",
       "      <td>-0.9059</td>\n",
       "      <td>5.9070</td>\n",
       "      <td>2.8407</td>\n",
       "      <td>-15.2398</td>\n",
       "      <td>10.4407</td>\n",
       "      <td>-2.5731</td>\n",
       "      <td>6.1796</td>\n",
       "      <td>10.6093</td>\n",
       "      <td>-5.9158</td>\n",
       "      <td>8.1723</td>\n",
       "      <td>2.8521</td>\n",
       "      <td>9.1738</td>\n",
       "      <td>0.6665</td>\n",
       "      <td>-3.8294</td>\n",
       "      <td>-1.0370</td>\n",
       "      <td>11.7770</td>\n",
       "      <td>11.2834</td>\n",
       "      <td>8.0485</td>\n",
       "      <td>-24.6840</td>\n",
       "      <td>12.7404</td>\n",
       "      <td>-35.1659</td>\n",
       "      <td>0.7613</td>\n",
       "      <td>8.3838</td>\n",
       "      <td>12.6832</td>\n",
       "      <td>9.5503</td>\n",
       "      <td>1.7895</td>\n",
       "      <td>5.2091</td>\n",
       "      <td>8.0913</td>\n",
       "      <td>12.3972</td>\n",
       "      <td>14.4698</td>\n",
       "      <td>6.5850</td>\n",
       "      <td>3.3164</td>\n",
       "      <td>9.4638</td>\n",
       "      <td>15.7820</td>\n",
       "      <td>-25.0222</td>\n",
       "      <td>3.4418</td>\n",
       "      <td>-4.3923</td>\n",
       "      <td>8.6464</td>\n",
       "      <td>6.3072</td>\n",
       "      <td>5.6221</td>\n",
       "      <td>23.6143</td>\n",
       "      <td>5.0220</td>\n",
       "      <td>-3.9989</td>\n",
       "      <td>4.0462</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>1.2516</td>\n",
       "      <td>24.4187</td>\n",
       "      <td>4.5290</td>\n",
       "      <td>15.4235</td>\n",
       "      <td>11.6875</td>\n",
       "      <td>23.6273</td>\n",
       "      <td>4.0806</td>\n",
       "      <td>15.2733</td>\n",
       "      <td>0.7839</td>\n",
       "      <td>10.5404</td>\n",
       "      <td>1.6212</td>\n",
       "      <td>-5.2896</td>\n",
       "      <td>1.6027</td>\n",
       "      <td>17.9762</td>\n",
       "      <td>-2.3174</td>\n",
       "      <td>15.6298</td>\n",
       "      <td>4.5474</td>\n",
       "      <td>7.5509</td>\n",
       "      <td>-7.5866</td>\n",
       "      <td>7.0364</td>\n",
       "      <td>14.4027</td>\n",
       "      <td>10.7795</td>\n",
       "      <td>7.2887</td>\n",
       "      <td>-1.0930</td>\n",
       "      <td>11.3596</td>\n",
       "      <td>18.1486</td>\n",
       "      <td>...</td>\n",
       "      <td>-19.8592</td>\n",
       "      <td>22.5316</td>\n",
       "      <td>18.6129</td>\n",
       "      <td>1.3512</td>\n",
       "      <td>9.3291</td>\n",
       "      <td>4.2835</td>\n",
       "      <td>10.3907</td>\n",
       "      <td>7.0874</td>\n",
       "      <td>14.3256</td>\n",
       "      <td>14.4135</td>\n",
       "      <td>4.2827</td>\n",
       "      <td>6.9750</td>\n",
       "      <td>1.6480</td>\n",
       "      <td>11.6896</td>\n",
       "      <td>2.5762</td>\n",
       "      <td>-2.5459</td>\n",
       "      <td>5.3446</td>\n",
       "      <td>38.1015</td>\n",
       "      <td>3.5732</td>\n",
       "      <td>5.0988</td>\n",
       "      <td>30.5644</td>\n",
       "      <td>11.3025</td>\n",
       "      <td>3.9618</td>\n",
       "      <td>-8.2464</td>\n",
       "      <td>2.7038</td>\n",
       "      <td>12.3441</td>\n",
       "      <td>12.5431</td>\n",
       "      <td>-1.3683</td>\n",
       "      <td>3.5974</td>\n",
       "      <td>13.9761</td>\n",
       "      <td>14.3003</td>\n",
       "      <td>1.0486</td>\n",
       "      <td>8.9500</td>\n",
       "      <td>7.1954</td>\n",
       "      <td>-1.1984</td>\n",
       "      <td>1.9586</td>\n",
       "      <td>27.5609</td>\n",
       "      <td>24.6065</td>\n",
       "      <td>-2.8233</td>\n",
       "      <td>8.9821</td>\n",
       "      <td>3.8873</td>\n",
       "      <td>15.9638</td>\n",
       "      <td>10.0142</td>\n",
       "      <td>7.8388</td>\n",
       "      <td>9.9718</td>\n",
       "      <td>2.9253</td>\n",
       "      <td>10.4994</td>\n",
       "      <td>4.1622</td>\n",
       "      <td>3.7613</td>\n",
       "      <td>2.3701</td>\n",
       "      <td>18.0984</td>\n",
       "      <td>17.1765</td>\n",
       "      <td>7.6508</td>\n",
       "      <td>18.2452</td>\n",
       "      <td>17.0336</td>\n",
       "      <td>-10.9370</td>\n",
       "      <td>12.0500</td>\n",
       "      <td>-1.2155</td>\n",
       "      <td>19.9750</td>\n",
       "      <td>12.3892</td>\n",
       "      <td>31.8833</td>\n",
       "      <td>5.9684</td>\n",
       "      <td>7.2084</td>\n",
       "      <td>3.8899</td>\n",
       "      <td>-11.0882</td>\n",
       "      <td>17.2502</td>\n",
       "      <td>2.5881</td>\n",
       "      <td>-2.7018</td>\n",
       "      <td>0.5641</td>\n",
       "      <td>5.3430</td>\n",
       "      <td>-7.1541</td>\n",
       "      <td>-6.1920</td>\n",
       "      <td>18.2366</td>\n",
       "      <td>11.7134</td>\n",
       "      <td>14.7483</td>\n",
       "      <td>8.1013</td>\n",
       "      <td>11.8771</td>\n",
       "      <td>13.9552</td>\n",
       "      <td>-10.4701</td>\n",
       "      <td>5.6961</td>\n",
       "      <td>-3.7546</td>\n",
       "      <td>8.4117</td>\n",
       "      <td>1.8986</td>\n",
       "      <td>7.2601</td>\n",
       "      <td>-0.4639</td>\n",
       "      <td>-0.0498</td>\n",
       "      <td>7.9336</td>\n",
       "      <td>-12.8279</td>\n",
       "      <td>12.4124</td>\n",
       "      <td>1.8489</td>\n",
       "      <td>4.4666</td>\n",
       "      <td>4.7433</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.4214</td>\n",
       "      <td>23.0347</td>\n",
       "      <td>-1.2706</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>10.2922</td>\n",
       "      <td>17.9697</td>\n",
       "      <td>-8.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>6.2654</td>\n",
       "      <td>7.6784</td>\n",
       "      <td>-9.4458</td>\n",
       "      <td>-12.1419</td>\n",
       "      <td>13.8481</td>\n",
       "      <td>7.8895</td>\n",
       "      <td>7.7894</td>\n",
       "      <td>15.0553</td>\n",
       "      <td>8.4871</td>\n",
       "      <td>-3.0680</td>\n",
       "      <td>6.5263</td>\n",
       "      <td>11.3152</td>\n",
       "      <td>21.4246</td>\n",
       "      <td>18.9608</td>\n",
       "      <td>10.1102</td>\n",
       "      <td>2.7142</td>\n",
       "      <td>14.2080</td>\n",
       "      <td>13.5433</td>\n",
       "      <td>3.1736</td>\n",
       "      <td>-3.3423</td>\n",
       "      <td>5.9015</td>\n",
       "      <td>7.9352</td>\n",
       "      <td>-3.1582</td>\n",
       "      <td>9.4668</td>\n",
       "      <td>-0.0083</td>\n",
       "      <td>19.3239</td>\n",
       "      <td>12.4057</td>\n",
       "      <td>0.6329</td>\n",
       "      <td>2.7922</td>\n",
       "      <td>5.8184</td>\n",
       "      <td>19.3038</td>\n",
       "      <td>1.4450</td>\n",
       "      <td>-5.5963</td>\n",
       "      <td>14.0685</td>\n",
       "      <td>11.9171</td>\n",
       "      <td>11.5111</td>\n",
       "      <td>6.9087</td>\n",
       "      <td>-65.4863</td>\n",
       "      <td>13.8657</td>\n",
       "      <td>0.0444</td>\n",
       "      <td>-0.1346</td>\n",
       "      <td>14.4268</td>\n",
       "      <td>13.3273</td>\n",
       "      <td>10.4857</td>\n",
       "      <td>-1.4367</td>\n",
       "      <td>5.7555</td>\n",
       "      <td>-8.5414</td>\n",
       "      <td>14.1482</td>\n",
       "      <td>16.9840</td>\n",
       "      <td>6.1812</td>\n",
       "      <td>1.9548</td>\n",
       "      <td>9.2048</td>\n",
       "      <td>8.6591</td>\n",
       "      <td>-27.7439</td>\n",
       "      <td>-0.4952</td>\n",
       "      <td>-1.7839</td>\n",
       "      <td>5.2670</td>\n",
       "      <td>-4.3205</td>\n",
       "      <td>6.9860</td>\n",
       "      <td>1.6184</td>\n",
       "      <td>5.0301</td>\n",
       "      <td>-3.2431</td>\n",
       "      <td>40.1236</td>\n",
       "      <td>0.7737</td>\n",
       "      <td>-0.7264</td>\n",
       "      <td>4.5886</td>\n",
       "      <td>-4.5346</td>\n",
       "      <td>23.3521</td>\n",
       "      <td>1.0273</td>\n",
       "      <td>19.1600</td>\n",
       "      <td>7.1734</td>\n",
       "      <td>14.3937</td>\n",
       "      <td>2.9598</td>\n",
       "      <td>13.3317</td>\n",
       "      <td>-9.2587</td>\n",
       "      <td>-6.7075</td>\n",
       "      <td>7.8984</td>\n",
       "      <td>14.5265</td>\n",
       "      <td>7.0799</td>\n",
       "      <td>20.1670</td>\n",
       "      <td>8.0053</td>\n",
       "      <td>3.7954</td>\n",
       "      <td>-39.7997</td>\n",
       "      <td>7.0065</td>\n",
       "      <td>9.3627</td>\n",
       "      <td>10.4316</td>\n",
       "      <td>14.0553</td>\n",
       "      <td>0.0213</td>\n",
       "      <td>14.7246</td>\n",
       "      <td>35.2988</td>\n",
       "      <td>...</td>\n",
       "      <td>-22.9264</td>\n",
       "      <td>12.3562</td>\n",
       "      <td>17.3410</td>\n",
       "      <td>1.6940</td>\n",
       "      <td>7.1179</td>\n",
       "      <td>5.1934</td>\n",
       "      <td>8.8230</td>\n",
       "      <td>10.6617</td>\n",
       "      <td>14.0837</td>\n",
       "      <td>28.2749</td>\n",
       "      <td>-0.1937</td>\n",
       "      <td>5.9654</td>\n",
       "      <td>1.0719</td>\n",
       "      <td>7.9923</td>\n",
       "      <td>2.9138</td>\n",
       "      <td>-3.6135</td>\n",
       "      <td>1.4684</td>\n",
       "      <td>25.6795</td>\n",
       "      <td>13.8224</td>\n",
       "      <td>4.7478</td>\n",
       "      <td>41.1037</td>\n",
       "      <td>12.7140</td>\n",
       "      <td>5.2964</td>\n",
       "      <td>9.7289</td>\n",
       "      <td>3.9370</td>\n",
       "      <td>12.1316</td>\n",
       "      <td>12.5815</td>\n",
       "      <td>7.0642</td>\n",
       "      <td>5.6518</td>\n",
       "      <td>10.9346</td>\n",
       "      <td>11.4266</td>\n",
       "      <td>0.9442</td>\n",
       "      <td>7.7532</td>\n",
       "      <td>6.6173</td>\n",
       "      <td>-6.8304</td>\n",
       "      <td>6.4730</td>\n",
       "      <td>17.1728</td>\n",
       "      <td>25.8128</td>\n",
       "      <td>2.6791</td>\n",
       "      <td>13.9547</td>\n",
       "      <td>6.6289</td>\n",
       "      <td>-4.3965</td>\n",
       "      <td>11.7159</td>\n",
       "      <td>16.1080</td>\n",
       "      <td>7.6874</td>\n",
       "      <td>9.1570</td>\n",
       "      <td>11.5670</td>\n",
       "      <td>-12.7047</td>\n",
       "      <td>3.7574</td>\n",
       "      <td>9.9110</td>\n",
       "      <td>20.1461</td>\n",
       "      <td>1.2995</td>\n",
       "      <td>5.8493</td>\n",
       "      <td>19.8234</td>\n",
       "      <td>4.7022</td>\n",
       "      <td>10.6101</td>\n",
       "      <td>13.0021</td>\n",
       "      <td>-12.6068</td>\n",
       "      <td>27.0846</td>\n",
       "      <td>8.0913</td>\n",
       "      <td>33.5107</td>\n",
       "      <td>5.6953</td>\n",
       "      <td>5.4663</td>\n",
       "      <td>18.2201</td>\n",
       "      <td>6.5769</td>\n",
       "      <td>21.2607</td>\n",
       "      <td>3.2304</td>\n",
       "      <td>-1.7759</td>\n",
       "      <td>3.1283</td>\n",
       "      <td>5.5518</td>\n",
       "      <td>1.4493</td>\n",
       "      <td>-2.6627</td>\n",
       "      <td>19.8056</td>\n",
       "      <td>2.3705</td>\n",
       "      <td>18.4685</td>\n",
       "      <td>16.3309</td>\n",
       "      <td>-3.3456</td>\n",
       "      <td>13.5261</td>\n",
       "      <td>1.7189</td>\n",
       "      <td>5.1743</td>\n",
       "      <td>-7.6938</td>\n",
       "      <td>9.7685</td>\n",
       "      <td>4.8910</td>\n",
       "      <td>12.2198</td>\n",
       "      <td>11.8503</td>\n",
       "      <td>-7.8931</td>\n",
       "      <td>6.4209</td>\n",
       "      <td>5.9270</td>\n",
       "      <td>16.0201</td>\n",
       "      <td>-0.2829</td>\n",
       "      <td>-1.4905</td>\n",
       "      <td>9.5214</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>9.1942</td>\n",
       "      <td>13.2876</td>\n",
       "      <td>-1.5121</td>\n",
       "      <td>3.9267</td>\n",
       "      <td>9.5031</td>\n",
       "      <td>17.9974</td>\n",
       "      <td>-8.8104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_code  target    var_0   var_1    var_2   var_3    var_4   var_5   var_6  \\\n",
       "0  train_0       0   8.9255 -6.7863  11.9081  5.0930  11.4607 -9.2834  5.1187   \n",
       "1  train_1       0  11.5006 -4.1473  13.8588  5.3890  12.3622  7.0433  5.6208   \n",
       "2  train_2       0   8.6093 -2.7457  12.0805  7.8928  10.5825 -9.0837  6.9427   \n",
       "3  train_3       0  11.0604 -2.1518   8.9522  7.1957  12.5846 -1.8361  5.8428   \n",
       "4  train_4       0   9.8369 -1.4834  12.8746  6.6375  12.2772  2.4486  5.9405   \n",
       "\n",
       "     var_7   var_8   var_9  var_10   var_11   var_12   var_13  var_14  \\\n",
       "0  18.6266 -4.9200  5.7470  2.9252   3.1821  14.0137   0.5745  8.7989   \n",
       "1  16.5338  3.1468  8.0851 -0.4032   8.0585  14.0239   8.4135  5.4345   \n",
       "2  14.6155 -4.9193  5.9525 -0.3249 -11.2648  14.1929   7.3124  7.5244   \n",
       "3  14.9250 -5.8609  8.2450  2.3061   2.8102  13.8463  11.9704  6.4569   \n",
       "4  19.2514  6.2654  7.6784 -9.4458 -12.1419  13.8481   7.8895  7.7894   \n",
       "\n",
       "    var_15   var_16   var_17   var_18   var_19   var_20   var_21   var_22  \\\n",
       "0  14.5691   5.7487  -7.2393   4.2840  30.7133  10.5350  16.2191   2.5791   \n",
       "1  13.7003  13.8275 -15.5849   7.8000  28.5708   3.4287   2.7407   8.5524   \n",
       "2  14.6472   7.6782  -1.7395   4.7011  20.4775  17.7559  18.1377   1.2145   \n",
       "3  14.8372  10.7430  -0.4299  15.9426  13.7257  20.3010  12.5579   6.8202   \n",
       "4  15.0553   8.4871  -3.0680   6.5263  11.3152  21.4246  18.9608  10.1102   \n",
       "\n",
       "   var_23   var_24   var_25   var_26  var_27  var_28  var_29   var_30  \\\n",
       "0  2.4716  14.3831  13.4325  -5.1488 -0.4073  4.9306  5.9965  -0.3085   \n",
       "1  3.3716   6.9779  13.8910 -11.7684 -2.5586  5.0464  0.5481  -9.2987   \n",
       "2  3.5137   5.6777  13.2177  -7.9940 -2.9029  5.8463  6.1439 -11.1025   \n",
       "3  2.7229  12.1354  13.7367   0.8135 -0.9059  5.9070  2.8407 -15.2398   \n",
       "4  2.7142  14.2080  13.5433   3.1736 -3.3423  5.9015  7.9352  -3.1582   \n",
       "\n",
       "    var_31  var_32   var_33   var_34   var_35  var_36  var_37   var_38  \\\n",
       "0  12.9041 -3.8766  16.8911  11.1920  10.5785  0.6764  7.8871   4.6667   \n",
       "1   7.8755  1.2859  19.3710  11.3702   0.7399  2.7995  5.8434  10.8160   \n",
       "2  12.4858 -2.2871  19.0422  11.0449   4.1087  4.6974  6.9346  10.8917   \n",
       "3  10.4407 -2.5731   6.1796  10.6093  -5.9158  8.1723  2.8521   9.1738   \n",
       "4   9.4668 -0.0083  19.3239  12.4057   0.6329  2.7922  5.8184  19.3038   \n",
       "\n",
       "   var_39   var_40   var_41   var_42   var_43   var_44   var_45   var_46  \\\n",
       "0  3.8743  -5.2387   7.3746  11.5767  12.0446  11.6418  -7.0170   5.9226   \n",
       "1  3.6783 -11.1147   1.8730   9.8775  11.7842   1.2444 -47.3797   7.3718   \n",
       "2  0.9003 -13.5174   2.2439  11.5283  12.0406   4.1006  -7.9078  11.1405   \n",
       "3  0.6665  -3.8294  -1.0370  11.7770  11.2834   8.0485 -24.6840  12.7404   \n",
       "4  1.4450  -5.5963  14.0685  11.9171  11.5111   6.9087 -65.4863  13.8657   \n",
       "\n",
       "    var_47   var_48   var_49   var_50   var_51  var_52  var_53  var_54  \\\n",
       "0 -14.2136  16.0283   5.3253  12.9194  29.0460 -0.6940  5.1736 -0.7474   \n",
       "1   0.1948  34.4014  25.7037  11.8343  13.2256 -4.1083  6.6885 -8.0946   \n",
       "2  -5.7864  20.7477   6.8874  12.9143  19.5856  0.7268  6.4059  9.3124   \n",
       "3 -35.1659   0.7613   8.3838  12.6832   9.5503  1.7895  5.2091  8.0913   \n",
       "4   0.0444  -0.1346  14.4268  13.3273  10.4857 -1.4367  5.7555 -8.5414   \n",
       "\n",
       "    var_55   var_56  var_57  var_58   var_59   var_60   var_61  var_62  \\\n",
       "0  14.8322  11.2668  5.3822  2.0183  10.1166  16.1828   4.9590  2.0771   \n",
       "1  18.5995  19.3219  7.0118  1.9210   8.8682   8.0109  -7.2417  1.7944   \n",
       "2   6.2846  15.6372  5.8200  1.1000   9.1854  12.5963 -10.3734  0.8748   \n",
       "3  12.3972  14.4698  6.5850  3.3164   9.4638  15.7820 -25.0222  3.4418   \n",
       "4  14.1482  16.9840  6.1812  1.9548   9.2048   8.6591 -27.7439 -0.4952   \n",
       "\n",
       "   var_63  var_64  var_65  var_66   var_67  var_68  var_69   var_70  var_71  \\\n",
       "0 -0.2154  8.6748  9.5319  5.8056  22.4321  5.0109 -4.7010  21.6374  0.5663   \n",
       "1 -1.3147  8.1042  1.5365  5.4007   7.9344  5.0220  2.2302  40.5632  0.5134   \n",
       "2  5.8042  3.7163 -1.1016  7.3667   9.8565  5.0228 -5.7828   2.3612  0.8520   \n",
       "3 -4.3923  8.6464  6.3072  5.6221  23.6143  5.0220 -3.9989   4.0462  0.2500   \n",
       "4 -1.7839  5.2670 -4.3205  6.9860   1.6184  5.0301 -3.2431  40.1236  0.7737   \n",
       "\n",
       "   var_72   var_73   var_74   var_75   var_76   var_77  var_78   var_79  \\\n",
       "0  5.1999   8.8600  43.1127  18.3816  -2.3440  23.4104  6.5199  12.1983   \n",
       "1  3.1701  20.1068   7.7841   7.0529   3.2709  23.4822  5.5075  13.7814   \n",
       "2  6.3577  12.1719  19.7312  19.4465   4.5048  23.2378  6.3191  12.8046   \n",
       "3  1.2516  24.4187   4.5290  15.4235  11.6875  23.6273  4.0806  15.2733   \n",
       "4 -0.7264   4.5886  -4.5346  23.3521   1.0273  19.1600  7.1734  14.3937   \n",
       "\n",
       "    var_80   var_81   var_82   var_83  var_84   var_85   var_86   var_87  \\\n",
       "0  13.6468  13.8372   1.3675   2.9423 -4.5213  21.4669   9.3225  16.4597   \n",
       "1   2.5462  18.1782   0.3683  -4.8210 -5.4850  13.7867 -13.5901  11.0993   \n",
       "2   7.4729  15.7811  13.3529  10.1852  5.4604  19.0773  -4.4577   9.5413   \n",
       "3   0.7839  10.5404   1.6212  -5.2896  1.6027  17.9762  -2.3174  15.6298   \n",
       "4   2.9598  13.3317  -9.2587  -6.7075  7.8984  14.5265   7.0799  20.1670   \n",
       "\n",
       "    var_88   var_89   var_90  var_91   var_92   var_93   var_94  var_95  \\\n",
       "0   7.9984  -1.7069 -21.4494  6.7806  11.0924   9.9913  14.8421  0.1812   \n",
       "1   7.9022  12.2301   0.4768  6.8852   8.0905  10.9631  11.7569 -1.2722   \n",
       "2  11.9052   2.1447 -22.4038  7.0883  14.1613  10.5080  14.2621  0.2647   \n",
       "3   4.5474   7.5509  -7.5866  7.0364  14.4027  10.7795   7.2887 -1.0930   \n",
       "4   8.0053   3.7954 -39.7997  7.0065   9.3627  10.4316  14.0553  0.0213   \n",
       "\n",
       "    var_96   var_97   ...     var_100  var_101  var_102  var_103  var_104  \\\n",
       "0   8.9642  16.2572   ...      9.4763  13.3102  26.5376   1.4403  14.7100   \n",
       "1  24.7876  26.6881   ...    -13.6950   8.4068  35.4734   1.7093  15.1866   \n",
       "2  20.4031  17.0360   ...     -0.3939  12.6317  14.8863   1.3854  15.0284   \n",
       "3  11.3596  18.1486   ...    -19.8592  22.5316  18.6129   1.3512   9.3291   \n",
       "4  14.7246  35.2988   ...    -22.9264  12.3562  17.3410   1.6940   7.1179   \n",
       "\n",
       "   var_105  var_106  var_107  var_108  var_109  var_110  var_111  var_112  \\\n",
       "0   6.0454   9.5426  17.1554  14.1104  24.3627   2.0323   6.7602   3.9141   \n",
       "1   2.6227   7.3412  32.0888  13.9550  13.0858   6.6203   7.1051   5.3523   \n",
       "2   3.9995   5.3683   8.6273  14.1963  20.3882   3.2304   5.7033   4.5255   \n",
       "3   4.2835  10.3907   7.0874  14.3256  14.4135   4.2827   6.9750   1.6480   \n",
       "4   5.1934   8.8230  10.6617  14.0837  28.2749  -0.1937   5.9654   1.0719   \n",
       "\n",
       "   var_113  var_114  var_115  var_116  var_117  var_118  var_119  var_120  \\\n",
       "0  -0.4851   2.5240   1.5093   2.5516  15.5752 -13.4221   7.2739  16.0094   \n",
       "1   8.5426   3.6159   4.1569   3.0454   7.8522 -11.5100   7.5109  31.5899   \n",
       "2   2.1929   3.1290   2.9044   1.1696  28.7632 -17.2738   2.1056  21.1613   \n",
       "3  11.6896   2.5762  -2.5459   5.3446  38.1015   3.5732   5.0988  30.5644   \n",
       "4   7.9923   2.9138  -3.6135   1.4684  25.6795  13.8224   4.7478  41.1037   \n",
       "\n",
       "   var_121  var_122  var_123  var_124  var_125  var_126  var_127  var_128  \\\n",
       "0   9.7268   0.8897   0.7754   4.2218  12.0039  13.8571  -0.7338  -1.9245   \n",
       "1   9.5018   8.2736  10.1633   0.1225  12.5942  14.5697   2.4354   0.8194   \n",
       "2   8.9573   2.7768  -2.1746   3.6932  12.4653  14.1978  -2.5511  -0.9479   \n",
       "3  11.3025   3.9618  -8.2464   2.7038  12.3441  12.5431  -1.3683   3.5974   \n",
       "4  12.7140   5.2964   9.7289   3.9370  12.1316  12.5815   7.0642   5.6518   \n",
       "\n",
       "   var_129  var_130  var_131  var_132  var_133  var_134  var_135  var_136  \\\n",
       "0  15.4462  12.8287   0.3587   9.6508   6.5674   5.1726   3.1345  29.4547   \n",
       "1  16.5346  12.4205  -0.1780   5.7582   7.0513   1.9568  -8.9921   9.7797   \n",
       "2  17.1092  11.5419   0.0975   8.8186   6.6231   3.9358 -11.7218  24.5437   \n",
       "3  13.9761  14.3003   1.0486   8.9500   7.1954  -1.1984   1.9586  27.5609   \n",
       "4  10.9346  11.4266   0.9442   7.7532   6.6173  -6.8304   6.4730  17.1728   \n",
       "\n",
       "   var_137  var_138  var_139  var_140  var_141  var_142  var_143  var_144  \\\n",
       "0  31.4045   2.8279  15.6599   8.3307  -5.6011  19.0614  11.2663   8.6989   \n",
       "1  18.1577  -1.9721  16.1622   3.6937   6.6803  -0.3243  12.2806   8.6086   \n",
       "2  15.5827   3.8212   8.6674   7.3834  -2.4438  10.2158   7.4844   9.1104   \n",
       "3  24.6065  -2.8233   8.9821   3.8873  15.9638  10.0142   7.8388   9.9718   \n",
       "4  25.8128   2.6791  13.9547   6.6289  -4.3965  11.7159  16.1080   7.6874   \n",
       "\n",
       "   var_145  var_146  var_147  var_148  var_149  var_150  var_151  var_152  \\\n",
       "0   8.3694  11.5659 -16.4727   4.0288  17.9244  18.5177  10.7800   9.0056   \n",
       "1  11.0738   8.9231  11.7700   4.2578  -4.4223  20.6294  14.8743   9.4317   \n",
       "2   4.3649  11.4934   1.7624   4.0714  -1.2681  14.3330   8.0088   4.4015   \n",
       "3   2.9253  10.4994   4.1622   3.7613   2.3701  18.0984  17.1765   7.6508   \n",
       "4   9.1570  11.5670 -12.7047   3.7574   9.9110  20.1461   1.2995   5.8493   \n",
       "\n",
       "   var_153  var_154  var_155  var_156  var_157  var_158  var_159  var_160  \\\n",
       "0  16.6964  10.4838   1.6573  12.1749 -13.1324  17.6054  11.5423  15.4576   \n",
       "1  16.7242  -0.5687   0.1898  12.2419  -9.6953  22.3949  10.6261  29.4846   \n",
       "2  14.1479  -5.1747   0.5778  14.5362  -1.7624  33.8820  11.6041  13.2070   \n",
       "3  18.2452  17.0336 -10.9370  12.0500  -1.2155  19.9750  12.3892  31.8833   \n",
       "4  19.8234   4.7022  10.6101  13.0021 -12.6068  27.0846   8.0913  33.5107   \n",
       "\n",
       "   var_161  var_162  var_163  var_164  var_165  var_166  var_167  var_168  \\\n",
       "0   5.3133   3.6159   5.0384   6.6760  12.6644   2.7004  -0.6975   9.5981   \n",
       "1   5.8683   3.8208  15.8348  -5.0121  15.1345   3.2003   9.3192   3.8821   \n",
       "2   5.8442   4.7086   5.7141  -1.0410  20.5092   3.2790  -5.5952   7.3176   \n",
       "3   5.9684   7.2084   3.8899 -11.0882  17.2502   2.5881  -2.7018   0.5641   \n",
       "4   5.6953   5.4663  18.2201   6.5769  21.2607   3.2304  -1.7759   3.1283   \n",
       "\n",
       "   var_169  var_170  var_171  var_172  var_173  var_174  var_175  var_176  \\\n",
       "0   5.4879  -4.7645  -8.4254  20.8773   3.1531  18.5618   7.7423 -10.1245   \n",
       "1   5.7999   5.5378   5.0988  22.0330   5.5134  30.2645  10.4968  -7.2352   \n",
       "2   5.7690  -7.0927  -3.9116   7.2569  -5.8234  25.6820  10.9202  -0.3104   \n",
       "3   5.3430  -7.1541  -6.1920  18.2366  11.7134  14.7483   8.1013  11.8771   \n",
       "4   5.5518   1.4493  -2.6627  19.8056   2.3705  18.4685  16.3309  -3.3456   \n",
       "\n",
       "   var_177  var_178  var_179  var_180  var_181  var_182  var_183  var_184  \\\n",
       "0  13.7241  -3.5189   1.7202  -8.4051   9.0164   3.0657  14.3691  25.8398   \n",
       "1  16.5721  -7.3477  11.0752  -5.5937   9.4878 -14.9100   9.4245  22.5441   \n",
       "2   8.8438  -9.7009   2.4013  -4.2935   9.3908 -13.2648   3.1545  23.0866   \n",
       "3  13.9552 -10.4701   5.6961  -3.7546   8.4117   1.8986   7.2601  -0.4639   \n",
       "4  13.5261   1.7189   5.1743  -7.6938   9.7685   4.8910  12.2198  11.8503   \n",
       "\n",
       "   var_185  var_186  var_187  var_188  var_189  var_190  var_191  var_192  \\\n",
       "0   5.8764  11.8411 -19.7159  17.5743   0.5857   4.4354   3.9642   3.1364   \n",
       "1  -4.8622   7.6543 -15.9319  13.3175  -0.3566   7.6421   7.7214   2.5837   \n",
       "2  -5.3000   5.3745  -6.2660  10.1934  -0.8417   2.9057   9.7905   1.6704   \n",
       "3  -0.0498   7.9336 -12.8279  12.4124   1.8489   4.4666   4.7433   0.7178   \n",
       "4  -7.8931   6.4209   5.9270  16.0201  -0.2829  -1.4905   9.5214  -0.1508   \n",
       "\n",
       "   var_193  var_194  var_195  var_196  var_197  var_198  var_199  \n",
       "0   1.6910  18.5227  -2.3978   7.8784   8.5635  12.7803  -1.0914  \n",
       "1  10.9516  15.4305   2.0339   8.1267   8.7889  18.3560   1.9518  \n",
       "2   1.6858  21.6042   3.1417  -6.5213   8.2675  14.7222   0.3965  \n",
       "3   1.4214  23.0347  -1.2706  -2.9275  10.2922  17.9697  -8.9996  \n",
       "4   9.1942  13.2876  -1.5121   3.9267   9.5031  17.9974  -8.8104  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba5573cfaec6625aed13e98c6e034809e2997b5b"
   },
   "source": [
    "Test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "93aa6148650a23671d5f01a834f948c5e6721234"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>var_11</th>\n",
       "      <th>var_12</th>\n",
       "      <th>var_13</th>\n",
       "      <th>var_14</th>\n",
       "      <th>var_15</th>\n",
       "      <th>var_16</th>\n",
       "      <th>var_17</th>\n",
       "      <th>var_18</th>\n",
       "      <th>var_19</th>\n",
       "      <th>var_20</th>\n",
       "      <th>var_21</th>\n",
       "      <th>var_22</th>\n",
       "      <th>var_23</th>\n",
       "      <th>var_24</th>\n",
       "      <th>var_25</th>\n",
       "      <th>var_26</th>\n",
       "      <th>var_27</th>\n",
       "      <th>var_28</th>\n",
       "      <th>var_29</th>\n",
       "      <th>var_30</th>\n",
       "      <th>var_31</th>\n",
       "      <th>var_32</th>\n",
       "      <th>var_33</th>\n",
       "      <th>var_34</th>\n",
       "      <th>var_35</th>\n",
       "      <th>var_36</th>\n",
       "      <th>var_37</th>\n",
       "      <th>var_38</th>\n",
       "      <th>var_39</th>\n",
       "      <th>var_40</th>\n",
       "      <th>var_41</th>\n",
       "      <th>var_42</th>\n",
       "      <th>var_43</th>\n",
       "      <th>var_44</th>\n",
       "      <th>var_45</th>\n",
       "      <th>var_46</th>\n",
       "      <th>var_47</th>\n",
       "      <th>var_48</th>\n",
       "      <th>var_49</th>\n",
       "      <th>var_50</th>\n",
       "      <th>var_51</th>\n",
       "      <th>var_52</th>\n",
       "      <th>var_53</th>\n",
       "      <th>var_54</th>\n",
       "      <th>var_55</th>\n",
       "      <th>var_56</th>\n",
       "      <th>var_57</th>\n",
       "      <th>var_58</th>\n",
       "      <th>var_59</th>\n",
       "      <th>var_60</th>\n",
       "      <th>var_61</th>\n",
       "      <th>var_62</th>\n",
       "      <th>var_63</th>\n",
       "      <th>var_64</th>\n",
       "      <th>var_65</th>\n",
       "      <th>var_66</th>\n",
       "      <th>var_67</th>\n",
       "      <th>var_68</th>\n",
       "      <th>var_69</th>\n",
       "      <th>var_70</th>\n",
       "      <th>var_71</th>\n",
       "      <th>var_72</th>\n",
       "      <th>var_73</th>\n",
       "      <th>var_74</th>\n",
       "      <th>var_75</th>\n",
       "      <th>var_76</th>\n",
       "      <th>var_77</th>\n",
       "      <th>var_78</th>\n",
       "      <th>var_79</th>\n",
       "      <th>var_80</th>\n",
       "      <th>var_81</th>\n",
       "      <th>var_82</th>\n",
       "      <th>var_83</th>\n",
       "      <th>var_84</th>\n",
       "      <th>var_85</th>\n",
       "      <th>var_86</th>\n",
       "      <th>var_87</th>\n",
       "      <th>var_88</th>\n",
       "      <th>var_89</th>\n",
       "      <th>var_90</th>\n",
       "      <th>var_91</th>\n",
       "      <th>var_92</th>\n",
       "      <th>var_93</th>\n",
       "      <th>var_94</th>\n",
       "      <th>var_95</th>\n",
       "      <th>var_96</th>\n",
       "      <th>var_97</th>\n",
       "      <th>var_98</th>\n",
       "      <th>...</th>\n",
       "      <th>var_100</th>\n",
       "      <th>var_101</th>\n",
       "      <th>var_102</th>\n",
       "      <th>var_103</th>\n",
       "      <th>var_104</th>\n",
       "      <th>var_105</th>\n",
       "      <th>var_106</th>\n",
       "      <th>var_107</th>\n",
       "      <th>var_108</th>\n",
       "      <th>var_109</th>\n",
       "      <th>var_110</th>\n",
       "      <th>var_111</th>\n",
       "      <th>var_112</th>\n",
       "      <th>var_113</th>\n",
       "      <th>var_114</th>\n",
       "      <th>var_115</th>\n",
       "      <th>var_116</th>\n",
       "      <th>var_117</th>\n",
       "      <th>var_118</th>\n",
       "      <th>var_119</th>\n",
       "      <th>var_120</th>\n",
       "      <th>var_121</th>\n",
       "      <th>var_122</th>\n",
       "      <th>var_123</th>\n",
       "      <th>var_124</th>\n",
       "      <th>var_125</th>\n",
       "      <th>var_126</th>\n",
       "      <th>var_127</th>\n",
       "      <th>var_128</th>\n",
       "      <th>var_129</th>\n",
       "      <th>var_130</th>\n",
       "      <th>var_131</th>\n",
       "      <th>var_132</th>\n",
       "      <th>var_133</th>\n",
       "      <th>var_134</th>\n",
       "      <th>var_135</th>\n",
       "      <th>var_136</th>\n",
       "      <th>var_137</th>\n",
       "      <th>var_138</th>\n",
       "      <th>var_139</th>\n",
       "      <th>var_140</th>\n",
       "      <th>var_141</th>\n",
       "      <th>var_142</th>\n",
       "      <th>var_143</th>\n",
       "      <th>var_144</th>\n",
       "      <th>var_145</th>\n",
       "      <th>var_146</th>\n",
       "      <th>var_147</th>\n",
       "      <th>var_148</th>\n",
       "      <th>var_149</th>\n",
       "      <th>var_150</th>\n",
       "      <th>var_151</th>\n",
       "      <th>var_152</th>\n",
       "      <th>var_153</th>\n",
       "      <th>var_154</th>\n",
       "      <th>var_155</th>\n",
       "      <th>var_156</th>\n",
       "      <th>var_157</th>\n",
       "      <th>var_158</th>\n",
       "      <th>var_159</th>\n",
       "      <th>var_160</th>\n",
       "      <th>var_161</th>\n",
       "      <th>var_162</th>\n",
       "      <th>var_163</th>\n",
       "      <th>var_164</th>\n",
       "      <th>var_165</th>\n",
       "      <th>var_166</th>\n",
       "      <th>var_167</th>\n",
       "      <th>var_168</th>\n",
       "      <th>var_169</th>\n",
       "      <th>var_170</th>\n",
       "      <th>var_171</th>\n",
       "      <th>var_172</th>\n",
       "      <th>var_173</th>\n",
       "      <th>var_174</th>\n",
       "      <th>var_175</th>\n",
       "      <th>var_176</th>\n",
       "      <th>var_177</th>\n",
       "      <th>var_178</th>\n",
       "      <th>var_179</th>\n",
       "      <th>var_180</th>\n",
       "      <th>var_181</th>\n",
       "      <th>var_182</th>\n",
       "      <th>var_183</th>\n",
       "      <th>var_184</th>\n",
       "      <th>var_185</th>\n",
       "      <th>var_186</th>\n",
       "      <th>var_187</th>\n",
       "      <th>var_188</th>\n",
       "      <th>var_189</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>11.0656</td>\n",
       "      <td>7.7798</td>\n",
       "      <td>12.9536</td>\n",
       "      <td>9.4292</td>\n",
       "      <td>11.4327</td>\n",
       "      <td>-2.3805</td>\n",
       "      <td>5.8493</td>\n",
       "      <td>18.2675</td>\n",
       "      <td>2.1337</td>\n",
       "      <td>8.8100</td>\n",
       "      <td>-2.0248</td>\n",
       "      <td>-4.3554</td>\n",
       "      <td>13.9696</td>\n",
       "      <td>0.3458</td>\n",
       "      <td>7.5408</td>\n",
       "      <td>14.5001</td>\n",
       "      <td>7.7028</td>\n",
       "      <td>-19.0919</td>\n",
       "      <td>15.5806</td>\n",
       "      <td>16.1763</td>\n",
       "      <td>3.7088</td>\n",
       "      <td>18.8064</td>\n",
       "      <td>1.5899</td>\n",
       "      <td>3.0654</td>\n",
       "      <td>6.4509</td>\n",
       "      <td>14.1192</td>\n",
       "      <td>-9.4902</td>\n",
       "      <td>-2.1917</td>\n",
       "      <td>5.7107</td>\n",
       "      <td>3.7864</td>\n",
       "      <td>-1.7981</td>\n",
       "      <td>9.2645</td>\n",
       "      <td>2.0657</td>\n",
       "      <td>12.7753</td>\n",
       "      <td>11.3334</td>\n",
       "      <td>8.1462</td>\n",
       "      <td>-0.0610</td>\n",
       "      <td>3.5331</td>\n",
       "      <td>9.7804</td>\n",
       "      <td>8.7625</td>\n",
       "      <td>-15.6305</td>\n",
       "      <td>18.8766</td>\n",
       "      <td>11.2864</td>\n",
       "      <td>11.8362</td>\n",
       "      <td>13.3680</td>\n",
       "      <td>-31.9891</td>\n",
       "      <td>12.1776</td>\n",
       "      <td>8.7714</td>\n",
       "      <td>17.2011</td>\n",
       "      <td>16.8508</td>\n",
       "      <td>13.0534</td>\n",
       "      <td>14.4069</td>\n",
       "      <td>-4.8525</td>\n",
       "      <td>7.3213</td>\n",
       "      <td>-0.5259</td>\n",
       "      <td>16.6365</td>\n",
       "      <td>19.3036</td>\n",
       "      <td>6.4129</td>\n",
       "      <td>-5.3948</td>\n",
       "      <td>9.3269</td>\n",
       "      <td>11.9314</td>\n",
       "      <td>-3.5750</td>\n",
       "      <td>-0.7706</td>\n",
       "      <td>0.8705</td>\n",
       "      <td>6.9282</td>\n",
       "      <td>2.8914</td>\n",
       "      <td>5.9744</td>\n",
       "      <td>17.4851</td>\n",
       "      <td>5.0125</td>\n",
       "      <td>-1.4230</td>\n",
       "      <td>33.3401</td>\n",
       "      <td>0.8018</td>\n",
       "      <td>-4.7906</td>\n",
       "      <td>30.2708</td>\n",
       "      <td>26.8339</td>\n",
       "      <td>21.7205</td>\n",
       "      <td>7.3075</td>\n",
       "      <td>14.0810</td>\n",
       "      <td>3.1192</td>\n",
       "      <td>17.4265</td>\n",
       "      <td>9.4883</td>\n",
       "      <td>16.9060</td>\n",
       "      <td>14.5117</td>\n",
       "      <td>10.0276</td>\n",
       "      <td>-0.9706</td>\n",
       "      <td>20.4588</td>\n",
       "      <td>4.7945</td>\n",
       "      <td>20.4160</td>\n",
       "      <td>13.1633</td>\n",
       "      <td>7.9307</td>\n",
       "      <td>-7.6509</td>\n",
       "      <td>7.0834</td>\n",
       "      <td>15.2324</td>\n",
       "      <td>10.1416</td>\n",
       "      <td>5.9156</td>\n",
       "      <td>-0.5775</td>\n",
       "      <td>5.7600</td>\n",
       "      <td>30.3238</td>\n",
       "      <td>2.1251</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.2198</td>\n",
       "      <td>17.3089</td>\n",
       "      <td>30.9548</td>\n",
       "      <td>1.4918</td>\n",
       "      <td>12.8721</td>\n",
       "      <td>3.4902</td>\n",
       "      <td>8.2856</td>\n",
       "      <td>11.9794</td>\n",
       "      <td>14.0176</td>\n",
       "      <td>15.0763</td>\n",
       "      <td>3.7662</td>\n",
       "      <td>6.0426</td>\n",
       "      <td>4.4243</td>\n",
       "      <td>14.1799</td>\n",
       "      <td>2.0921</td>\n",
       "      <td>1.5493</td>\n",
       "      <td>3.2206</td>\n",
       "      <td>0.0172</td>\n",
       "      <td>-6.6602</td>\n",
       "      <td>8.4785</td>\n",
       "      <td>42.0248</td>\n",
       "      <td>11.4164</td>\n",
       "      <td>0.4564</td>\n",
       "      <td>9.4006</td>\n",
       "      <td>0.9685</td>\n",
       "      <td>12.4929</td>\n",
       "      <td>14.1240</td>\n",
       "      <td>4.0388</td>\n",
       "      <td>-4.4442</td>\n",
       "      <td>16.6684</td>\n",
       "      <td>12.5380</td>\n",
       "      <td>0.9205</td>\n",
       "      <td>10.5998</td>\n",
       "      <td>7.5147</td>\n",
       "      <td>-4.1748</td>\n",
       "      <td>-0.4824</td>\n",
       "      <td>10.5267</td>\n",
       "      <td>17.7547</td>\n",
       "      <td>-6.5226</td>\n",
       "      <td>-2.5502</td>\n",
       "      <td>-5.1547</td>\n",
       "      <td>-2.1246</td>\n",
       "      <td>19.8319</td>\n",
       "      <td>13.0752</td>\n",
       "      <td>9.2275</td>\n",
       "      <td>3.0213</td>\n",
       "      <td>11.6793</td>\n",
       "      <td>-11.6827</td>\n",
       "      <td>4.1017</td>\n",
       "      <td>5.2954</td>\n",
       "      <td>18.7741</td>\n",
       "      <td>9.8892</td>\n",
       "      <td>7.5219</td>\n",
       "      <td>14.9745</td>\n",
       "      <td>18.9880</td>\n",
       "      <td>1.0842</td>\n",
       "      <td>11.9125</td>\n",
       "      <td>-4.5103</td>\n",
       "      <td>16.1361</td>\n",
       "      <td>11.0067</td>\n",
       "      <td>5.9232</td>\n",
       "      <td>5.4113</td>\n",
       "      <td>3.8302</td>\n",
       "      <td>5.7380</td>\n",
       "      <td>-8.6105</td>\n",
       "      <td>22.9530</td>\n",
       "      <td>2.5531</td>\n",
       "      <td>-0.2836</td>\n",
       "      <td>4.3416</td>\n",
       "      <td>5.1855</td>\n",
       "      <td>4.2603</td>\n",
       "      <td>1.6779</td>\n",
       "      <td>29.0849</td>\n",
       "      <td>8.4685</td>\n",
       "      <td>18.1317</td>\n",
       "      <td>12.2818</td>\n",
       "      <td>-0.6912</td>\n",
       "      <td>10.2226</td>\n",
       "      <td>-5.5579</td>\n",
       "      <td>2.2926</td>\n",
       "      <td>-4.5358</td>\n",
       "      <td>10.3903</td>\n",
       "      <td>-15.4937</td>\n",
       "      <td>3.9697</td>\n",
       "      <td>31.3521</td>\n",
       "      <td>-1.1651</td>\n",
       "      <td>9.2874</td>\n",
       "      <td>-23.5705</td>\n",
       "      <td>13.2643</td>\n",
       "      <td>1.6591</td>\n",
       "      <td>-2.1556</td>\n",
       "      <td>11.8495</td>\n",
       "      <td>-1.4300</td>\n",
       "      <td>2.4508</td>\n",
       "      <td>13.7112</td>\n",
       "      <td>2.4669</td>\n",
       "      <td>4.3654</td>\n",
       "      <td>10.7200</td>\n",
       "      <td>15.4722</td>\n",
       "      <td>-8.7197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>8.5304</td>\n",
       "      <td>1.2543</td>\n",
       "      <td>11.3047</td>\n",
       "      <td>5.1858</td>\n",
       "      <td>9.1974</td>\n",
       "      <td>-4.0117</td>\n",
       "      <td>6.0196</td>\n",
       "      <td>18.6316</td>\n",
       "      <td>-4.4131</td>\n",
       "      <td>5.9739</td>\n",
       "      <td>-1.3809</td>\n",
       "      <td>-0.3310</td>\n",
       "      <td>14.1129</td>\n",
       "      <td>2.5667</td>\n",
       "      <td>5.4988</td>\n",
       "      <td>14.1853</td>\n",
       "      <td>7.0196</td>\n",
       "      <td>4.6564</td>\n",
       "      <td>29.1609</td>\n",
       "      <td>0.0910</td>\n",
       "      <td>12.1469</td>\n",
       "      <td>3.1389</td>\n",
       "      <td>5.2578</td>\n",
       "      <td>2.4228</td>\n",
       "      <td>16.2064</td>\n",
       "      <td>13.5023</td>\n",
       "      <td>-5.2341</td>\n",
       "      <td>-3.6648</td>\n",
       "      <td>5.7080</td>\n",
       "      <td>2.9965</td>\n",
       "      <td>-10.4720</td>\n",
       "      <td>11.4938</td>\n",
       "      <td>-0.9660</td>\n",
       "      <td>15.3445</td>\n",
       "      <td>10.6361</td>\n",
       "      <td>0.8966</td>\n",
       "      <td>6.7428</td>\n",
       "      <td>2.3421</td>\n",
       "      <td>12.8678</td>\n",
       "      <td>-1.5536</td>\n",
       "      <td>10.0309</td>\n",
       "      <td>3.1337</td>\n",
       "      <td>10.5742</td>\n",
       "      <td>11.7664</td>\n",
       "      <td>2.1782</td>\n",
       "      <td>-41.1924</td>\n",
       "      <td>13.5322</td>\n",
       "      <td>-17.3834</td>\n",
       "      <td>6.3806</td>\n",
       "      <td>12.5589</td>\n",
       "      <td>11.6887</td>\n",
       "      <td>25.3930</td>\n",
       "      <td>1.5776</td>\n",
       "      <td>6.8481</td>\n",
       "      <td>8.7348</td>\n",
       "      <td>16.4239</td>\n",
       "      <td>21.7056</td>\n",
       "      <td>6.9345</td>\n",
       "      <td>1.6678</td>\n",
       "      <td>9.5249</td>\n",
       "      <td>5.3383</td>\n",
       "      <td>-18.7083</td>\n",
       "      <td>1.3382</td>\n",
       "      <td>-1.7401</td>\n",
       "      <td>5.8398</td>\n",
       "      <td>3.1051</td>\n",
       "      <td>4.4307</td>\n",
       "      <td>16.0005</td>\n",
       "      <td>5.0306</td>\n",
       "      <td>-7.3365</td>\n",
       "      <td>12.2806</td>\n",
       "      <td>0.6992</td>\n",
       "      <td>-0.7772</td>\n",
       "      <td>21.5123</td>\n",
       "      <td>6.7803</td>\n",
       "      <td>18.1896</td>\n",
       "      <td>6.9388</td>\n",
       "      <td>22.1336</td>\n",
       "      <td>6.3755</td>\n",
       "      <td>13.1525</td>\n",
       "      <td>1.9772</td>\n",
       "      <td>14.0406</td>\n",
       "      <td>6.6904</td>\n",
       "      <td>9.9732</td>\n",
       "      <td>-11.5679</td>\n",
       "      <td>20.4525</td>\n",
       "      <td>9.4951</td>\n",
       "      <td>9.6343</td>\n",
       "      <td>8.1252</td>\n",
       "      <td>2.6059</td>\n",
       "      <td>-17.4201</td>\n",
       "      <td>7.1848</td>\n",
       "      <td>15.3484</td>\n",
       "      <td>10.6522</td>\n",
       "      <td>5.9897</td>\n",
       "      <td>0.3392</td>\n",
       "      <td>10.3516</td>\n",
       "      <td>29.8204</td>\n",
       "      <td>1.9998</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.7257</td>\n",
       "      <td>15.4712</td>\n",
       "      <td>35.6020</td>\n",
       "      <td>1.6570</td>\n",
       "      <td>13.0783</td>\n",
       "      <td>2.7752</td>\n",
       "      <td>6.4986</td>\n",
       "      <td>4.6835</td>\n",
       "      <td>13.7963</td>\n",
       "      <td>17.7261</td>\n",
       "      <td>1.7375</td>\n",
       "      <td>5.5689</td>\n",
       "      <td>3.6609</td>\n",
       "      <td>8.9725</td>\n",
       "      <td>4.1159</td>\n",
       "      <td>1.0693</td>\n",
       "      <td>2.0234</td>\n",
       "      <td>8.2760</td>\n",
       "      <td>-6.8610</td>\n",
       "      <td>0.2780</td>\n",
       "      <td>17.0488</td>\n",
       "      <td>11.6704</td>\n",
       "      <td>3.1215</td>\n",
       "      <td>8.5093</td>\n",
       "      <td>5.6367</td>\n",
       "      <td>12.0099</td>\n",
       "      <td>14.2372</td>\n",
       "      <td>-6.1600</td>\n",
       "      <td>-5.6690</td>\n",
       "      <td>8.9094</td>\n",
       "      <td>11.0605</td>\n",
       "      <td>0.4583</td>\n",
       "      <td>9.7974</td>\n",
       "      <td>7.0891</td>\n",
       "      <td>2.6849</td>\n",
       "      <td>8.4970</td>\n",
       "      <td>15.7774</td>\n",
       "      <td>4.8775</td>\n",
       "      <td>3.6129</td>\n",
       "      <td>6.7530</td>\n",
       "      <td>11.1003</td>\n",
       "      <td>15.3593</td>\n",
       "      <td>2.2105</td>\n",
       "      <td>8.2280</td>\n",
       "      <td>9.0717</td>\n",
       "      <td>-5.0947</td>\n",
       "      <td>8.7644</td>\n",
       "      <td>-2.2873</td>\n",
       "      <td>4.1240</td>\n",
       "      <td>-13.3006</td>\n",
       "      <td>18.7454</td>\n",
       "      <td>9.3783</td>\n",
       "      <td>1.5284</td>\n",
       "      <td>16.0407</td>\n",
       "      <td>7.7732</td>\n",
       "      <td>1.4316</td>\n",
       "      <td>14.8679</td>\n",
       "      <td>3.3619</td>\n",
       "      <td>11.5799</td>\n",
       "      <td>14.2058</td>\n",
       "      <td>30.9641</td>\n",
       "      <td>5.6723</td>\n",
       "      <td>3.6873</td>\n",
       "      <td>13.0429</td>\n",
       "      <td>-10.6572</td>\n",
       "      <td>15.5134</td>\n",
       "      <td>3.2185</td>\n",
       "      <td>9.0535</td>\n",
       "      <td>7.0535</td>\n",
       "      <td>5.3924</td>\n",
       "      <td>-0.7720</td>\n",
       "      <td>-8.1783</td>\n",
       "      <td>29.9227</td>\n",
       "      <td>-5.6274</td>\n",
       "      <td>10.5018</td>\n",
       "      <td>9.6083</td>\n",
       "      <td>-0.4935</td>\n",
       "      <td>8.1696</td>\n",
       "      <td>-4.3605</td>\n",
       "      <td>5.2110</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>12.0030</td>\n",
       "      <td>-10.3812</td>\n",
       "      <td>5.8496</td>\n",
       "      <td>25.1958</td>\n",
       "      <td>-8.8468</td>\n",
       "      <td>11.8263</td>\n",
       "      <td>-8.7112</td>\n",
       "      <td>15.9072</td>\n",
       "      <td>0.9812</td>\n",
       "      <td>10.6165</td>\n",
       "      <td>8.8349</td>\n",
       "      <td>0.9403</td>\n",
       "      <td>10.1282</td>\n",
       "      <td>15.5765</td>\n",
       "      <td>0.4773</td>\n",
       "      <td>-1.4852</td>\n",
       "      <td>9.8714</td>\n",
       "      <td>19.1293</td>\n",
       "      <td>-20.9760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>5.4827</td>\n",
       "      <td>-10.3581</td>\n",
       "      <td>10.1407</td>\n",
       "      <td>7.0479</td>\n",
       "      <td>10.2628</td>\n",
       "      <td>9.8052</td>\n",
       "      <td>4.8950</td>\n",
       "      <td>20.2537</td>\n",
       "      <td>1.5233</td>\n",
       "      <td>8.3442</td>\n",
       "      <td>-4.7057</td>\n",
       "      <td>-3.0422</td>\n",
       "      <td>13.6751</td>\n",
       "      <td>3.8183</td>\n",
       "      <td>10.8535</td>\n",
       "      <td>14.2126</td>\n",
       "      <td>9.8837</td>\n",
       "      <td>2.6541</td>\n",
       "      <td>21.2181</td>\n",
       "      <td>20.8163</td>\n",
       "      <td>12.4666</td>\n",
       "      <td>12.3696</td>\n",
       "      <td>4.7473</td>\n",
       "      <td>2.7936</td>\n",
       "      <td>5.2189</td>\n",
       "      <td>13.5670</td>\n",
       "      <td>-15.4246</td>\n",
       "      <td>-0.1655</td>\n",
       "      <td>7.2633</td>\n",
       "      <td>3.4310</td>\n",
       "      <td>-9.1508</td>\n",
       "      <td>9.7320</td>\n",
       "      <td>3.1062</td>\n",
       "      <td>22.3076</td>\n",
       "      <td>11.9593</td>\n",
       "      <td>9.9255</td>\n",
       "      <td>4.0702</td>\n",
       "      <td>4.9934</td>\n",
       "      <td>8.0667</td>\n",
       "      <td>0.8804</td>\n",
       "      <td>-19.0841</td>\n",
       "      <td>5.2272</td>\n",
       "      <td>9.5977</td>\n",
       "      <td>12.1801</td>\n",
       "      <td>8.3565</td>\n",
       "      <td>15.1170</td>\n",
       "      <td>10.0921</td>\n",
       "      <td>-20.8504</td>\n",
       "      <td>8.6758</td>\n",
       "      <td>8.1292</td>\n",
       "      <td>11.8932</td>\n",
       "      <td>10.6869</td>\n",
       "      <td>-0.6434</td>\n",
       "      <td>5.6510</td>\n",
       "      <td>9.3742</td>\n",
       "      <td>25.8831</td>\n",
       "      <td>19.8701</td>\n",
       "      <td>5.4834</td>\n",
       "      <td>-4.0304</td>\n",
       "      <td>8.5160</td>\n",
       "      <td>8.9776</td>\n",
       "      <td>-5.6619</td>\n",
       "      <td>2.8117</td>\n",
       "      <td>2.5996</td>\n",
       "      <td>9.0986</td>\n",
       "      <td>7.1167</td>\n",
       "      <td>4.9466</td>\n",
       "      <td>13.8268</td>\n",
       "      <td>5.0093</td>\n",
       "      <td>4.7782</td>\n",
       "      <td>19.2081</td>\n",
       "      <td>0.4340</td>\n",
       "      <td>0.8459</td>\n",
       "      <td>34.8598</td>\n",
       "      <td>20.7048</td>\n",
       "      <td>16.4953</td>\n",
       "      <td>-9.7077</td>\n",
       "      <td>19.6357</td>\n",
       "      <td>7.6587</td>\n",
       "      <td>15.5744</td>\n",
       "      <td>16.1691</td>\n",
       "      <td>14.3299</td>\n",
       "      <td>1.3360</td>\n",
       "      <td>-0.4412</td>\n",
       "      <td>-0.2830</td>\n",
       "      <td>14.9105</td>\n",
       "      <td>-3.9016</td>\n",
       "      <td>14.6881</td>\n",
       "      <td>7.3220</td>\n",
       "      <td>-5.1443</td>\n",
       "      <td>-34.3488</td>\n",
       "      <td>7.0194</td>\n",
       "      <td>12.4785</td>\n",
       "      <td>9.6665</td>\n",
       "      <td>13.2595</td>\n",
       "      <td>-0.5624</td>\n",
       "      <td>5.6347</td>\n",
       "      <td>9.5853</td>\n",
       "      <td>1.4515</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.5065</td>\n",
       "      <td>14.1663</td>\n",
       "      <td>28.0256</td>\n",
       "      <td>1.3935</td>\n",
       "      <td>10.8257</td>\n",
       "      <td>4.2954</td>\n",
       "      <td>8.2125</td>\n",
       "      <td>26.2595</td>\n",
       "      <td>14.0232</td>\n",
       "      <td>19.4604</td>\n",
       "      <td>8.6896</td>\n",
       "      <td>8.1036</td>\n",
       "      <td>1.2057</td>\n",
       "      <td>8.9156</td>\n",
       "      <td>0.9777</td>\n",
       "      <td>2.3797</td>\n",
       "      <td>3.1638</td>\n",
       "      <td>37.8664</td>\n",
       "      <td>-3.3864</td>\n",
       "      <td>-2.4090</td>\n",
       "      <td>29.7978</td>\n",
       "      <td>12.2056</td>\n",
       "      <td>4.7688</td>\n",
       "      <td>7.9344</td>\n",
       "      <td>2.2102</td>\n",
       "      <td>12.6482</td>\n",
       "      <td>14.3377</td>\n",
       "      <td>2.3268</td>\n",
       "      <td>2.3930</td>\n",
       "      <td>13.7005</td>\n",
       "      <td>12.7047</td>\n",
       "      <td>0.7507</td>\n",
       "      <td>7.7726</td>\n",
       "      <td>6.5950</td>\n",
       "      <td>0.2990</td>\n",
       "      <td>12.9154</td>\n",
       "      <td>29.9162</td>\n",
       "      <td>6.8031</td>\n",
       "      <td>10.5031</td>\n",
       "      <td>-6.0452</td>\n",
       "      <td>-4.5298</td>\n",
       "      <td>1.3903</td>\n",
       "      <td>5.0469</td>\n",
       "      <td>12.9740</td>\n",
       "      <td>9.3878</td>\n",
       "      <td>-0.1113</td>\n",
       "      <td>11.6749</td>\n",
       "      <td>16.8588</td>\n",
       "      <td>4.2600</td>\n",
       "      <td>14.6476</td>\n",
       "      <td>14.4431</td>\n",
       "      <td>14.1649</td>\n",
       "      <td>9.4875</td>\n",
       "      <td>16.5769</td>\n",
       "      <td>7.2638</td>\n",
       "      <td>-2.2008</td>\n",
       "      <td>12.5953</td>\n",
       "      <td>7.4487</td>\n",
       "      <td>23.1407</td>\n",
       "      <td>10.4597</td>\n",
       "      <td>39.3654</td>\n",
       "      <td>5.5228</td>\n",
       "      <td>3.3159</td>\n",
       "      <td>4.3324</td>\n",
       "      <td>-0.5382</td>\n",
       "      <td>13.3009</td>\n",
       "      <td>3.1243</td>\n",
       "      <td>-4.1731</td>\n",
       "      <td>1.2330</td>\n",
       "      <td>6.1513</td>\n",
       "      <td>-0.0391</td>\n",
       "      <td>1.4950</td>\n",
       "      <td>16.8874</td>\n",
       "      <td>-2.9787</td>\n",
       "      <td>27.4035</td>\n",
       "      <td>15.8819</td>\n",
       "      <td>-10.9660</td>\n",
       "      <td>15.6415</td>\n",
       "      <td>-9.4056</td>\n",
       "      <td>4.4611</td>\n",
       "      <td>-3.0835</td>\n",
       "      <td>8.5549</td>\n",
       "      <td>-2.8517</td>\n",
       "      <td>13.4770</td>\n",
       "      <td>24.4721</td>\n",
       "      <td>-3.4824</td>\n",
       "      <td>4.9178</td>\n",
       "      <td>-2.0720</td>\n",
       "      <td>11.5390</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>-0.7484</td>\n",
       "      <td>10.9935</td>\n",
       "      <td>1.9803</td>\n",
       "      <td>2.1800</td>\n",
       "      <td>12.9813</td>\n",
       "      <td>2.1281</td>\n",
       "      <td>-7.1086</td>\n",
       "      <td>7.0618</td>\n",
       "      <td>19.8956</td>\n",
       "      <td>-23.1794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>8.5374</td>\n",
       "      <td>-1.3222</td>\n",
       "      <td>12.0220</td>\n",
       "      <td>6.5749</td>\n",
       "      <td>8.8458</td>\n",
       "      <td>3.1744</td>\n",
       "      <td>4.9397</td>\n",
       "      <td>20.5660</td>\n",
       "      <td>3.3755</td>\n",
       "      <td>7.4578</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>-5.0659</td>\n",
       "      <td>14.0526</td>\n",
       "      <td>13.5010</td>\n",
       "      <td>8.7660</td>\n",
       "      <td>14.7352</td>\n",
       "      <td>10.0383</td>\n",
       "      <td>-15.3508</td>\n",
       "      <td>2.1273</td>\n",
       "      <td>21.4797</td>\n",
       "      <td>14.5372</td>\n",
       "      <td>12.5527</td>\n",
       "      <td>2.9707</td>\n",
       "      <td>4.2398</td>\n",
       "      <td>13.7796</td>\n",
       "      <td>14.1408</td>\n",
       "      <td>1.0061</td>\n",
       "      <td>-1.3479</td>\n",
       "      <td>5.2570</td>\n",
       "      <td>6.5911</td>\n",
       "      <td>6.2161</td>\n",
       "      <td>9.5540</td>\n",
       "      <td>2.3628</td>\n",
       "      <td>10.2124</td>\n",
       "      <td>10.8047</td>\n",
       "      <td>-2.5588</td>\n",
       "      <td>6.0720</td>\n",
       "      <td>3.2613</td>\n",
       "      <td>16.5632</td>\n",
       "      <td>8.8336</td>\n",
       "      <td>-4.8327</td>\n",
       "      <td>0.9554</td>\n",
       "      <td>12.3754</td>\n",
       "      <td>11.4241</td>\n",
       "      <td>6.6917</td>\n",
       "      <td>-12.9761</td>\n",
       "      <td>13.7343</td>\n",
       "      <td>5.0150</td>\n",
       "      <td>31.3923</td>\n",
       "      <td>5.8555</td>\n",
       "      <td>12.6082</td>\n",
       "      <td>1.4182</td>\n",
       "      <td>-4.1185</td>\n",
       "      <td>6.2536</td>\n",
       "      <td>1.4257</td>\n",
       "      <td>13.5426</td>\n",
       "      <td>15.4090</td>\n",
       "      <td>6.8761</td>\n",
       "      <td>1.7476</td>\n",
       "      <td>10.0413</td>\n",
       "      <td>15.2857</td>\n",
       "      <td>-4.1378</td>\n",
       "      <td>0.7928</td>\n",
       "      <td>2.5301</td>\n",
       "      <td>8.1458</td>\n",
       "      <td>2.5738</td>\n",
       "      <td>5.9876</td>\n",
       "      <td>13.0758</td>\n",
       "      <td>5.0087</td>\n",
       "      <td>-9.7824</td>\n",
       "      <td>8.9289</td>\n",
       "      <td>0.4205</td>\n",
       "      <td>-2.5463</td>\n",
       "      <td>2.9428</td>\n",
       "      <td>10.7087</td>\n",
       "      <td>12.2008</td>\n",
       "      <td>12.5465</td>\n",
       "      <td>19.4201</td>\n",
       "      <td>5.5060</td>\n",
       "      <td>14.1586</td>\n",
       "      <td>17.5941</td>\n",
       "      <td>15.4375</td>\n",
       "      <td>-13.2668</td>\n",
       "      <td>14.0885</td>\n",
       "      <td>4.0357</td>\n",
       "      <td>22.3119</td>\n",
       "      <td>1.8571</td>\n",
       "      <td>16.5210</td>\n",
       "      <td>10.8149</td>\n",
       "      <td>0.3256</td>\n",
       "      <td>-21.4797</td>\n",
       "      <td>6.9174</td>\n",
       "      <td>9.9483</td>\n",
       "      <td>10.3696</td>\n",
       "      <td>11.0362</td>\n",
       "      <td>0.1892</td>\n",
       "      <td>19.4321</td>\n",
       "      <td>40.3383</td>\n",
       "      <td>1.4105</td>\n",
       "      <td>...</td>\n",
       "      <td>1.7021</td>\n",
       "      <td>2.5363</td>\n",
       "      <td>3.8763</td>\n",
       "      <td>1.5173</td>\n",
       "      <td>13.4083</td>\n",
       "      <td>2.8965</td>\n",
       "      <td>7.0919</td>\n",
       "      <td>21.6304</td>\n",
       "      <td>14.2000</td>\n",
       "      <td>23.0368</td>\n",
       "      <td>10.3445</td>\n",
       "      <td>6.0369</td>\n",
       "      <td>5.0227</td>\n",
       "      <td>12.6600</td>\n",
       "      <td>2.1278</td>\n",
       "      <td>4.0592</td>\n",
       "      <td>1.9084</td>\n",
       "      <td>11.6095</td>\n",
       "      <td>7.5397</td>\n",
       "      <td>8.1972</td>\n",
       "      <td>20.0844</td>\n",
       "      <td>10.4440</td>\n",
       "      <td>8.4676</td>\n",
       "      <td>5.0350</td>\n",
       "      <td>4.3103</td>\n",
       "      <td>12.0067</td>\n",
       "      <td>13.7149</td>\n",
       "      <td>1.6143</td>\n",
       "      <td>-1.2328</td>\n",
       "      <td>22.7248</td>\n",
       "      <td>12.6609</td>\n",
       "      <td>0.8039</td>\n",
       "      <td>4.7666</td>\n",
       "      <td>6.7888</td>\n",
       "      <td>5.8537</td>\n",
       "      <td>-4.5434</td>\n",
       "      <td>19.0111</td>\n",
       "      <td>12.6907</td>\n",
       "      <td>-2.9322</td>\n",
       "      <td>12.7898</td>\n",
       "      <td>12.0466</td>\n",
       "      <td>13.1646</td>\n",
       "      <td>7.7063</td>\n",
       "      <td>11.6549</td>\n",
       "      <td>9.8274</td>\n",
       "      <td>1.8061</td>\n",
       "      <td>8.6963</td>\n",
       "      <td>1.8057</td>\n",
       "      <td>3.8265</td>\n",
       "      <td>-16.3027</td>\n",
       "      <td>13.7106</td>\n",
       "      <td>9.7908</td>\n",
       "      <td>5.8497</td>\n",
       "      <td>15.4378</td>\n",
       "      <td>5.0372</td>\n",
       "      <td>-8.7673</td>\n",
       "      <td>13.6035</td>\n",
       "      <td>-3.5002</td>\n",
       "      <td>13.9785</td>\n",
       "      <td>14.6118</td>\n",
       "      <td>19.7251</td>\n",
       "      <td>5.3882</td>\n",
       "      <td>3.6775</td>\n",
       "      <td>7.4753</td>\n",
       "      <td>-11.0780</td>\n",
       "      <td>24.8712</td>\n",
       "      <td>2.6415</td>\n",
       "      <td>2.2673</td>\n",
       "      <td>7.2788</td>\n",
       "      <td>5.6406</td>\n",
       "      <td>7.2048</td>\n",
       "      <td>3.4504</td>\n",
       "      <td>2.4130</td>\n",
       "      <td>11.1674</td>\n",
       "      <td>14.5499</td>\n",
       "      <td>10.6151</td>\n",
       "      <td>-5.7922</td>\n",
       "      <td>13.9407</td>\n",
       "      <td>7.1078</td>\n",
       "      <td>1.1019</td>\n",
       "      <td>9.4590</td>\n",
       "      <td>9.8243</td>\n",
       "      <td>5.9917</td>\n",
       "      <td>5.1634</td>\n",
       "      <td>8.1154</td>\n",
       "      <td>3.6638</td>\n",
       "      <td>3.3102</td>\n",
       "      <td>-19.7819</td>\n",
       "      <td>13.4499</td>\n",
       "      <td>1.3104</td>\n",
       "      <td>9.5702</td>\n",
       "      <td>9.0766</td>\n",
       "      <td>1.6580</td>\n",
       "      <td>3.5813</td>\n",
       "      <td>15.1874</td>\n",
       "      <td>3.1656</td>\n",
       "      <td>3.9567</td>\n",
       "      <td>9.2295</td>\n",
       "      <td>13.0168</td>\n",
       "      <td>-4.2108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>11.7058</td>\n",
       "      <td>-0.1327</td>\n",
       "      <td>14.1295</td>\n",
       "      <td>7.7506</td>\n",
       "      <td>9.1035</td>\n",
       "      <td>-8.5848</td>\n",
       "      <td>6.8595</td>\n",
       "      <td>10.6048</td>\n",
       "      <td>2.9890</td>\n",
       "      <td>7.1437</td>\n",
       "      <td>5.1025</td>\n",
       "      <td>-3.2827</td>\n",
       "      <td>14.1013</td>\n",
       "      <td>8.9672</td>\n",
       "      <td>4.7276</td>\n",
       "      <td>14.5811</td>\n",
       "      <td>11.8615</td>\n",
       "      <td>3.1480</td>\n",
       "      <td>18.0126</td>\n",
       "      <td>13.8006</td>\n",
       "      <td>1.6026</td>\n",
       "      <td>16.3059</td>\n",
       "      <td>6.7954</td>\n",
       "      <td>3.6015</td>\n",
       "      <td>13.6569</td>\n",
       "      <td>13.8807</td>\n",
       "      <td>8.6228</td>\n",
       "      <td>-2.2654</td>\n",
       "      <td>5.2255</td>\n",
       "      <td>7.0165</td>\n",
       "      <td>-15.6961</td>\n",
       "      <td>10.6239</td>\n",
       "      <td>-4.7674</td>\n",
       "      <td>17.5447</td>\n",
       "      <td>11.8668</td>\n",
       "      <td>3.0154</td>\n",
       "      <td>4.2546</td>\n",
       "      <td>6.7601</td>\n",
       "      <td>5.9613</td>\n",
       "      <td>0.3695</td>\n",
       "      <td>-14.4364</td>\n",
       "      <td>5.1392</td>\n",
       "      <td>11.6336</td>\n",
       "      <td>12.0338</td>\n",
       "      <td>18.9670</td>\n",
       "      <td>12.0144</td>\n",
       "      <td>16.2096</td>\n",
       "      <td>-2.1966</td>\n",
       "      <td>1.1174</td>\n",
       "      <td>13.4532</td>\n",
       "      <td>12.7925</td>\n",
       "      <td>4.3775</td>\n",
       "      <td>-0.1543</td>\n",
       "      <td>5.6794</td>\n",
       "      <td>0.8210</td>\n",
       "      <td>19.1358</td>\n",
       "      <td>12.6589</td>\n",
       "      <td>6.4394</td>\n",
       "      <td>4.3425</td>\n",
       "      <td>8.7003</td>\n",
       "      <td>12.0586</td>\n",
       "      <td>-10.4753</td>\n",
       "      <td>-0.0337</td>\n",
       "      <td>5.6603</td>\n",
       "      <td>6.2529</td>\n",
       "      <td>1.5238</td>\n",
       "      <td>4.5356</td>\n",
       "      <td>20.1344</td>\n",
       "      <td>5.0267</td>\n",
       "      <td>-1.8628</td>\n",
       "      <td>39.8219</td>\n",
       "      <td>1.0498</td>\n",
       "      <td>-0.9113</td>\n",
       "      <td>38.5076</td>\n",
       "      <td>2.2201</td>\n",
       "      <td>9.5235</td>\n",
       "      <td>8.1522</td>\n",
       "      <td>14.9224</td>\n",
       "      <td>6.1573</td>\n",
       "      <td>15.5221</td>\n",
       "      <td>11.8133</td>\n",
       "      <td>16.7661</td>\n",
       "      <td>-14.6524</td>\n",
       "      <td>-0.4469</td>\n",
       "      <td>0.0306</td>\n",
       "      <td>22.5276</td>\n",
       "      <td>6.9774</td>\n",
       "      <td>2.2563</td>\n",
       "      <td>3.5779</td>\n",
       "      <td>1.4268</td>\n",
       "      <td>9.0680</td>\n",
       "      <td>7.0197</td>\n",
       "      <td>19.7765</td>\n",
       "      <td>10.0499</td>\n",
       "      <td>11.4803</td>\n",
       "      <td>0.2548</td>\n",
       "      <td>16.7029</td>\n",
       "      <td>45.5510</td>\n",
       "      <td>1.5795</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.3858</td>\n",
       "      <td>17.8630</td>\n",
       "      <td>23.2274</td>\n",
       "      <td>1.4375</td>\n",
       "      <td>14.4838</td>\n",
       "      <td>4.3806</td>\n",
       "      <td>10.6976</td>\n",
       "      <td>18.4023</td>\n",
       "      <td>14.2212</td>\n",
       "      <td>16.0638</td>\n",
       "      <td>6.3933</td>\n",
       "      <td>6.8699</td>\n",
       "      <td>2.7253</td>\n",
       "      <td>12.6458</td>\n",
       "      <td>3.2376</td>\n",
       "      <td>3.4218</td>\n",
       "      <td>-0.5658</td>\n",
       "      <td>-5.6840</td>\n",
       "      <td>4.7753</td>\n",
       "      <td>10.3320</td>\n",
       "      <td>39.7127</td>\n",
       "      <td>11.2319</td>\n",
       "      <td>-1.2978</td>\n",
       "      <td>12.4827</td>\n",
       "      <td>6.5034</td>\n",
       "      <td>12.7157</td>\n",
       "      <td>13.3054</td>\n",
       "      <td>-1.9678</td>\n",
       "      <td>-1.2363</td>\n",
       "      <td>11.5686</td>\n",
       "      <td>12.6428</td>\n",
       "      <td>0.4792</td>\n",
       "      <td>7.1984</td>\n",
       "      <td>7.1434</td>\n",
       "      <td>-0.2056</td>\n",
       "      <td>-16.3908</td>\n",
       "      <td>27.1589</td>\n",
       "      <td>23.5997</td>\n",
       "      <td>-4.6175</td>\n",
       "      <td>11.7989</td>\n",
       "      <td>12.5683</td>\n",
       "      <td>-3.6145</td>\n",
       "      <td>22.1069</td>\n",
       "      <td>9.5539</td>\n",
       "      <td>9.2721</td>\n",
       "      <td>-1.6214</td>\n",
       "      <td>12.9327</td>\n",
       "      <td>6.8080</td>\n",
       "      <td>4.2135</td>\n",
       "      <td>22.1044</td>\n",
       "      <td>20.0502</td>\n",
       "      <td>6.9953</td>\n",
       "      <td>9.3823</td>\n",
       "      <td>20.5534</td>\n",
       "      <td>3.4368</td>\n",
       "      <td>-15.2208</td>\n",
       "      <td>13.0974</td>\n",
       "      <td>-14.0888</td>\n",
       "      <td>11.7586</td>\n",
       "      <td>14.5259</td>\n",
       "      <td>22.8700</td>\n",
       "      <td>5.6688</td>\n",
       "      <td>6.1159</td>\n",
       "      <td>13.2433</td>\n",
       "      <td>-11.9785</td>\n",
       "      <td>26.2040</td>\n",
       "      <td>3.2348</td>\n",
       "      <td>-5.5775</td>\n",
       "      <td>5.7036</td>\n",
       "      <td>6.1717</td>\n",
       "      <td>-1.6039</td>\n",
       "      <td>-2.4866</td>\n",
       "      <td>17.2728</td>\n",
       "      <td>2.3640</td>\n",
       "      <td>14.0037</td>\n",
       "      <td>12.9165</td>\n",
       "      <td>-12.0311</td>\n",
       "      <td>10.1161</td>\n",
       "      <td>-8.7562</td>\n",
       "      <td>6.0889</td>\n",
       "      <td>-1.3620</td>\n",
       "      <td>10.3559</td>\n",
       "      <td>-7.4915</td>\n",
       "      <td>9.4588</td>\n",
       "      <td>3.9829</td>\n",
       "      <td>5.8580</td>\n",
       "      <td>8.3635</td>\n",
       "      <td>-24.8254</td>\n",
       "      <td>11.4928</td>\n",
       "      <td>1.6321</td>\n",
       "      <td>4.2259</td>\n",
       "      <td>9.1723</td>\n",
       "      <td>1.2835</td>\n",
       "      <td>3.3778</td>\n",
       "      <td>19.5542</td>\n",
       "      <td>-0.2860</td>\n",
       "      <td>-5.1612</td>\n",
       "      <td>7.2882</td>\n",
       "      <td>13.9260</td>\n",
       "      <td>-9.1846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_code    var_0    var_1    var_2   var_3    var_4   var_5   var_6  \\\n",
       "0  test_0  11.0656   7.7798  12.9536  9.4292  11.4327 -2.3805  5.8493   \n",
       "1  test_1   8.5304   1.2543  11.3047  5.1858   9.1974 -4.0117  6.0196   \n",
       "2  test_2   5.4827 -10.3581  10.1407  7.0479  10.2628  9.8052  4.8950   \n",
       "3  test_3   8.5374  -1.3222  12.0220  6.5749   8.8458  3.1744  4.9397   \n",
       "4  test_4  11.7058  -0.1327  14.1295  7.7506   9.1035 -8.5848  6.8595   \n",
       "\n",
       "     var_7   var_8   var_9  var_10  var_11   var_12   var_13   var_14  \\\n",
       "0  18.2675  2.1337  8.8100 -2.0248 -4.3554  13.9696   0.3458   7.5408   \n",
       "1  18.6316 -4.4131  5.9739 -1.3809 -0.3310  14.1129   2.5667   5.4988   \n",
       "2  20.2537  1.5233  8.3442 -4.7057 -3.0422  13.6751   3.8183  10.8535   \n",
       "3  20.5660  3.3755  7.4578  0.0095 -5.0659  14.0526  13.5010   8.7660   \n",
       "4  10.6048  2.9890  7.1437  5.1025 -3.2827  14.1013   8.9672   4.7276   \n",
       "\n",
       "    var_15   var_16   var_17   var_18   var_19   var_20   var_21  var_22  \\\n",
       "0  14.5001   7.7028 -19.0919  15.5806  16.1763   3.7088  18.8064  1.5899   \n",
       "1  14.1853   7.0196   4.6564  29.1609   0.0910  12.1469   3.1389  5.2578   \n",
       "2  14.2126   9.8837   2.6541  21.2181  20.8163  12.4666  12.3696  4.7473   \n",
       "3  14.7352  10.0383 -15.3508   2.1273  21.4797  14.5372  12.5527  2.9707   \n",
       "4  14.5811  11.8615   3.1480  18.0126  13.8006   1.6026  16.3059  6.7954   \n",
       "\n",
       "   var_23   var_24   var_25   var_26  var_27  var_28  var_29   var_30  \\\n",
       "0  3.0654   6.4509  14.1192  -9.4902 -2.1917  5.7107  3.7864  -1.7981   \n",
       "1  2.4228  16.2064  13.5023  -5.2341 -3.6648  5.7080  2.9965 -10.4720   \n",
       "2  2.7936   5.2189  13.5670 -15.4246 -0.1655  7.2633  3.4310  -9.1508   \n",
       "3  4.2398  13.7796  14.1408   1.0061 -1.3479  5.2570  6.5911   6.2161   \n",
       "4  3.6015  13.6569  13.8807   8.6228 -2.2654  5.2255  7.0165 -15.6961   \n",
       "\n",
       "    var_31  var_32   var_33   var_34  var_35  var_36  var_37   var_38  var_39  \\\n",
       "0   9.2645  2.0657  12.7753  11.3334  8.1462 -0.0610  3.5331   9.7804  8.7625   \n",
       "1  11.4938 -0.9660  15.3445  10.6361  0.8966  6.7428  2.3421  12.8678 -1.5536   \n",
       "2   9.7320  3.1062  22.3076  11.9593  9.9255  4.0702  4.9934   8.0667  0.8804   \n",
       "3   9.5540  2.3628  10.2124  10.8047 -2.5588  6.0720  3.2613  16.5632  8.8336   \n",
       "4  10.6239 -4.7674  17.5447  11.8668  3.0154  4.2546  6.7601   5.9613  0.3695   \n",
       "\n",
       "    var_40   var_41   var_42   var_43   var_44   var_45   var_46   var_47  \\\n",
       "0 -15.6305  18.8766  11.2864  11.8362  13.3680 -31.9891  12.1776   8.7714   \n",
       "1  10.0309   3.1337  10.5742  11.7664   2.1782 -41.1924  13.5322 -17.3834   \n",
       "2 -19.0841   5.2272   9.5977  12.1801   8.3565  15.1170  10.0921 -20.8504   \n",
       "3  -4.8327   0.9554  12.3754  11.4241   6.6917 -12.9761  13.7343   5.0150   \n",
       "4 -14.4364   5.1392  11.6336  12.0338  18.9670  12.0144  16.2096  -2.1966   \n",
       "\n",
       "    var_48   var_49   var_50   var_51  var_52  var_53  var_54   var_55  \\\n",
       "0  17.2011  16.8508  13.0534  14.4069 -4.8525  7.3213 -0.5259  16.6365   \n",
       "1   6.3806  12.5589  11.6887  25.3930  1.5776  6.8481  8.7348  16.4239   \n",
       "2   8.6758   8.1292  11.8932  10.6869 -0.6434  5.6510  9.3742  25.8831   \n",
       "3  31.3923   5.8555  12.6082   1.4182 -4.1185  6.2536  1.4257  13.5426   \n",
       "4   1.1174  13.4532  12.7925   4.3775 -0.1543  5.6794  0.8210  19.1358   \n",
       "\n",
       "    var_56  var_57  var_58   var_59   var_60   var_61  var_62  var_63  var_64  \\\n",
       "0  19.3036  6.4129 -5.3948   9.3269  11.9314  -3.5750 -0.7706  0.8705  6.9282   \n",
       "1  21.7056  6.9345  1.6678   9.5249   5.3383 -18.7083  1.3382 -1.7401  5.8398   \n",
       "2  19.8701  5.4834 -4.0304   8.5160   8.9776  -5.6619  2.8117  2.5996  9.0986   \n",
       "3  15.4090  6.8761  1.7476  10.0413  15.2857  -4.1378  0.7928  2.5301  8.1458   \n",
       "4  12.6589  6.4394  4.3425   8.7003  12.0586 -10.4753 -0.0337  5.6603  6.2529   \n",
       "\n",
       "   var_65  var_66   var_67  var_68  var_69   var_70  var_71  var_72   var_73  \\\n",
       "0  2.8914  5.9744  17.4851  5.0125 -1.4230  33.3401  0.8018 -4.7906  30.2708   \n",
       "1  3.1051  4.4307  16.0005  5.0306 -7.3365  12.2806  0.6992 -0.7772  21.5123   \n",
       "2  7.1167  4.9466  13.8268  5.0093  4.7782  19.2081  0.4340  0.8459  34.8598   \n",
       "3  2.5738  5.9876  13.0758  5.0087 -9.7824   8.9289  0.4205 -2.5463   2.9428   \n",
       "4  1.5238  4.5356  20.1344  5.0267 -1.8628  39.8219  1.0498 -0.9113  38.5076   \n",
       "\n",
       "    var_74   var_75   var_76   var_77  var_78   var_79   var_80   var_81  \\\n",
       "0  26.8339  21.7205   7.3075  14.0810  3.1192  17.4265   9.4883  16.9060   \n",
       "1   6.7803  18.1896   6.9388  22.1336  6.3755  13.1525   1.9772  14.0406   \n",
       "2  20.7048  16.4953  -9.7077  19.6357  7.6587  15.5744  16.1691  14.3299   \n",
       "3  10.7087  12.2008  12.5465  19.4201  5.5060  14.1586  17.5941  15.4375   \n",
       "4   2.2201   9.5235   8.1522  14.9224  6.1573  15.5221  11.8133  16.7661   \n",
       "\n",
       "    var_82   var_83   var_84   var_85  var_86   var_87   var_88  var_89  \\\n",
       "0  14.5117  10.0276  -0.9706  20.4588  4.7945  20.4160  13.1633  7.9307   \n",
       "1   6.6904   9.9732 -11.5679  20.4525  9.4951   9.6343   8.1252  2.6059   \n",
       "2   1.3360  -0.4412  -0.2830  14.9105 -3.9016  14.6881   7.3220 -5.1443   \n",
       "3 -13.2668  14.0885   4.0357  22.3119  1.8571  16.5210  10.8149  0.3256   \n",
       "4 -14.6524  -0.4469   0.0306  22.5276  6.9774   2.2563   3.5779  1.4268   \n",
       "\n",
       "    var_90  var_91   var_92   var_93   var_94  var_95   var_96   var_97  \\\n",
       "0  -7.6509  7.0834  15.2324  10.1416   5.9156 -0.5775   5.7600  30.3238   \n",
       "1 -17.4201  7.1848  15.3484  10.6522   5.9897  0.3392  10.3516  29.8204   \n",
       "2 -34.3488  7.0194  12.4785   9.6665  13.2595 -0.5624   5.6347   9.5853   \n",
       "3 -21.4797  6.9174   9.9483  10.3696  11.0362  0.1892  19.4321  40.3383   \n",
       "4   9.0680  7.0197  19.7765  10.0499  11.4803  0.2548  16.7029  45.5510   \n",
       "\n",
       "   var_98   ...     var_100  var_101  var_102  var_103  var_104  var_105  \\\n",
       "0  2.1251   ...     -9.2198  17.3089  30.9548   1.4918  12.8721   3.4902   \n",
       "1  1.9998   ...     -1.7257  15.4712  35.6020   1.6570  13.0783   2.7752   \n",
       "2  1.4515   ...     -3.5065  14.1663  28.0256   1.3935  10.8257   4.2954   \n",
       "3  1.4105   ...      1.7021   2.5363   3.8763   1.5173  13.4083   2.8965   \n",
       "4  1.5795   ...    -14.3858  17.8630  23.2274   1.4375  14.4838   4.3806   \n",
       "\n",
       "   var_106  var_107  var_108  var_109  var_110  var_111  var_112  var_113  \\\n",
       "0   8.2856  11.9794  14.0176  15.0763   3.7662   6.0426   4.4243  14.1799   \n",
       "1   6.4986   4.6835  13.7963  17.7261   1.7375   5.5689   3.6609   8.9725   \n",
       "2   8.2125  26.2595  14.0232  19.4604   8.6896   8.1036   1.2057   8.9156   \n",
       "3   7.0919  21.6304  14.2000  23.0368  10.3445   6.0369   5.0227  12.6600   \n",
       "4  10.6976  18.4023  14.2212  16.0638   6.3933   6.8699   2.7253  12.6458   \n",
       "\n",
       "   var_114  var_115  var_116  var_117  var_118  var_119  var_120  var_121  \\\n",
       "0   2.0921   1.5493   3.2206   0.0172  -6.6602   8.4785  42.0248  11.4164   \n",
       "1   4.1159   1.0693   2.0234   8.2760  -6.8610   0.2780  17.0488  11.6704   \n",
       "2   0.9777   2.3797   3.1638  37.8664  -3.3864  -2.4090  29.7978  12.2056   \n",
       "3   2.1278   4.0592   1.9084  11.6095   7.5397   8.1972  20.0844  10.4440   \n",
       "4   3.2376   3.4218  -0.5658  -5.6840   4.7753  10.3320  39.7127  11.2319   \n",
       "\n",
       "   var_122  var_123  var_124  var_125  var_126  var_127  var_128  var_129  \\\n",
       "0   0.4564   9.4006   0.9685  12.4929  14.1240   4.0388  -4.4442  16.6684   \n",
       "1   3.1215   8.5093   5.6367  12.0099  14.2372  -6.1600  -5.6690   8.9094   \n",
       "2   4.7688   7.9344   2.2102  12.6482  14.3377   2.3268   2.3930  13.7005   \n",
       "3   8.4676   5.0350   4.3103  12.0067  13.7149   1.6143  -1.2328  22.7248   \n",
       "4  -1.2978  12.4827   6.5034  12.7157  13.3054  -1.9678  -1.2363  11.5686   \n",
       "\n",
       "   var_130  var_131  var_132  var_133  var_134  var_135  var_136  var_137  \\\n",
       "0  12.5380   0.9205  10.5998   7.5147  -4.1748  -0.4824  10.5267  17.7547   \n",
       "1  11.0605   0.4583   9.7974   7.0891   2.6849   8.4970  15.7774   4.8775   \n",
       "2  12.7047   0.7507   7.7726   6.5950   0.2990  12.9154  29.9162   6.8031   \n",
       "3  12.6609   0.8039   4.7666   6.7888   5.8537  -4.5434  19.0111  12.6907   \n",
       "4  12.6428   0.4792   7.1984   7.1434  -0.2056 -16.3908  27.1589  23.5997   \n",
       "\n",
       "   var_138  var_139  var_140  var_141  var_142  var_143  var_144  var_145  \\\n",
       "0  -6.5226  -2.5502  -5.1547  -2.1246  19.8319  13.0752   9.2275   3.0213   \n",
       "1   3.6129   6.7530  11.1003  15.3593   2.2105   8.2280   9.0717  -5.0947   \n",
       "2  10.5031  -6.0452  -4.5298   1.3903   5.0469  12.9740   9.3878  -0.1113   \n",
       "3  -2.9322  12.7898  12.0466  13.1646   7.7063  11.6549   9.8274   1.8061   \n",
       "4  -4.6175  11.7989  12.5683  -3.6145  22.1069   9.5539   9.2721  -1.6214   \n",
       "\n",
       "   var_146  var_147  var_148  var_149  var_150  var_151  var_152  var_153  \\\n",
       "0  11.6793 -11.6827   4.1017   5.2954  18.7741   9.8892   7.5219  14.9745   \n",
       "1   8.7644  -2.2873   4.1240 -13.3006  18.7454   9.3783   1.5284  16.0407   \n",
       "2  11.6749  16.8588   4.2600  14.6476  14.4431  14.1649   9.4875  16.5769   \n",
       "3   8.6963   1.8057   3.8265 -16.3027  13.7106   9.7908   5.8497  15.4378   \n",
       "4  12.9327   6.8080   4.2135  22.1044  20.0502   6.9953   9.3823  20.5534   \n",
       "\n",
       "   var_154  var_155  var_156  var_157  var_158  var_159  var_160  var_161  \\\n",
       "0  18.9880   1.0842  11.9125  -4.5103  16.1361  11.0067   5.9232   5.4113   \n",
       "1   7.7732   1.4316  14.8679   3.3619  11.5799  14.2058  30.9641   5.6723   \n",
       "2   7.2638  -2.2008  12.5953   7.4487  23.1407  10.4597  39.3654   5.5228   \n",
       "3   5.0372  -8.7673  13.6035  -3.5002  13.9785  14.6118  19.7251   5.3882   \n",
       "4   3.4368 -15.2208  13.0974 -14.0888  11.7586  14.5259  22.8700   5.6688   \n",
       "\n",
       "   var_162  var_163  var_164  var_165  var_166  var_167  var_168  var_169  \\\n",
       "0   3.8302   5.7380  -8.6105  22.9530   2.5531  -0.2836   4.3416   5.1855   \n",
       "1   3.6873  13.0429 -10.6572  15.5134   3.2185   9.0535   7.0535   5.3924   \n",
       "2   3.3159   4.3324  -0.5382  13.3009   3.1243  -4.1731   1.2330   6.1513   \n",
       "3   3.6775   7.4753 -11.0780  24.8712   2.6415   2.2673   7.2788   5.6406   \n",
       "4   6.1159  13.2433 -11.9785  26.2040   3.2348  -5.5775   5.7036   6.1717   \n",
       "\n",
       "   var_170  var_171  var_172  var_173  var_174  var_175  var_176  var_177  \\\n",
       "0   4.2603   1.6779  29.0849   8.4685  18.1317  12.2818  -0.6912  10.2226   \n",
       "1  -0.7720  -8.1783  29.9227  -5.6274  10.5018   9.6083  -0.4935   8.1696   \n",
       "2  -0.0391   1.4950  16.8874  -2.9787  27.4035  15.8819 -10.9660  15.6415   \n",
       "3   7.2048   3.4504   2.4130  11.1674  14.5499  10.6151  -5.7922  13.9407   \n",
       "4  -1.6039  -2.4866  17.2728   2.3640  14.0037  12.9165 -12.0311  10.1161   \n",
       "\n",
       "   var_178  var_179  var_180  var_181  var_182  var_183  var_184  var_185  \\\n",
       "0  -5.5579   2.2926  -4.5358  10.3903 -15.4937   3.9697  31.3521  -1.1651   \n",
       "1  -4.3605   5.2110   0.4087  12.0030 -10.3812   5.8496  25.1958  -8.8468   \n",
       "2  -9.4056   4.4611  -3.0835   8.5549  -2.8517  13.4770  24.4721  -3.4824   \n",
       "3   7.1078   1.1019   9.4590   9.8243   5.9917   5.1634   8.1154   3.6638   \n",
       "4  -8.7562   6.0889  -1.3620  10.3559  -7.4915   9.4588   3.9829   5.8580   \n",
       "\n",
       "   var_186  var_187  var_188  var_189  var_190  var_191  var_192  var_193  \\\n",
       "0   9.2874 -23.5705  13.2643   1.6591  -2.1556  11.8495  -1.4300   2.4508   \n",
       "1  11.8263  -8.7112  15.9072   0.9812  10.6165   8.8349   0.9403  10.1282   \n",
       "2   4.9178  -2.0720  11.5390   1.1821  -0.7484  10.9935   1.9803   2.1800   \n",
       "3   3.3102 -19.7819  13.4499   1.3104   9.5702   9.0766   1.6580   3.5813   \n",
       "4   8.3635 -24.8254  11.4928   1.6321   4.2259   9.1723   1.2835   3.3778   \n",
       "\n",
       "   var_194  var_195  var_196  var_197  var_198  var_199  \n",
       "0  13.7112   2.4669   4.3654  10.7200  15.4722  -8.7197  \n",
       "1  15.5765   0.4773  -1.4852   9.8714  19.1293 -20.9760  \n",
       "2  12.9813   2.1281  -7.1086   7.0618  19.8956 -23.1794  \n",
       "3  15.1874   3.1656   3.9567   9.2295  13.0168  -4.2108  \n",
       "4  19.5542  -0.2860  -5.1612   7.2882  13.9260  -9.1846  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba5573cfaec6625aed13e98c6e034809e2997b5b"
   },
   "source": [
    "Distribution of target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "6f7cd5bdd625e69c75e10f586a826da80c814cdf"
   },
   "outputs": [],
   "source": [
    "target = 'target'\n",
    "predictors = train_df.columns.values.tolist()[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "cc8cef4f66aa77c4183cc4f99acd6082b6f036bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    179902\n",
       "1     20098\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ad51d7daad193e0dab467f82dfad4c7ce7876d56"
   },
   "source": [
    "The problem is unbalanced! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "496907f42901e10bb883858252b2783e30ff2e43"
   },
   "source": [
    "In this kernel I will be using **50% Stratified rows** as holdout rows for the validation-set to get optimal parameters. Later I will use 5 fold cross validation in the final model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "7ec87a3460c6358b9a134afea5bac561f7a84226"
   },
   "outputs": [],
   "source": [
    "bayesian_tr_index, bayesian_val_index  = list(StratifiedKFold(n_splits=2, shuffle=True, random_state=1).split(train_df, train_df.target.values))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cbcebd0aacaeb637a1e119971303c9fcd60f9ea5"
   },
   "source": [
    "These `bayesian_tr_index` and `bayesian_val_index` indexes will be used for the bayesian optimization as training and validation index of training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0f18184730f7261c3ddc2253ee025f9910ffedb6"
   },
   "source": [
    "<a id=\"2\"></a> <br>\n",
    "## 2. Black box function to be optimized (LightGBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4a5be20e9809494709c5da85861775c8720816ba"
   },
   "source": [
    "As data is loaded, let's create the black box function for LightGBM to find parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "196288ebb7caf614e230a0449b40354266efbc45"
   },
   "outputs": [],
   "source": [
    "def LGB_bayesian(\n",
    "    num_leaves,  # int\n",
    "    min_data_in_leaf,  # int\n",
    "    learning_rate,\n",
    "    min_sum_hessian_in_leaf,    # int  \n",
    "    feature_fraction,\n",
    "    lambda_l1,\n",
    "    lambda_l2,\n",
    "    min_gain_to_split,\n",
    "    max_depth):\n",
    "    \n",
    "    # LightGBM expects next three parameters need to be integer. So we make them integer\n",
    "    num_leaves = int(num_leaves)\n",
    "    min_data_in_leaf = int(min_data_in_leaf)\n",
    "    max_depth = int(max_depth)\n",
    "\n",
    "    assert type(num_leaves) == int\n",
    "    assert type(min_data_in_leaf) == int\n",
    "    assert type(max_depth) == int\n",
    "\n",
    "    param = {\n",
    "        'num_leaves': num_leaves,\n",
    "        'max_bin': 63,\n",
    "        'min_data_in_leaf': min_data_in_leaf,\n",
    "        'learning_rate': learning_rate,\n",
    "        'min_sum_hessian_in_leaf': min_sum_hessian_in_leaf,\n",
    "        'bagging_fraction': 1.0,\n",
    "        'bagging_freq': 5,\n",
    "        'feature_fraction': feature_fraction,\n",
    "        'lambda_l1': lambda_l1,\n",
    "        'lambda_l2': lambda_l2,\n",
    "        'min_gain_to_split': min_gain_to_split,\n",
    "        'max_depth': max_depth,\n",
    "        'save_binary': True, \n",
    "        'seed': 1337,\n",
    "        'feature_fraction_seed': 1337,\n",
    "        'bagging_seed': 1337,\n",
    "        'drop_seed': 1337,\n",
    "        'data_random_seed': 1337,\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': 1,\n",
    "        'metric': 'auc',\n",
    "        'is_unbalance': True,\n",
    "        'boost_from_average': False,   \n",
    "\n",
    "    }    \n",
    "    \n",
    "    \n",
    "    xg_train = lgb.Dataset(train_df.iloc[bayesian_tr_index][predictors].values,\n",
    "                           label=train_df.iloc[bayesian_tr_index][target].values,\n",
    "                           feature_name=predictors,\n",
    "                           free_raw_data = False\n",
    "                           )\n",
    "    xg_valid = lgb.Dataset(train_df.iloc[bayesian_val_index][predictors].values,\n",
    "                           label=train_df.iloc[bayesian_val_index][target].values,\n",
    "                           feature_name=predictors,\n",
    "                           free_raw_data = False\n",
    "                           )   \n",
    "\n",
    "    num_round = 5000\n",
    "    clf = lgb.train(param, xg_train, num_round, valid_sets = [xg_valid], verbose_eval=250, early_stopping_rounds = 50)\n",
    "    \n",
    "    predictions = clf.predict(train_df.iloc[bayesian_val_index][predictors].values, num_iteration=clf.best_iteration)   \n",
    "    \n",
    "    score = metrics.roc_auc_score(train_df.iloc[bayesian_val_index][target].values, predictions)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "40b3959e5f0672b6e030408cd197da137e1b66ee"
   },
   "source": [
    "The above `LGB_bayesian` function will act as black box function for Bayesian optimization. I already defined the the trainng and validation dataset for LightGBM inside the `LGB_bayesian` function. \n",
    "\n",
    "The `LGB_bayesian` function takes values for `num_leaves`, `min_data_in_leaf`, `learning_rate`, `min_sum_hessian_in_leaf`, `feature_fraction`, `lambda_l1`, `lambda_l2`, `min_gain_to_split`, `max_depth` from Bayesian optimization framework. Keep in mind that `num_leaves`, `min_data_in_leaf`, and `max_depth` should be integer for LightGBM. But Bayesian Optimization sends continous vales to function. So I force them to be integer. I am only going to find optimal parameter values of them. The reader may increase or decrease number of parameters to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8fb52d3d130c2477a6ab71b2d6797c787dce9f21"
   },
   "source": [
    "Now I need to give bounds for these parameters, so that Bayesian optimization only search inside the bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "044ed09d293f7a712af6fe6c00e935df19ca1cf4"
   },
   "outputs": [],
   "source": [
    "# Bounded region of parameter space\n",
    "bounds_LGB = {\n",
    "    'num_leaves': (5, 20), \n",
    "    'min_data_in_leaf': (5, 20),  \n",
    "    'learning_rate': (0.01, 0.3),\n",
    "    'min_sum_hessian_in_leaf': (0.00001, 0.01),    \n",
    "    'feature_fraction': (0.05, 0.5),\n",
    "    'lambda_l1': (0, 5.0), \n",
    "    'lambda_l2': (0, 5.0), \n",
    "    'min_gain_to_split': (0, 1.0),\n",
    "    'max_depth':(3,15),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1a2a217d1bcc8b27e67b9cc7f4b7bbb237b8ee72"
   },
   "source": [
    "Let's put all of them in BayesianOptimization object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "c0fc26566d5b5bf64a9f507cd06bf0ab85b584e3"
   },
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "af6565a9d43d2b5a153a3f2009afca7618a699c1"
   },
   "outputs": [],
   "source": [
    "LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "95f597ee9781e5621c98fe89d69a30601902c716"
   },
   "source": [
    "Now, let's the the key space (parameters) we are going to optimize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "281f53a7d9bca23329e965986939443bc63a927c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feature_fraction', 'lambda_l1', 'lambda_l2', 'learning_rate', 'max_depth', 'min_data_in_leaf', 'min_gain_to_split', 'min_sum_hessian_in_leaf', 'num_leaves']\n"
     ]
    }
   ],
   "source": [
    "print(LGB_BO.space.keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a2fbbcbe963f659df1884511af1a0009f28ef6d5"
   },
   "source": [
    "I have created the BayesianOptimization object (`LGB_BO`), it will not work until I call maximize. Before calling it, I want to explain two parameters of BayesianOptimization object (`LGB_BO`) which we can pass to maximize:\n",
    "- `init_points`: How many initial random runs of **random** exploration we want to perform. In our case `LGB_bayesian` will be called `n_iter` times.\n",
    "- `n_iter`: How many runs of bayesian optimization we want to perform after number of `init_points` runs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e06903e320665c4f77e9b029681cf6f651dfc9b4"
   },
   "source": [
    "Now, it's time to call the function from Bayesian optimization framework to maximize. I allow `LGB_BO` object to run for 5 `init_points` (exploration) and 5 `n_iter` (exploitation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "6c12e5f092c2d0f690fd720e40aec69268c9e70f"
   },
   "outputs": [],
   "source": [
    "init_points = 5\n",
    "n_iter = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "f5f6d3f1a7752d818330b92980344956bf934e81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "|   iter    |  target   | featur... | lambda_l1 | lambda_l2 | learni... | max_depth | min_da... | min_ga... | min_su... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[178]\tvalid_0's auc: 0.878546\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8785  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 1.188   \u001b[0m | \u001b[0m 4.121   \u001b[0m | \u001b[0m 0.2901  \u001b[0m | \u001b[0m 14.67   \u001b[0m | \u001b[0m 11.8    \u001b[0m | \u001b[0m 0.609   \u001b[0m | \u001b[0m 0.007758\u001b[0m | \u001b[0m 14.62   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.84073\n",
      "[500]\tvalid_0's auc: 0.866356\n",
      "[750]\tvalid_0's auc: 0.877017\n",
      "[1000]\tvalid_0's auc: 0.882954\n",
      "[1250]\tvalid_0's auc: 0.886643\n",
      "[1500]\tvalid_0's auc: 0.888876\n",
      "[1750]\tvalid_0's auc: 0.890244\n",
      "[2000]\tvalid_0's auc: 0.891098\n",
      "[2250]\tvalid_0's auc: 0.891491\n",
      "Early stopping, best iteration is:\n",
      "[2387]\tvalid_0's auc: 0.891578\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.8916  \u001b[0m | \u001b[95m 0.3749  \u001b[0m | \u001b[95m 0.1752  \u001b[0m | \u001b[95m 1.492   \u001b[0m | \u001b[95m 0.02697 \u001b[0m | \u001b[95m 13.28   \u001b[0m | \u001b[95m 10.59   \u001b[0m | \u001b[95m 0.6798  \u001b[0m | \u001b[95m 0.00257 \u001b[0m | \u001b[95m 10.21   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.879455\n",
      "[500]\tvalid_0's auc: 0.889134\n",
      "[750]\tvalid_0's auc: 0.892162\n",
      "Early stopping, best iteration is:\n",
      "[802]\tvalid_0's auc: 0.89247\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.8925  \u001b[0m | \u001b[95m 0.05424 \u001b[0m | \u001b[95m 1.792   \u001b[0m | \u001b[95m 4.745   \u001b[0m | \u001b[95m 0.07319 \u001b[0m | \u001b[95m 6.833   \u001b[0m | \u001b[95m 18.77   \u001b[0m | \u001b[95m 0.0319  \u001b[0m | \u001b[95m 0.000660\u001b[0m | \u001b[95m 14.45   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.879482\n",
      "[500]\tvalid_0's auc: 0.884038\n",
      "Early stopping, best iteration is:\n",
      "[555]\tvalid_0's auc: 0.884316\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.8843  \u001b[0m | \u001b[0m 0.4432  \u001b[0m | \u001b[0m 0.04358 \u001b[0m | \u001b[0m 3.733   \u001b[0m | \u001b[0m 0.2457  \u001b[0m | \u001b[0m 3.909   \u001b[0m | \u001b[0m 14.85   \u001b[0m | \u001b[0m 0.5093  \u001b[0m | \u001b[0m 0.004804\u001b[0m | \u001b[0m 19.33   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.881955\n",
      "[500]\tvalid_0's auc: 0.890201\n",
      "[750]\tvalid_0's auc: 0.89183\n",
      "Early stopping, best iteration is:\n",
      "[794]\tvalid_0's auc: 0.891892\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.8919  \u001b[0m | \u001b[0m 0.05001 \u001b[0m | \u001b[0m 1.235   \u001b[0m | \u001b[0m 3.561   \u001b[0m | \u001b[0m 0.1041  \u001b[0m | \u001b[0m 6.324   \u001b[0m | \u001b[0m 15.43   \u001b[0m | \u001b[0m 0.9186  \u001b[0m | \u001b[0m 0.002452\u001b[0m | \u001b[0m 11.87   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.882933\n",
      "Early stopping, best iteration is:\n",
      "[404]\tvalid_0's auc: 0.887774\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.8878  \u001b[0m | \u001b[0m 0.2888  \u001b[0m | \u001b[0m 0.06764 \u001b[0m | \u001b[0m 0.06368 \u001b[0m | \u001b[0m 0.2366  \u001b[0m | \u001b[0m 8.822   \u001b[0m | \u001b[0m 19.91   \u001b[0m | \u001b[0m 0.7963  \u001b[0m | \u001b[0m 0.009826\u001b[0m | \u001b[0m 5.699   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.765996\n",
      "[500]\tvalid_0's auc: 0.804867\n",
      "[750]\tvalid_0's auc: 0.823897\n",
      "[1000]\tvalid_0's auc: 0.836169\n",
      "[1250]\tvalid_0's auc: 0.845239\n",
      "[1500]\tvalid_0's auc: 0.852387\n",
      "[1750]\tvalid_0's auc: 0.858029\n",
      "[2000]\tvalid_0's auc: 0.862335\n",
      "[2250]\tvalid_0's auc: 0.866039\n",
      "[2500]\tvalid_0's auc: 0.869036\n",
      "[2750]\tvalid_0's auc: 0.871479\n",
      "[3000]\tvalid_0's auc: 0.873636\n",
      "[3250]\tvalid_0's auc: 0.875542\n",
      "[3500]\tvalid_0's auc: 0.877235\n",
      "[3750]\tvalid_0's auc: 0.878804\n",
      "[4000]\tvalid_0's auc: 0.880166\n",
      "[4250]\tvalid_0's auc: 0.88121\n",
      "[4500]\tvalid_0's auc: 0.882269\n",
      "[4750]\tvalid_0's auc: 0.883133\n",
      "[5000]\tvalid_0's auc: 0.883885\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's auc: 0.883885\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.8839  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 1.265e-0\u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 3.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1e-05   \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.870052\n",
      "[500]\tvalid_0's auc: 0.885015\n",
      "[750]\tvalid_0's auc: 0.88999\n",
      "[1000]\tvalid_0's auc: 0.89117\n",
      "Early stopping, best iteration is:\n",
      "[1077]\tvalid_0's auc: 0.891403\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.8914  \u001b[0m | \u001b[0m 0.2457  \u001b[0m | \u001b[0m 0.4681  \u001b[0m | \u001b[0m 4.634   \u001b[0m | \u001b[0m 0.1089  \u001b[0m | \u001b[0m 5.749   \u001b[0m | \u001b[0m 5.199   \u001b[0m | \u001b[0m 0.03918 \u001b[0m | \u001b[0m 0.007056\u001b[0m | \u001b[0m 5.492   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.765951\n",
      "[500]\tvalid_0's auc: 0.804874\n",
      "[750]\tvalid_0's auc: 0.825005\n",
      "[1000]\tvalid_0's auc: 0.837705\n",
      "[1250]\tvalid_0's auc: 0.846748\n",
      "[1500]\tvalid_0's auc: 0.853921\n",
      "[1750]\tvalid_0's auc: 0.859675\n",
      "[2000]\tvalid_0's auc: 0.863974\n",
      "[2250]\tvalid_0's auc: 0.867603\n",
      "[2500]\tvalid_0's auc: 0.870682\n",
      "[2750]\tvalid_0's auc: 0.873071\n",
      "[3000]\tvalid_0's auc: 0.875263\n",
      "[3250]\tvalid_0's auc: 0.877154\n",
      "[3500]\tvalid_0's auc: 0.878814\n",
      "[3750]\tvalid_0's auc: 0.880322\n",
      "[4000]\tvalid_0's auc: 0.881642\n",
      "[4250]\tvalid_0's auc: 0.882795\n",
      "[4500]\tvalid_0's auc: 0.883854\n",
      "[4750]\tvalid_0's auc: 0.884847\n",
      "[5000]\tvalid_0's auc: 0.885672\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's auc: 0.885672\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.8857  \u001b[0m | \u001b[0m 0.312   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 9.085e-0\u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 15.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.866218\n",
      "[500]\tvalid_0's auc: 0.881947\n",
      "[750]\tvalid_0's auc: 0.887384\n",
      "[1000]\tvalid_0's auc: 0.889271\n",
      "Early stopping, best iteration is:\n",
      "[1164]\tvalid_0's auc: 0.889639\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8896  \u001b[0m | \u001b[0m 0.4663  \u001b[0m | \u001b[0m 4.555   \u001b[0m | \u001b[0m 0.07814 \u001b[0m | \u001b[0m 0.1009  \u001b[0m | \u001b[0m 3.392   \u001b[0m | \u001b[0m 10.03   \u001b[0m | \u001b[0m 0.01464 \u001b[0m | \u001b[0m 0.008685\u001b[0m | \u001b[0m 7.056   \u001b[0m |\n",
      "=====================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print('-' * 130)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bdea40277b15b11b5b92238e519cf5fc2576f64f"
   },
   "source": [
    "As the optimization is done, let's see what is the maximum value we have got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "d0d5a3fe41a7d9f1625cdfb5eceb113f6cabcf93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8924701144136037"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGB_BO.max['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ad0a200a70e00955a924ca5f3fc622b492eaf3e3"
   },
   "source": [
    "The validation AUC for parameters is 0.89 ! Let's see parameters is responsible for this score :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "ced952bfd9397a68e3a2d394a4f7e075b40a9aff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_fraction': 0.05423574653643624,\n",
       " 'lambda_l1': 1.7916689135248487,\n",
       " 'lambda_l2': 4.745470908391052,\n",
       " 'learning_rate': 0.07319071264818977,\n",
       " 'max_depth': 6.8326963965643746,\n",
       " 'min_data_in_leaf': 18.76658579000881,\n",
       " 'min_gain_to_split': 0.03190366643989473,\n",
       " 'min_sum_hessian_in_leaf': 0.0006601945250547198,\n",
       " 'num_leaves': 14.447434986617345}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGB_BO.max['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7f6bc1d0859b643bdbbeba1e0e3c8b336860c023"
   },
   "source": [
    "Now we can use these parameters to our final model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "06096bd955639e891320f5b31408fad1785f5949"
   },
   "source": [
    "Wait, I want to show one more cool option from BayesianOptimization library. You can probe the `LGB_bayesian` function, if you have an idea of the optimal parameters or it you get **parameters from other kernel** like mine [mine](https://www.kaggle.com/fayzur/customer-transaction-prediction). I will copy and paste parameters from my other kernel here. You can probe as folowing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "a0275beb85200e0c8cce95b477dd756e8d1ef2ef"
   },
   "outputs": [],
   "source": [
    "# parameters from version 2 of\n",
    "#https://www.kaggle.com/fayzur/customer-transaction-prediction?scriptVersionId=10522231\n",
    "\n",
    "LGB_BO.probe(\n",
    "    params={'feature_fraction': 0.1403, \n",
    "            'lambda_l1': 4.218, \n",
    "            'lambda_l2': 1.734, \n",
    "            'learning_rate': 0.07, \n",
    "            'max_depth': 14, \n",
    "            'min_data_in_leaf': 17, \n",
    "            'min_gain_to_split': 0.1501, \n",
    "            'min_sum_hessian_in_leaf': 0.000446, \n",
    "            'num_leaves': 6},\n",
    "    lazy=True, # \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c2893bc7de2bf69fd02ef26e90fc354f71b83e10"
   },
   "source": [
    "OK, by default these will be explored lazily (lazy=True), meaning these points will be evaluated only the next time you call maximize. Let's do a maximize call of `LGB_BO` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "8e3feb820520bb03000225416e338610e2d90b1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | featur... | lambda_l1 | lambda_l2 | learni... | max_depth | min_da... | min_ga... | min_su... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.86243\n",
      "[500]\tvalid_0's auc: 0.880738\n",
      "[750]\tvalid_0's auc: 0.887841\n",
      "[1000]\tvalid_0's auc: 0.890905\n",
      "[1250]\tvalid_0's auc: 0.892214\n",
      "[1500]\tvalid_0's auc: 0.892655\n",
      "Early stopping, best iteration is:\n",
      "[1466]\tvalid_0's auc: 0.892672\n",
      "| \u001b[95m 11      \u001b[0m | \u001b[95m 0.8927  \u001b[0m | \u001b[95m 0.1403  \u001b[0m | \u001b[95m 4.218   \u001b[0m | \u001b[95m 1.734   \u001b[0m | \u001b[95m 0.07    \u001b[0m | \u001b[95m 14.0    \u001b[0m | \u001b[95m 17.0    \u001b[0m | \u001b[95m 0.1501  \u001b[0m | \u001b[95m 0.000446\u001b[0m | \u001b[95m 6.0     \u001b[0m |\n",
      "=====================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "LGB_BO.maximize(init_points=0, n_iter=0) # remember no init_points or n_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9bd70047d5313694681d73ed049339d6b00261c0"
   },
   "source": [
    "Finally, the list of all parameters probed and their corresponding target values is available via the property LGB_BO.res."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "416bfb9650849e75106ee63b40dfd7519aa2ee29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: \n",
      "\t{'target': 0.8785456504868869, 'params': {'feature_fraction': 0.3999660847582191, 'lambda_l1': 1.1877061001745615, 'lambda_l2': 4.1213926633068425, 'learning_rate': 0.2900672674324699, 'max_depth': 14.67121336685872, 'min_data_in_leaf': 11.801738711259683, 'min_gain_to_split': 0.6090424627612779, 'min_sum_hessian_in_leaf': 0.007757509880902418, 'num_leaves': 14.624200171386038}}\n",
      "Iteration 1: \n",
      "\t{'target': 0.8915776403640969, 'params': {'feature_fraction': 0.3749082032826262, 'lambda_l1': 0.17518262050718658, 'lambda_l2': 1.492247354445897, 'learning_rate': 0.026968622645801674, 'max_depth': 13.284731311046386, 'min_data_in_leaf': 10.592810418122113, 'min_gain_to_split': 0.679847951578097, 'min_sum_hessian_in_leaf': 0.002570236693773035, 'num_leaves': 10.21371822728738}}\n",
      "Iteration 2: \n",
      "\t{'target': 0.8924701144136037, 'params': {'feature_fraction': 0.05423574653643624, 'lambda_l1': 1.7916689135248487, 'lambda_l2': 4.745470908391052, 'learning_rate': 0.07319071264818977, 'max_depth': 6.8326963965643746, 'min_data_in_leaf': 18.76658579000881, 'min_gain_to_split': 0.03190366643989473, 'min_sum_hessian_in_leaf': 0.0006601945250547198, 'num_leaves': 14.447434986617345}}\n",
      "Iteration 3: \n",
      "\t{'target': 0.8843155956741141, 'params': {'feature_fraction': 0.4432160494757924, 'lambda_l1': 0.04357866151892431, 'lambda_l2': 3.7328861849696877, 'learning_rate': 0.24572393959076078, 'max_depth': 3.9086093540672007, 'min_data_in_leaf': 14.846830018954368, 'min_gain_to_split': 0.5092622000835182, 'min_sum_hessian_in_leaf': 0.004804035081048867, 'num_leaves': 19.333612173965996}}\n",
      "Iteration 4: \n",
      "\t{'target': 0.8918922376241952, 'params': {'feature_fraction': 0.0500054151062683, 'lambda_l1': 1.2348935049595817, 'lambda_l2': 3.5611633895579176, 'learning_rate': 0.1041287944381468, 'max_depth': 6.323956276605713, 'min_data_in_leaf': 15.431681788302118, 'min_gain_to_split': 0.9185517481459488, 'min_sum_hessian_in_leaf': 0.0024523122649579236, 'num_leaves': 11.871287259151455}}\n",
      "Iteration 5: \n",
      "\t{'target': 0.8877741089318031, 'params': {'feature_fraction': 0.28880235499863005, 'lambda_l1': 0.0676352739721825, 'lambda_l2': 0.06367783660314708, 'learning_rate': 0.23658833958998987, 'max_depth': 8.82154329429286, 'min_data_in_leaf': 19.911222977094404, 'min_gain_to_split': 0.7962518122529608, 'min_sum_hessian_in_leaf': 0.00982563254531773, 'num_leaves': 5.699237677594455}}\n",
      "Iteration 6: \n",
      "\t{'target': 0.8838845198764629, 'params': {'feature_fraction': 0.5, 'lambda_l1': 5.0, 'lambda_l2': 1.2648294958898453e-09, 'learning_rate': 0.010000000280869852, 'max_depth': 3.0, 'min_data_in_leaf': 5.000000002619426, 'min_gain_to_split': 0.0, 'min_sum_hessian_in_leaf': 1.0000022537751995e-05, 'num_leaves': 20.0}}\n",
      "Iteration 7: \n",
      "\t{'target': 0.8914029939138292, 'params': {'feature_fraction': 0.24571681728644235, 'lambda_l1': 0.46805485709572603, 'lambda_l2': 4.634252859543099, 'learning_rate': 0.10893673688261334, 'max_depth': 5.74869199423548, 'min_data_in_leaf': 5.199199561225826, 'min_gain_to_split': 0.03917651107600029, 'min_sum_hessian_in_leaf': 0.007056042775055096, 'num_leaves': 5.49237053333988}}\n",
      "Iteration 8: \n",
      "\t{'target': 0.885671663972105, 'params': {'feature_fraction': 0.3120275776133379, 'lambda_l1': 5.0, 'lambda_l2': 9.084821479118918e-09, 'learning_rate': 0.01, 'max_depth': 14.999999991290764, 'min_data_in_leaf': 5.0, 'min_gain_to_split': 1.0, 'min_sum_hessian_in_leaf': 0.009999999929977302, 'num_leaves': 5.0}}\n",
      "Iteration 9: \n",
      "\t{'target': 0.8896394504207458, 'params': {'feature_fraction': 0.46632352517727926, 'lambda_l1': 4.554786486122389, 'lambda_l2': 0.07813817250912569, 'learning_rate': 0.1009416028187477, 'max_depth': 3.3923822872270604, 'min_data_in_leaf': 10.028922050131339, 'min_gain_to_split': 0.01464358752550321, 'min_sum_hessian_in_leaf': 0.008685058946939706, 'num_leaves': 7.056018802118709}}\n",
      "Iteration 10: \n",
      "\t{'target': 0.8926723153666576, 'params': {'feature_fraction': 0.1403, 'lambda_l1': 4.218, 'lambda_l2': 1.734, 'learning_rate': 0.07, 'max_depth': 14.0, 'min_data_in_leaf': 17.0, 'min_gain_to_split': 0.1501, 'min_sum_hessian_in_leaf': 0.000446, 'num_leaves': 6.0}}\n"
     ]
    }
   ],
   "source": [
    "for i, res in enumerate(LGB_BO.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "32a1bd2e328c121915b77177e72f281e6b64ca4c"
   },
   "source": [
    "We have got a better validation score in the probe! As previously I ran `LGB_BO` only for 10 runs. In practice I increase it to arround 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "5e881cbd9dc198f4cc91bb65823542ec473c6b39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8926723153666576"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGB_BO.max['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "1a132698b07c7652d62c329f5d6508211540834f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_fraction': 0.1403,\n",
       " 'lambda_l1': 4.218,\n",
       " 'lambda_l2': 1.734,\n",
       " 'learning_rate': 0.07,\n",
       " 'max_depth': 14.0,\n",
       " 'min_data_in_leaf': 17.0,\n",
       " 'min_gain_to_split': 0.1501,\n",
       " 'min_sum_hessian_in_leaf': 0.000446,\n",
       " 'num_leaves': 6.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGB_BO.max['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e17fc50406f38145fffae3c4aa585f5520433cab"
   },
   "source": [
    "Let's build a model together use therse parameters ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b91db8b5c98da4f014f1863ba7de18e241f517c6"
   },
   "source": [
    "<a id=\"3\"></a> <br>\n",
    "## 3. Training LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "56d006d366fc5c2686249887b5d1f302d4a708f5"
   },
   "outputs": [],
   "source": [
    "param_lgb = {\n",
    "        'num_leaves': int(LGB_BO.max['params']['num_leaves']), # remember to int here\n",
    "        'max_bin': 63,\n",
    "        'min_data_in_leaf': int(LGB_BO.max['params']['min_data_in_leaf']), # remember to int here\n",
    "        'learning_rate': LGB_BO.max['params']['learning_rate'],\n",
    "        'min_sum_hessian_in_leaf': LGB_BO.max['params']['min_sum_hessian_in_leaf'],\n",
    "        'bagging_fraction': 1.0, \n",
    "        'bagging_freq': 5, \n",
    "        'feature_fraction': LGB_BO.max['params']['feature_fraction'],\n",
    "        'lambda_l1': LGB_BO.max['params']['lambda_l1'],\n",
    "        'lambda_l2': LGB_BO.max['params']['lambda_l2'],\n",
    "        'min_gain_to_split': LGB_BO.max['params']['min_gain_to_split'],\n",
    "        'max_depth': int(LGB_BO.max['params']['max_depth']), # remember to int here\n",
    "        'save_binary': True,\n",
    "        'seed': 1337,\n",
    "        'feature_fraction_seed': 1337,\n",
    "        'bagging_seed': 1337,\n",
    "        'drop_seed': 1337,\n",
    "        'data_random_seed': 1337,\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': 1,\n",
    "        'metric': 'auc',\n",
    "        'is_unbalance': True,\n",
    "        'boost_from_average': False,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d5a87047368fe7504c895aab4e9deb1b43746d7d"
   },
   "source": [
    "As you see, I assined `LGB_BO`'s optimal parameters to the `param_lgb` dictionary and they will be used to train a model with 5 fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7ab8ef627687c7602b81a9ac83f90ff3a2094d0c"
   },
   "source": [
    "Number of Kfolds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "9f306e20ae748715da17a2f15702ea1aa4d81497"
   },
   "outputs": [],
   "source": [
    "nfold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "f2f01a6006dcb34ffb53ca33a055e592ee9e75e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "0e9f0fd37207aeaa33f3d758ed22a32b462fc93d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "0e9f0fd37207aeaa33f3d758ed22a32b462fc93d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fold 1\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.865007\n",
      "[500]\tvalid_0's auc: 0.883335\n",
      "[750]\tvalid_0's auc: 0.890893\n",
      "[1000]\tvalid_0's auc: 0.894616\n",
      "[1250]\tvalid_0's auc: 0.896093\n",
      "[1500]\tvalid_0's auc: 0.896619\n",
      "Early stopping, best iteration is:\n",
      "[1603]\tvalid_0's auc: 0.89686\n",
      "\n",
      "fold 2\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.860373\n",
      "[500]\tvalid_0's auc: 0.879811\n",
      "[750]\tvalid_0's auc: 0.88715\n",
      "[1000]\tvalid_0's auc: 0.891324\n",
      "[1250]\tvalid_0's auc: 0.893482\n",
      "[1500]\tvalid_0's auc: 0.894638\n",
      "Early stopping, best iteration is:\n",
      "[1545]\tvalid_0's auc: 0.894804\n",
      "\n",
      "fold 3\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.86603\n",
      "[500]\tvalid_0's auc: 0.884838\n",
      "[750]\tvalid_0's auc: 0.892292\n",
      "[1000]\tvalid_0's auc: 0.895874\n",
      "[1250]\tvalid_0's auc: 0.897865\n",
      "Early stopping, best iteration is:\n",
      "[1310]\tvalid_0's auc: 0.898187\n",
      "\n",
      "fold 4\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.858726\n",
      "[500]\tvalid_0's auc: 0.879387\n",
      "[750]\tvalid_0's auc: 0.886873\n",
      "[1000]\tvalid_0's auc: 0.890344\n",
      "[1250]\tvalid_0's auc: 0.891781\n",
      "[1500]\tvalid_0's auc: 0.892681\n",
      "Early stopping, best iteration is:\n",
      "[1662]\tvalid_0's auc: 0.893023\n",
      "\n",
      "fold 5\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.861535\n",
      "[500]\tvalid_0's auc: 0.880695\n",
      "[750]\tvalid_0's auc: 0.888037\n",
      "[1000]\tvalid_0's auc: 0.891692\n",
      "[1250]\tvalid_0's auc: 0.89359\n",
      "[1500]\tvalid_0's auc: 0.89438\n",
      "Early stopping, best iteration is:\n",
      "[1600]\tvalid_0's auc: 0.894584\n",
      "\n",
      "\n",
      "CV AUC: 0.90\n"
     ]
    }
   ],
   "source": [
    "oof = np.zeros(len(train_df))\n",
    "predictions = np.zeros((len(test_df),nfold))\n",
    "\n",
    "i = 1\n",
    "for train_index, valid_index in skf.split(train_df, train_df.target.values):\n",
    "    print(\"\\nfold {}\".format(i))\n",
    "    xg_train = lgb.Dataset(train_df.iloc[train_index][predictors].values,\n",
    "                           label=train_df.iloc[train_index][target].values,\n",
    "                           feature_name=predictors,\n",
    "                           free_raw_data = False\n",
    "                           )\n",
    "    xg_valid = lgb.Dataset(train_df.iloc[valid_index][predictors].values,\n",
    "                           label=train_df.iloc[valid_index][target].values,\n",
    "                           feature_name=predictors,\n",
    "                           free_raw_data = False\n",
    "                           )   \n",
    "\n",
    "    \n",
    "    clf = lgb.train(param_lgb, xg_train, 5000, valid_sets = [xg_valid], verbose_eval=250, early_stopping_rounds = 50)\n",
    "    oof[valid_index] = clf.predict(train_df.iloc[valid_index][predictors].values, num_iteration=clf.best_iteration) \n",
    "    \n",
    "    predictions[:,i-1] += clf.predict(test_df[predictors], num_iteration=clf.best_iteration)\n",
    "    i = i + 1\n",
    "\n",
    "print(\"\\n\\nCV AUC: {:<0.2f}\".format(metrics.roc_auc_score(train_df.target.values, oof)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e6c8fde3d71858028688bd9a62bf1a4430b6bbb1"
   },
   "source": [
    "So we got 0.90 AUC in 5 fold cross validation. And 5 fold prediction look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "a889d2c1f63dbe989b3f0b0373ce9e1c80e3ee10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41628346, 0.39960379, 0.38409074, 0.35182696, 0.40993432],\n",
       "       [0.63407781, 0.6236449 , 0.61534627, 0.66678195, 0.66385472],\n",
       "       [0.65491558, 0.7424437 , 0.63334601, 0.60578336, 0.70685663],\n",
       "       ...,\n",
       "       [0.05168246, 0.0319463 , 0.04241488, 0.03894831, 0.03609798],\n",
       "       [0.41015456, 0.48430607, 0.53886698, 0.48510382, 0.49223725],\n",
       "       [0.40877008, 0.39991497, 0.44468547, 0.48166165, 0.41461139]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "01339e95305a72802fd23a2c740cb7cc988d1c27"
   },
   "source": [
    "If you are still reading, bare with me. I will not take much of your time. :D We are almost done. Let's do a rank averaging on 5 fold predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "116f2c0c3c43095d456a88d50425de0a3e7fdd11"
   },
   "source": [
    "<a id=\"4\"></a> <br>\n",
    "## 4. Rank averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "6ec40259b4636edd2c336583c5eb5c62feaaaa31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank averaging on 5 fold predictions\n"
     ]
    }
   ],
   "source": [
    "print(\"Rank averaging on\", nfold, \"fold predictions\")\n",
    "rank_predictions = np.zeros((predictions.shape[0],1))\n",
    "for i in range(nfold):\n",
    "    rank_predictions[:, 0] = np.add(rank_predictions[:, 0], rankdata(predictions[:, i].reshape(-1,1))/rank_predictions.shape[0]) \n",
    "\n",
    "rank_predictions /= nfold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "74eea39312bae04f6542a2f557c0454eda19d2f3"
   },
   "source": [
    "Let's submit prediction to Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "08a4c881ae24d784e9ee3c197d1188e26fdb40c7"
   },
   "source": [
    "<a id=\"5\"></a> <br>\n",
    "## 5. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "1fe8a68970387c8e57d373b4d0944731a32ccd51"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>0.693134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>0.875524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>0.889322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>0.881537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>0.539898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test_5</td>\n",
       "      <td>0.012080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>test_6</td>\n",
       "      <td>0.120680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>test_7</td>\n",
       "      <td>0.849158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>test_8</td>\n",
       "      <td>0.032887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>test_9</td>\n",
       "      <td>0.125836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_code    target\n",
       "0  test_0  0.693134\n",
       "1  test_1  0.875524\n",
       "2  test_2  0.889322\n",
       "3  test_3  0.881537\n",
       "4  test_4  0.539898\n",
       "5  test_5  0.012080\n",
       "6  test_6  0.120680\n",
       "7  test_7  0.849158\n",
       "8  test_8  0.032887\n",
       "9  test_9  0.125836"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\n",
    "sub_df[\"target\"] = rank_predictions\n",
    "sub_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "65cd74398d3f80186d65a2ff3431403a6846e229"
   },
   "outputs": [],
   "source": [
    "sub_df.to_csv(\"Customer_Transaction_rank_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba5573cfaec6625aed13e98c6e034809e2997b5b"
   },
   "source": [
    "Do not forget to upvote :) Also fork and modify for your own use. ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "497ff211885b3ab9864adf2d2938d2d06050974d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
