{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "93d7b4b8f5f6e5289cfc0312d650744e64905bc7"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# The purpose of this notebook\n",
    "\n",
    "**UPDATE 1:** *In version 5 of this notebook, I demonstrated that the model is capable of reaching the LB score of 0.896. Now, I would like to see if the augmentation idea from [this kernel](https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment) would help us to reach an even better score.*\n",
    "\n",
    "**UPDATE 2:** *Version 10 of this notebook shows that the augmentation idea does not work very well for the logistic regression -- the CV score clearly went down to 0.892. Good to know -- no more digging in this direction.* \n",
    "\n",
    "I have run across [this nice script](https://www.kaggle.com/ymatioun/santander-linear-model-with-additional-features) by Youri Matiounine in which a number of new features are added and linear regression is performed on the resulting data set. I was surprised by the high performance of this simple model: the LB score is about 0.894 which is close to what you can get using the heavy artillery like LighGBM. At the same time, I felt like there is a room for improvement -- after all, this is a classification rather than a regression problem, so I was wondering what will happen if we perform a logistic regression on Matiounine's data set. This notebook is my humble attempt to answer this question. \n",
    "\n",
    "Matiounine's features can be used in other models as well. To avoid the necessety of re-computing them every time when we switch from one model to another, I show how to store the processed data in [feather files](https://pypi.org/project/feather-format/), so that next time they can be loaded very fast into memory. This is much faster and safer than using CSV format.\n",
    "\n",
    "# Computing the new features\n",
    "\n",
    "Importing libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "319c9748ad2d9b82cc875000f58afa2129aeb9c3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "import feather\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, rankdata\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "31a0c430046df842333652c410b3181d800f0551"
   },
   "source": [
    "Now, let's read the CSV files containing the training and testing data and measure how long it takes.\n",
    "\n",
    "Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "0d080b4a0bf27808a316196c71948a96280ef177"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train data...\n",
      "It takes 9.57 seconds to read 'train.csv'.\n"
     ]
    }
   ],
   "source": [
    "path_train = '../input/train.feather'\n",
    "path_test = '../input/test.feather'\n",
    "\n",
    "print(\"Reading train data...\")\n",
    "start = time.time()\n",
    "train = pd.read_csv('../input/train.csv')\n",
    "end = time.time()\n",
    "\n",
    "print(\"It takes {0:.2f} seconds to read 'train.csv'.\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1e6904f34859901e764adde45ed0bb3bc13e4f58"
   },
   "source": [
    "Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "0fca1a0b7f595147cc5c3641b1a45c9d7f8e2340"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading test data...\n",
      "It takes 8.45 seconds to read 'test.csv'.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"Reading test data...\")\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "end = time.time()\n",
    "\n",
    "print(\"It takes {0:.2f} seconds to read 'test.csv'.\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9c74d587203855a0a8eb7da6b2f6abb3090bb60d"
   },
   "source": [
    "Saving the 'target' and 'ID_code' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "74a87959eb66d371c314180f4877d1afdde136b7"
   },
   "outputs": [],
   "source": [
    "target = train.pop('target')\n",
    "train_ids = train.pop('ID_code')\n",
    "test_ids = test.pop('ID_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8c2c537288b4915a1f860065a2046e47cae19459"
   },
   "source": [
    "Saving the number of rows in 'train' for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "b1026519541d70d9206f9941fc29d19005fa1dcd"
   },
   "outputs": [],
   "source": [
    "len_train = len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "af2947142503c41f3c26e9c805e14e033fceb955"
   },
   "source": [
    "Merging test and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "fc7bb057b85c4a8b12b102e7432e261ff6a92954"
   },
   "outputs": [],
   "source": [
    "merged = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5b29b8bd47b43d76ee650e12e063c34c3c1ad189"
   },
   "source": [
    "Removing data we no longer need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "bca8a00d9d62f3a4479c524b66d6e906ac155b7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del test, train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ef8301089d9bfd8880ad0165e3d1c248a5fb1fde"
   },
   "source": [
    "Saving the list of original features in a new list `original_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "134f8d281a4fafdbbbd51fb3429015d271d895ac"
   },
   "outputs": [],
   "source": [
    "original_features = merged.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8787d83673d27fe9529524257c660933af610ab2"
   },
   "source": [
    "Adding more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "06df646dee338e944955dd6059df57cd6c73afa0"
   },
   "outputs": [],
   "source": [
    "for col in merged.columns:\n",
    "    # Normalize the data, so that it can be used in norm.cdf(), \n",
    "    # as though it is a standard normal variable\n",
    "    merged[col] = ((merged[col] - merged[col].mean()) \n",
    "    / merged[col].std()).astype('float32')\n",
    "\n",
    "    # Square\n",
    "    merged[col+'^2'] = merged[col] * merged[col]\n",
    "\n",
    "    # Cube\n",
    "    merged[col+'^3'] = merged[col] * merged[col] * merged[col]\n",
    "\n",
    "    # 4th power\n",
    "    merged[col+'^4'] = merged[col] * merged[col] * merged[col] * merged[col]\n",
    "\n",
    "    # Cumulative percentile (not normalized)\n",
    "    merged[col+'_cp'] = rankdata(merged[col]).astype('float32')\n",
    "\n",
    "    # Cumulative normal percentile\n",
    "    merged[col+'_cnp'] = norm.cdf(merged[col]).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d5fd487e4440606deb9e936346e982513f0718c9"
   },
   "source": [
    "Getting the list of names of the added features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "456a64b4d2c1ada1b6db546a1d004537df4bd238"
   },
   "outputs": [],
   "source": [
    "new_features = set(merged.columns) - set(original_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8188eb856e421905972cc6f34ab4b43e87dd41f8"
   },
   "source": [
    "Normalize the data. Again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "7180731459fe9ce60f95b94b77f3d7f9a565823d"
   },
   "outputs": [],
   "source": [
    "for col in new_features:\n",
    "    merged[col] = ((merged[col] - merged[col].mean()) \n",
    "    / merged[col].std()).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3f1039a0b002c1db092a9b3d590759531facc3e6"
   },
   "source": [
    "Saving the data to feather files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "9f04f23ad704daa0207a03c9c6e5d680ac0caed8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing target to a feather files...\n",
      "Writing train_ids to a feather files...\n",
      "Writing test_ids to a feather files...\n",
      "Writing train to a feather files...\n",
      "Writing test to a feather files...\n"
     ]
    }
   ],
   "source": [
    "path_target = 'target.feather'\n",
    "\n",
    "path_train_ids = 'train_ids_extra_features.feather'\n",
    "path_test_ids = 'test_ids_extra_features.feather'\n",
    "\n",
    "path_train = 'train_extra_features.feather'\n",
    "path_test = 'test_extra_features.feather'\n",
    "\n",
    "print(\"Writing target to a feather files...\")\n",
    "pd.DataFrame({'target' : target.values}).to_feather(path_target)\n",
    "\n",
    "print(\"Writing train_ids to a feather files...\")\n",
    "pd.DataFrame({'ID_code' : train_ids.values}).to_feather(path_train_ids)\n",
    "\n",
    "print(\"Writing test_ids to a feather files...\")\n",
    "pd.DataFrame({'ID_code' : test_ids.values}).to_feather(path_test_ids)\n",
    "\n",
    "print(\"Writing train to a feather files...\")\n",
    "feather.write_dataframe(merged.iloc[:len_train], path_train)\n",
    "\n",
    "print(\"Writing test to a feather files...\")\n",
    "feather.write_dataframe(merged.iloc[len_train:], path_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "640948a1a36e2d3d73f18ceb9cfb816be6d11d7b"
   },
   "source": [
    "Removing data we no longer need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del target, train_ids, test_ids, merged\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "837f988316528d5c3d4530043448fe5849be3fa5"
   },
   "source": [
    "# Loading the data from feather files\n",
    "\n",
    "Now let's load of these data back into memory. This will help us to illustrate the advantage of using the feather file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "60b26db1cf85167b14f9223af995a8656bdaa316"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading target\n",
      "0.003959 sec\n"
     ]
    }
   ],
   "source": [
    "path_target = 'target.feather'\n",
    "\n",
    "path_train_ids = 'train_ids_extra_features.feather'\n",
    "path_test_ids = 'test_ids_extra_features.feather'\n",
    "\n",
    "path_train = 'train_extra_features.feather'\n",
    "path_test = 'test_extra_features.feather'\n",
    "\n",
    "print(\"Reading target\")\n",
    "start = time.time()\n",
    "y = feather.read_dataframe(path_target).values.ravel()\n",
    "end = time.time()\n",
    "\n",
    "print(\"{0:5f} sec\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "2f60516cb907e9e62f97eb99ebb00db079edc6e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train_ids\n",
      "0.016006 sec\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading train_ids\")\n",
    "start = time.time()\n",
    "train_ids = feather.read_dataframe(path_train_ids).values.ravel()\n",
    "end = time.time()\n",
    "\n",
    "print(\"{0:5f} sec\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "4c8ad8191f0a4cd976645e7d7b59f7c16c48311f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading test_ids\n",
      "0.015214 sec\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading test_ids\")\n",
    "start = time.time()\n",
    "test_ids = feather.read_dataframe(path_test_ids).values.ravel()\n",
    "end = time.time()\n",
    "\n",
    "print(\"{0:5f} sec\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "afe5ba0c48d46a05e09c2de00b094a5a479fded6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data\n",
      "0.634786 sec\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading training data\")\n",
    "\n",
    "start = time.time()\n",
    "train = feather.read_dataframe(path_train)\n",
    "end = time.time()\n",
    "\n",
    "print(\"{0:5f} sec\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "4764997b330eb79e2962c6ea207b2bf43d75b7a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading testing data\n",
      "0.652537 sec\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading testing data\")\n",
    "\n",
    "start = time.time()\n",
    "test = feather.read_dataframe(path_test)\n",
    "end = time.time()\n",
    "\n",
    "print(\"{0:5f} sec\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d3d1c00f01bdcc40525a6d59cf3bc463bdbcef11"
   },
   "source": [
    "Hopefully now you can see the great advantage of using the feather files: it is blazing fast. Just compare the timings shown above with those measured for the original CSV files: the processed data sets (stored in the feather file format) that we have just loaded are much bigger in size that the original ones (stored in the CSV files) but we can load them in almost no time!\n",
    "\n",
    "# Logistic regession with the added features.\n",
    "\n",
    "Now let's finally do some modeling! More specifically, we will build a straighforward logistic regression model to see whether or not we can improve on linear regression result (LB 0.894). \n",
    "\n",
    "Setting things up for the modeling phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "72ddd6eee811099caba7f2cc610e7f099d8fa84f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NFOLDS = 5\n",
    "RANDOM_STATE = 871972\n",
    "\n",
    "feature_list = train.columns\n",
    "\n",
    "test = test[feature_list]\n",
    "\n",
    "X = train.values.astype('float32')\n",
    "X_test = test.values.astype('float32')\n",
    "\n",
    "folds = StratifiedKFold(n_splits=NFOLDS, shuffle=True, \n",
    "                        random_state=RANDOM_STATE)\n",
    "oof_preds = np.zeros((len(train), 1))\n",
    "test_preds = np.zeros((len(test), 1))\n",
    "roc_cv =[]\n",
    "\n",
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6e5750e889c0aab08e0230a00641bb589a723d04"
   },
   "source": [
    "Defining a function for the augmentation proceduer (for deltails, see [this kernel](https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment)): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "8bdee398862caef3ddcfeaabadfc025e2fea280a"
   },
   "outputs": [],
   "source": [
    "def augment(x,y,t=2):\n",
    "    \n",
    "    if t==0:\n",
    "        return x, y\n",
    "    \n",
    "    xs,xn = [],[]\n",
    "    for i in range(t):\n",
    "        mask = y>0\n",
    "        x1 = x[mask].copy()\n",
    "        ids = np.arange(x1.shape[0])\n",
    "        for c in range(x1.shape[1]):\n",
    "            np.random.shuffle(ids)\n",
    "            x1[:,c] = x1[ids][:,c]\n",
    "        xs.append(x1)\n",
    "        del x1\n",
    "        gc.collect()\n",
    "\n",
    "    for i in range(t//2):\n",
    "        mask = y==0\n",
    "        x1 = x[mask].copy()\n",
    "        ids = np.arange(x1.shape[0])\n",
    "        for c in range(x1.shape[1]):\n",
    "            np.random.shuffle(ids)\n",
    "            x1[:,c] = x1[ids][:,c]\n",
    "        xn.append(x1)\n",
    "        del x1\n",
    "        gc.collect()\n",
    "        \n",
    "    print(\"The sizes of x, xn, and xs are {}, {}, {}, respectively.\".format(sys.getsizeof(x),\n",
    "                                                                            sys.getsizeof(xn),\n",
    "                                                                            sys.getsizeof(xs)\n",
    "                                                                           )\n",
    "         )\n",
    "    \n",
    "    xs = np.vstack(xs)\n",
    "    xn = np.vstack(xn)\n",
    "    \n",
    "    print(\"The sizes of x, xn, and xs are {}, {}, {}, respectively.\".format(sys.getsizeof(x)/1024**3,\n",
    "                                                                            sys.getsizeof(xn),\n",
    "                                                                            sys.getsizeof(xs)\n",
    "                                                                           )\n",
    "         )\n",
    "    \n",
    "    ys = np.ones(xs.shape[0])\n",
    "    yn = np.zeros(xn.shape[0])\n",
    "\n",
    "    y = np.concatenate([y,ys,yn])\n",
    "    print(\"The sizes of y, yn, and ys are {}, {}, {}, respectively.\".format(sys.getsizeof(y),\n",
    "                                                                            sys.getsizeof(yn),\n",
    "                                                                            sys.getsizeof(ys)\n",
    "                                                                           )\n",
    "         )\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "    return np.vstack([x,xs, xn]), y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0f8952de31eb35a24d805e2f05234419a787c2b5"
   },
   "source": [
    "Modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "bac555a0224df2ec57edea0d9efc2bea6087a1b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Fold: 0\n",
      "\n",
      "Fold 0, Augmentation 1\n",
      "Making predictions for the validation data\n",
      "Making predictions for the test data\n",
      "AUC = 0.8976502523927429\n",
      "Current Fold: 1\n",
      "\n",
      "Fold 1, Augmentation 1\n",
      "Making predictions for the validation data\n",
      "Making predictions for the test data\n",
      "AUC = 0.8969829087518688\n",
      "Current Fold: 2\n",
      "\n",
      "Fold 2, Augmentation 1\n",
      "Making predictions for the validation data\n",
      "Making predictions for the test data\n",
      "AUC = 0.8996536702258581\n",
      "Current Fold: 3\n",
      "\n",
      "Fold 3, Augmentation 1\n",
      "Making predictions for the validation data\n",
      "Making predictions for the test data\n",
      "AUC = 0.8962631156813364\n",
      "Current Fold: 4\n",
      "\n",
      "Fold 4, Augmentation 1\n",
      "Making predictions for the validation data\n",
      "Making predictions for the test data\n",
      "AUC = 0.8950210582556646\n"
     ]
    }
   ],
   "source": [
    "for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "    print(\"Current Fold: {}\".format(fold_))\n",
    "    trn_x, trn_y = X[trn_, :], y[trn_]\n",
    "    val_x, val_y = X[val_, :], y[val_]\n",
    "    \n",
    "    NAUGMENTATIONS=1#5\n",
    "    NSHUFFLES=0#2  # turning off the augmentation by shuffling since it did not help\n",
    "    \n",
    "    val_pred, test_fold_pred = 0, 0\n",
    "    for i in range(NAUGMENTATIONS):\n",
    "        \n",
    "        print(\"\\nFold {}, Augmentation {}\".format(fold_, i+1))\n",
    "        \n",
    "        trn_aug_x, trn_aug_y = augment(trn_x, trn_y, NSHUFFLES)\n",
    "        trn_aug_x = pd.DataFrame(trn_aug_x)\n",
    "        trn_aug_x = trn_aug_x.add_prefix('var_')\n",
    "        \n",
    "        clf = Pipeline([\n",
    "            #('scaler', StandardScaler()),\n",
    "            #('qt', QuantileTransformer(output_distribution='normal')),\n",
    "            ('lr_clf', LogisticRegression(solver='lbfgs', max_iter=1500, C=10))\n",
    "        ])\n",
    "\n",
    "        clf.fit(trn_aug_x, trn_aug_y)\n",
    "        \n",
    "        print(\"Making predictions for the validation data\")\n",
    "        val_pred += clf.predict_proba(val_x)[:,1]\n",
    "        \n",
    "        print(\"Making predictions for the test data\")\n",
    "        test_fold_pred += clf.predict_proba(X_test)[:,1]\n",
    "        \n",
    "    val_pred /= NAUGMENTATIONS\n",
    "    test_fold_pred /= NAUGMENTATIONS\n",
    "    \n",
    "    roc_cv.append(roc_auc_score(val_y, val_pred))\n",
    "    \n",
    "    print(\"AUC = {}\".format(roc_auc_score(val_y, val_pred)))\n",
    "    oof_preds[val_, :] = val_pred.reshape((-1, 1))\n",
    "    test_preds += test_fold_pred.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bdaeb55ef0787d12809ef93cb039f20a9ea48420"
   },
   "source": [
    "Predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "4f9c059d80cd7f3a88ec54c6981d5bf61175372c"
   },
   "outputs": [],
   "source": [
    "test_preds /= NFOLDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "01b3796195161127820576b0bf6874a0c2730b3b"
   },
   "source": [
    "Evaluating the cross-validation AUC score (we compute both the average AUC for all folds and the AUC for combined folds).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "2a717d9ff79b7d7debb7cfc12a01437925fa659d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of the folds' AUCs = 0.89711\n",
      "Combined folds' AUC = 0.89711\n",
      "The standard deviation = 0.00154\n"
     ]
    }
   ],
   "source": [
    "roc_score_1 = round(roc_auc_score(y, oof_preds.ravel()), 5)\n",
    "roc_score = round(sum(roc_cv)/len(roc_cv), 5)\n",
    "st_dev = round(np.array(roc_cv).std(), 5)\n",
    "\n",
    "print(\"Average of the folds' AUCs = {}\".format(roc_score))\n",
    "print(\"Combined folds' AUC = {}\".format(roc_score_1))\n",
    "print(\"The standard deviation = {}\".format(st_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6f8f29301f1a46851bbd8d73b53b42e3cf1b78b2"
   },
   "source": [
    "Creating the submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "cf48c73f9a06e7396c8a34dff4e80ba1b21fc59b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving submission file\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving submission file\")\n",
    "sample = pd.read_csv('../input/sample_submission.csv')\n",
    "sample.target = test_preds.astype(float)\n",
    "sample.ID_code = test_ids\n",
    "sample.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6ae9818982ca118b293d82ef58e8bdc5e11370e1"
   },
   "source": [
    "The LB score is now 0.896 versus 0.894 for linear regression. The mprovement of 0.001 is obviously very small. It looks like for this data linear and logistic regression work equally well! Moving forward, I think it would be interesting to see how the feature engineering presented here would affect other classification models (e.g. Gaussian Naive Bayes, LDA, LightGBM, XGBoost, CatBoost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "e3b88b41d876338362d22fbeb552bf3ec6db964b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
